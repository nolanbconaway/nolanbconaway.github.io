<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Nolan Conaway's Blog</title><link href="https://nolanbconaway.github.io/" rel="alternate"></link><link href="https://nolanbconaway.github.io/feeds/all.atom.xml" rel="self"></link><id>https://nolanbconaway.github.io/</id><updated>2020-01-05T00:00:00-05:00</updated><entry><title>2019: The high and low temperatures</title><link href="https://nolanbconaway.github.io/blog/2020/2019-the-high-and-low-temperatures.html" rel="alternate"></link><published>2020-01-05T00:00:00-05:00</published><updated>2020-01-05T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-01-05:/blog/2020/2019-the-high-and-low-temperatures.html</id><summary type="html">&lt;p&gt;Last winter I hooked up a &lt;a href="https://www.adafruit.com/product/374"&gt;DS18B20&lt;/a&gt; thermometer to a Raspberry Pi. I set up a &lt;a href="https://github.com/nolanbconaway/thermometer"&gt;python library&lt;/a&gt; to read the temperature, and a cron job to save the temperature in my postgres database every minute. Then I built a &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/"&gt;webapp&lt;/a&gt; to display the last 24 hours of data. The ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last winter I hooked up a &lt;a href="https://www.adafruit.com/product/374"&gt;DS18B20&lt;/a&gt; thermometer to a Raspberry Pi. I set up a &lt;a href="https://github.com/nolanbconaway/thermometer"&gt;python library&lt;/a&gt; to read the temperature, and a cron job to save the temperature in my postgres database every minute. Then I built a &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/"&gt;webapp&lt;/a&gt; to display the last 24 hours of data. The first stable readings were stored on Jan 5 2019, so we have arrived at one full year of readings from my thermometer project üéâ!&lt;/p&gt;
&lt;p&gt;The data are probably uninteresting to anyone who does not live in my apartment. Actually, they're not even interesting to all the people who &lt;em&gt;do&lt;/em&gt; live here ü§î.&lt;/p&gt;
&lt;p&gt;But I think the data can tell a story. At least some of it. This post tells the stories of unusual days in my apartment, from the perspective of temperature measurements.&lt;/p&gt;
&lt;h2 id="describing-a-days-temperatures"&gt;Describing a day's temperatures&lt;/h2&gt;
&lt;p&gt;Early on, I wrote out a SQL view that computes summary statistics on each day's temperature readings. It shows me descriptives like:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;readings&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;th&gt;range&lt;/th&gt;
&lt;th&gt;avg&lt;/th&gt;
&lt;th&gt;stddev&lt;/th&gt;
&lt;th&gt;avg_minute_delta&lt;/th&gt;
&lt;th&gt;day_delta&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-05&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;67.9&lt;/td&gt;
&lt;td&gt;74.9&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;71.5&lt;/td&gt;
&lt;td&gt;1.7&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;-3.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-06&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;71.2&lt;/td&gt;
&lt;td&gt;74.8&lt;/td&gt;
&lt;td&gt;3.6&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;-0.001&lt;/td&gt;
&lt;td&gt;1.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-07&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;68.9&lt;/td&gt;
&lt;td&gt;73.5&lt;/td&gt;
&lt;td&gt;4.6&lt;/td&gt;
&lt;td&gt;70.7&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-0.003&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-08&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;67.8&lt;/td&gt;
&lt;td&gt;72.5&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;td&gt;70.3&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;-2.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-09&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;69.5&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;3.5&lt;/td&gt;
&lt;td&gt;71.6&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Most of those columns have sensible names. &lt;code&gt;avg_minute_delta&lt;/code&gt; describes the average difference in temperature per minute across adjacent readings. &lt;code&gt;day_delta&lt;/code&gt; is the difference between the first and last reading in the day.&lt;/p&gt;
&lt;p&gt;I looked into these summary stats for days which stand out, days like:&lt;/p&gt;
&lt;h2 id="the-warmest-day"&gt;üî• the warmest day&lt;/h2&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2020/warmest.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;The day with the highest average temperature was &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-07-05"&gt;July 5&lt;/a&gt;. We were in Baltimore to celebrate July 4 with my family, so we closed up the windows and shut off the AC and the apartment absolutely &lt;em&gt;roasted&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="the-coldest-day"&gt;‚ùÑ the coldest day&lt;/h2&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2020/coldest.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Per NYC regulation, landlords are required to maintain a temperature of 68‚Ñâ during the day in the winter, but on &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-02-01"&gt;Feb 1&lt;/a&gt; the temperature hung below 66‚Ñâ all day long. &lt;/p&gt;
&lt;h2 id="the-most-variable-day"&gt;The most variable day&lt;/h2&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2020/most_stddev.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;On December 16 we were burgled (&lt;strong&gt;everyone was ok!&lt;/strong&gt;). The burglar entered through the fire escape window, which is near where the Raspberry Pi thermometer is tucked away. That huge dip in the temperature is exactly the time the burglar entered the apartment. The detective that caught our case thought this was &lt;em&gt;awesome&lt;/em&gt;!&lt;/p&gt;
&lt;h2 id="anomaly-detection"&gt;Anomaly Detection&lt;/h2&gt;
&lt;p&gt;We can also use statistics to identify days with unusual properties. I queried my database with Python like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sqlalchemy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;

&lt;span class="n"&gt;engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'SQLALCHEMY_DATABASE_URI'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;daily_stats&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_sql_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"daily_stats"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'dt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;daily_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="c1"&gt;#             readings    min    max  range    avg  stddev  avg_minute_delta  day_delta&lt;/span&gt;
&lt;span class="c1"&gt;# dt                                                                                   &lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-05      1440  67.89  74.86   6.98  71.54    1.65               0.0      -3.38&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-06      1440  71.15  74.75   3.60  72.96    0.75              -0.0       1.24&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-07      1440  68.90  73.51   4.61  70.65    0.95              -0.0       3.71&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-08      1440  67.78  72.50   4.72  70.28    1.12               0.0      -2.36&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-09      1440  69.46  72.95   3.49  71.62    0.79              -0.0       0.34&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we assume all days are drawn from a multivariate normal distribution across the summary stat metrics, \(x \sim \mathcal{N}\left(\mu, \Sigma\right) \), outliers would be considered dates with very low \( p\left(x \vert  \mathcal{N}\left(\mu, \Sigma\right)\right) \).&lt;/p&gt;
&lt;p&gt;I normalized the stats and calculated the empirical parameters for the multivariate Normal distribution. Then I used &lt;code&gt;scipy.stats&lt;/code&gt; to do the heavy lifting.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt;

&lt;span class="n"&gt;zscores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;daily_stats&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'readings'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# I don't care about reading count&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zscores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zscores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;density&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zscores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then I picked out the days with the lowest densities! Here are those dates:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;readings&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;th&gt;range&lt;/th&gt;
&lt;th&gt;avg&lt;/th&gt;
&lt;th&gt;stddev&lt;/th&gt;
&lt;th&gt;avg_minute_delta&lt;/th&gt;
&lt;th&gt;day_delta&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-12-16"&gt;2019-12-16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;50.787&lt;/td&gt;
&lt;td&gt;71.15&lt;/td&gt;
&lt;td&gt;20.363&lt;/td&gt;
&lt;td&gt;65.003&lt;/td&gt;
&lt;td&gt;6.334&lt;/td&gt;
&lt;td&gt;-0.003&lt;/td&gt;
&lt;td&gt;3.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-09-12"&gt;2019-09-12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;74.862&lt;/td&gt;
&lt;td&gt;86.9&lt;/td&gt;
&lt;td&gt;12.038&lt;/td&gt;
&lt;td&gt;83.073&lt;/td&gt;
&lt;td&gt;3.715&lt;/td&gt;
&lt;td&gt;-0.008&lt;/td&gt;
&lt;td&gt;11.925&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-09-19"&gt;2019-09-19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;65.187&lt;/td&gt;
&lt;td&gt;77.9&lt;/td&gt;
&lt;td&gt;12.713&lt;/td&gt;
&lt;td&gt;73.059&lt;/td&gt;
&lt;td&gt;4.709&lt;/td&gt;
&lt;td&gt;0.005&lt;/td&gt;
&lt;td&gt;-7.987&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-09-11"&gt;2019-09-11&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;77.9&lt;/td&gt;
&lt;td&gt;87.125&lt;/td&gt;
&lt;td&gt;9.225&lt;/td&gt;
&lt;td&gt;82.538&lt;/td&gt;
&lt;td&gt;3.736&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;-8.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-10-01"&gt;2019-10-01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;73.287&lt;/td&gt;
&lt;td&gt;82.737&lt;/td&gt;
&lt;td&gt;9.45&lt;/td&gt;
&lt;td&gt;78.11&lt;/td&gt;
&lt;td&gt;3.643&lt;/td&gt;
&lt;td&gt;0.005&lt;/td&gt;
&lt;td&gt;-7.313&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The first anomaly is an obvious one: the most variable day of the year which I wrote about above. See below for plots of the other four:&lt;/p&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-09-12.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-09-19.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-09-11.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-10-01.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;And for good measure, &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-03-22"&gt;here&lt;/a&gt; is the least anomalous date according to my very naive model.&lt;/p&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2020/nonanomaly.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Deploying my Pelican website to Github Pages.</title><link href="https://nolanbconaway.github.io/blog/2020/deploying-my-pelican-website-to-github-pages.html" rel="alternate"></link><published>2020-01-01T00:00:00-05:00</published><updated>2020-01-01T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-01-01:/blog/2020/deploying-my-pelican-website-to-github-pages.html</id><summary type="html">&lt;p&gt;üéâ If you're reading this, you're looking at my new website! üéâ&lt;/p&gt;
&lt;p&gt;I ditched Jekyll because I didn't want to have to maintain a ruby installation on my machine. My new site runs on Pelican and I hacked together a deployment flow to Github pages. Here's that hack:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Main Workflow&lt;/span&gt;

&lt;span class="nt"&gt;on ‚Ä¶&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;üéâ If you're reading this, you're looking at my new website! üéâ&lt;/p&gt;
&lt;p&gt;I ditched Jekyll because I didn't want to have to maintain a ruby installation on my machine. My new site runs on Pelican and I hacked together a deployment flow to Github pages. Here's that hack:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Main Workflow&lt;/span&gt;

&lt;span class="nt"&gt;on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;push&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;branches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;dev&lt;/span&gt;

&lt;span class="nt"&gt;jobs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;build-and-deploy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;runs-on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ubuntu-latest&lt;/span&gt;

    &lt;span class="nt"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;actions/checkout@v1&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;actions/setup-python@v1&lt;/span&gt;
        &lt;span class="nt"&gt;with&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="nt"&gt;python-version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"3.7"&lt;/span&gt;

      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install Dependencies&lt;/span&gt;
        &lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
          &lt;span class="no"&gt;python -m pip install --upgrade pip&lt;/span&gt;
          &lt;span class="no"&gt;pip install -r requirements.txt&lt;/span&gt;

      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Build Production Site&lt;/span&gt;
        &lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
          &lt;span class="no"&gt;make publish&lt;/span&gt;
          &lt;span class="no"&gt;touch output/.nojekyll&lt;/span&gt;

      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Deploy To Master Branch&lt;/span&gt;
        &lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;s0/git-publish-subdir-action@master&lt;/span&gt;
        &lt;span class="nt"&gt;env&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="nt"&gt;REPO&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;self&lt;/span&gt;
          &lt;span class="nt"&gt;BRANCH&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;master&lt;/span&gt;
          &lt;span class="nt"&gt;FOLDER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;output&lt;/span&gt;
          &lt;span class="nt"&gt;GITHUB_TOKEN&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ secrets.GITHUB_TOKEN }}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="translation"&gt;Translation...&lt;/h3&gt;
&lt;p&gt;On push to the &lt;code&gt;dev&lt;/code&gt; branch, Github will set up a python 3.7 environment with my requirements.txt. It'll build the static site into an &lt;code&gt;output&lt;/code&gt; directory using &lt;code&gt;make publish&lt;/code&gt;. I added a &lt;code&gt;.nojekyll&lt;/code&gt; file so that Github knows it's not working with a Jekyll page.&lt;/p&gt;
&lt;p&gt;I found an &lt;a href="https://github.com/marketplace/actions/push-git-subdirectory-as-branch"&gt;action&lt;/a&gt; that will push a subdirectory to another branch of the repo. I set that up to send the static site directory to the &lt;code&gt;master&lt;/code&gt; branch. Github will then automatically deploy the site since i am using my &lt;code&gt;username.github.io&lt;/code&gt; repo! üéä&lt;/p&gt;
&lt;p&gt;So the flow is that I make all updates to &lt;code&gt;dev&lt;/code&gt; which propagate automatically to &lt;code&gt;master&lt;/code&gt; and then deployment on push. Beautiful! You can check out the source code on &lt;a href="https://github.com/nolanbconaway/nolanbconaway.github.io/tree/dev"&gt;github&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Underground, A python library for processing realtime NYC subway data.</title><link href="https://nolanbconaway.github.io/blog/2019/underground-a-python-library-for-processing-realtime-nyc-subway-data.html" rel="alternate"></link><published>2019-10-06T00:00:00-04:00</published><updated>2019-10-06T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-10-06:/blog/2019/underground-a-python-library-for-processing-realtime-nyc-subway-data.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Shlack, yet another command line interface for slack.</title><link href="https://nolanbconaway.github.io/blog/2019/shlack-yet-another-command-line-interface-for-slack.html" rel="alternate"></link><published>2019-10-03T00:00:00-04:00</published><updated>2019-10-03T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-10-03:/blog/2019/shlack-yet-another-command-line-interface-for-slack.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Kimery Levering, Ken Kurtz, and I published some experiments in Memory and Congition.</title><link href="https://nolanbconaway.github.io/blog/2019/kimery-levering-ken-kurtz-and-i-published-some-experiments-in-memory-and-congition.html" rel="alternate"></link><published>2019-09-04T00:00:00-04:00</published><updated>2019-09-04T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-09-04:/blog/2019/kimery-levering-ken-kurtz-and-i-published-some-experiments-in-memory-and-congition.html</id><content type="html"></content><category term="pdf"></category></entry><entry><title>I set up a webapp to monitor my home network performance.</title><link href="https://nolanbconaway.github.io/blog/2019/i-set-up-a-webapp-to-monitor-my-home-network-performance.html" rel="alternate"></link><published>2019-06-06T00:00:00-04:00</published><updated>2019-06-06T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-06-06:/blog/2019/i-set-up-a-webapp-to-monitor-my-home-network-performance.html</id><content type="html"></content><category term="app"></category></entry><entry><title>A single sample poisson test.</title><link href="https://nolanbconaway.github.io/blog/2019/a-single-sample-poisson-test.html" rel="alternate"></link><published>2019-05-15T00:00:00-04:00</published><updated>2019-05-15T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-05-15:/blog/2019/a-single-sample-poisson-test.html</id><summary type="html">&lt;p&gt;I am working with a lot of Poisson data recently. The more I encounter these data, the more I realize that data scientists often incorrectly treat Poisson data as normally or linearly distributed variables.&lt;/p&gt;
&lt;p&gt;I recently added a new tool to my Poisson kit, which I think of as a ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am working with a lot of Poisson data recently. The more I encounter these data, the more I realize that data scientists often incorrectly treat Poisson data as normally or linearly distributed variables.&lt;/p&gt;
&lt;p&gt;I recently added a new tool to my Poisson kit, which I think of as a "Single Sample Poisson test". The question being asked is:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What is the probability of drawing \(N\) samples with mean \(\bar{x}=\sum_i{N_i} \div N\) from a Poisson distribution with mean \(\mu\)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An example to make things more concrete. Search platforms like google/bing/yahoo allow you to bid a certain amount to show an ad when someone searches for a given keyword. You only pay the search engine if the ad is clicked on. They usually won't charge you the full price of the bid, but some unknown lower amount that we refer to as the cost-per-click (CPC).&lt;/p&gt;
&lt;p&gt;Imagine we are hoping to pay a 100 cent ($1 USD) CPC.  We can measure how much we actually paid for each click, but as clicks come in they have variable prices:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Price (cents)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;9:01am&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:01am&lt;/td&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:01am&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:02am&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:02am&lt;/td&gt;
&lt;td&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Above are 5 samples with an average CPC of 102.6 cents. How likely are we to obtain 5 samples with an average of 102.6, assuming that the true CPC is 100? If \(N=5\) samples with \(\bar{x}=102.6\) is unlikely given a distribution with \(\mu=100\), then clearly we are not correctly targeting the CPC value.&lt;/p&gt;
&lt;h2 id="the-test"&gt;The Test&lt;/h2&gt;
&lt;p&gt;A traditional approach to this question might treat the samples as normally distributed and use a Student's T-Test or a Sign test. This works a lot of the time due to the fact that &lt;a href="http://socr.ucla.edu/Applets.dir/NormalApprox2PoissonApplet.html"&gt;Poisson data are approximately normal when \(\mu &amp;gt; 20\)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want a "pure-Poisson" approach, you need to think about it another way. I took part in a &lt;a href="https://stats.stackexchange.com/questions/399803/compute-sample-probabilities-given-a-poisson-distribution"&gt;very interesting SO discussion&lt;/a&gt; in which a &lt;a href="https://stats.stackexchange.com/users/85665/bruceet"&gt;Very Smart Person&lt;/a&gt; basically gave me the answer. I will relate that answer here.&lt;/p&gt;
&lt;p&gt;In short, drawing \(N\) samples with an average of \(\bar{x}\) is like drawing a single sample from a higher distribution, which has the value \(\sum_i{x_i} = N\bar{x}\). We can think about that value within the context of a distribution \(\mathsf{Poisson}(N \mu)\), which would tell us how unlikely our samples are:&lt;/p&gt;
&lt;p&gt;$$
N \bar x \sim \mathsf{Poisson}(N \mu).
$$&lt;/p&gt;
&lt;p&gt;For example, the CDF of \(\mathsf{Poisson}(N \mu)\) would tell us the proportion of values which are greater than or less than \(N\bar{x}\).&lt;/p&gt;
&lt;p&gt;If only a small proportion of values from the CPC distribution \(\mathsf{Poisson}(5 \cdot 100)\) are greater than \(5 \cdot 102.6\), this would indicate that it is unlikely to have obtained \(\bar{x}=102.6\) by chance and thus maybe there's something wrong with the CPC targeting.&lt;/p&gt;
&lt;h3 id="in-python"&gt;In Python&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;xbar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;102.6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;xbar&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;0.7285633495908114&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That shows that 72.9% of \(N=5\) samples from a distribution with \(\mu=100\) would have an average value of less than \(\bar{x}=102.6\). Conversely, 27.1% of samples would have a greater value. So our samples aren't very unlikely given a CPC of 100.&lt;/p&gt;
&lt;p&gt;As you increase the number of samples you'd expect \(\bar{x}=102.6\) to become more and more unlikely.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;     &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;xbar&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="mf"&gt;0.7994310510662929&lt;/span&gt;
&lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="mf"&gt;0.8795150478303122&lt;/span&gt;
&lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="mf"&gt;0.9236492476438214&lt;/span&gt;
&lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="mf"&gt;0.9503028216143891&lt;/span&gt;
&lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="mf"&gt;0.9671124727069668&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When \(N=50\), 96.7% of samples from the distribution would have a lower average value (conversely, only 3.3% of samples would have a greater value). We're much less likely to obtain \(\bar{x}=102.6\) by chance, and I might suggest looking into how CPCs are being targeted.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>A python implementation of the Poisson exact test (e-test).</title><link href="https://nolanbconaway.github.io/blog/2019/a-python-implementation-of-the-poisson-exact-test-e-test.html" rel="alternate"></link><published>2019-03-24T00:00:00-04:00</published><updated>2019-03-24T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-03-24:/blog/2019/a-python-implementation-of-the-poisson-exact-test-e-test.html</id><summary type="html">&lt;p&gt;At work we are doing tests on different paid search optimization tools. We wanted to see if a new tool offered improvements over what we had been using. In reality we are comparing the two on all sorts of metrics, but as an example we can focus on comparisons with ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work we are doing tests on different paid search optimization tools. We wanted to see if a new tool offered improvements over what we had been using. In reality we are comparing the two on all sorts of metrics, but as an example we can focus on comparisons with respect to the cost paid per click, &lt;em&gt;CPC&lt;/em&gt;, measured in USD.&lt;/p&gt;
&lt;p&gt;Cost per click is an example of a Poisson distributed variable. We started measuring the CPC during some control period using the current tools, then switched to the new tool in a test period.&lt;/p&gt;
&lt;p&gt;Those data might look something like:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Control&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CPC&lt;/td&gt;
&lt;td&gt;$10&lt;/td&gt;
&lt;td&gt;$9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cost&lt;/td&gt;
&lt;td&gt;$400&lt;/td&gt;
&lt;td&gt;$450&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Clicks&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The question is: How likely are we to obtain those data under the assumption that there was no change in overall CPC?&lt;/p&gt;
&lt;p&gt;That question specifies the null hypothesis: the two periods of data were generated from the same distribution. Under the alternative hypothesis the test period data might have a greater or lesser CPC.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://www.ucs.louisiana.edu/~kxk4695/JSPI-04.pdf"&gt;Poisson Exact Test&lt;/a&gt; (E-Test) is a hypothesis test for answering this kind of question. In R, you can use &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/poisson.test.html"&gt;&lt;code&gt;poisson.test&lt;/code&gt;&lt;/a&gt;, which implements the similar but inexact Poisson C-Test. But in Python, no such implementation exists.&lt;/p&gt;
&lt;p&gt;I dug up the original &lt;a href="http://www.ucs.louisiana.edu/~kxk4695/statcalc/pois2pval.for"&gt;Fortran code&lt;/a&gt; posted to the academic website belonging to one of the authors of the test. I edited it very minimally so that it could be wrapped by the Numpy &lt;a href="https://docs.scipy.org/doc/numpy/f2py/index.html"&gt;Fortran to Python interface generator&lt;/a&gt;. Then I packaged it and posted on pypi, so you can use it too!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nolanbconaway/poisson-etest"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/poisson-etest/0.0/"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://travis-ci.org/nolanbconaway/poisson-etest"&gt;Travis CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="usage"&gt;Usage&lt;/h2&gt;
&lt;p&gt;Just plug in the numbers to get your answer!&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;poisson_etest&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;poisson_etest&lt;/span&gt;

&lt;span class="n"&gt;control_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;
&lt;span class="n"&gt;control_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;
&lt;span class="n"&gt;test_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;450&lt;/span&gt;
&lt;span class="n"&gt;test_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;poisson_etest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;control_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;control_clicks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_clicks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 0.12732580695256054&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We believe more in the estimate of each period's CPC if we have more clicks. That is, I wouldn't say we know much about the CPC of the Test period after only obtaining a single click. Conversely, we have a strong estimate of the CPC after many hundreds.&lt;/p&gt;
&lt;p&gt;This test is useful in that it accounts for the sample size accordingly. Keeping the CPC's the same, lets check on the results if we had over 100 clicks in each period:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;control_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;105&lt;/span&gt;
&lt;span class="n"&gt;test_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="n"&gt;control_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;control_clicks&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;test_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_clicks&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, the probability that the two were generated from the same distribution is much smaller:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;poisson_etest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;control_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;control_clicks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_clicks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 0.0034409891789582165&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="blog"></category></entry><entry><title>A freezable dictionary for python.</title><link href="https://nolanbconaway.github.io/blog/2019/a-freezable-dictionary-for-python.html" rel="alternate"></link><published>2019-03-12T00:00:00-04:00</published><updated>2019-03-12T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-03-12:/blog/2019/a-freezable-dictionary-for-python.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Configuring a headless raspberry pi on your home network.</title><link href="https://nolanbconaway.github.io/blog/2019/rpi-setup.html" rel="alternate"></link><published>2019-03-06T00:00:00-05:00</published><updated>2019-03-06T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-03-06:/blog/2019/rpi-setup.html</id><summary type="html">&lt;hr/&gt;
&lt;p&gt;I do a lot of projects that involve raspberry pis. At this point, there are three tucked away in different corners of my apartment (one runs an online &lt;a href="/blog/2018/apartment-temp"&gt;temperature sensor&lt;/a&gt;). They all run headless and I SSH into them occasionally to see if anything has broken.&lt;/p&gt;
&lt;p&gt;I recently set up ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;hr/&gt;
&lt;p&gt;I do a lot of projects that involve raspberry pis. At this point, there are three tucked away in different corners of my apartment (one runs an online &lt;a href="/blog/2018/apartment-temp"&gt;temperature sensor&lt;/a&gt;). They all run headless and I SSH into them occasionally to see if anything has broken.&lt;/p&gt;
&lt;p&gt;I recently set up a fresh Raspberry Pi 3 B+. The project was to hook up a &lt;a href="https://www.adafruit.com/product/2097"&gt;PiTFT display&lt;/a&gt; and use it to show upcoming Manhattan-bound Q trains.&lt;/p&gt;
&lt;p&gt;The initial setup for a headless raspberry pi can be tricky. Since I've done this more than a handful of times, I thought I'd record the steps I take to configure my new machine.&lt;/p&gt;
&lt;h2 id="install-raspbian"&gt;Install Raspbian&lt;/h2&gt;
&lt;p&gt;I have never been able to remember the syntax to &lt;code&gt;dd&lt;/code&gt;, so I just head over to raspberrypi.org and follow &lt;a href="https://www.raspberrypi.org/documentation/installation/installing-images/README.md"&gt;their instructions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As for the Raspbian distribution, I choose the latest "lite" version (right now it is "Raspbian Stretch Lite") because I don't use the desktop environment. But you do you.&lt;/p&gt;
&lt;h3 id="before-you-proceed"&gt;Before you proceed ...&lt;/h3&gt;
&lt;p&gt;Consider the catch-22 situation you're in. In order to connect to a headless pi, you need to have set up ssh. But you also need to connect to it &lt;em&gt;in order to set up ssh&lt;/em&gt;. Luckily, there's a little trick to set up ssh before the first boot.&lt;/p&gt;
&lt;p&gt;Simply add a blank file, &lt;code&gt;ssh&lt;/code&gt;, to the root of the SD card. I never can remember where my SD card mount lives in the mac filesystem, so I navigate my finder to the mounted SD card and use &lt;a href="https://zipzapmac.com/Go2Shell"&gt;Go2Shell&lt;/a&gt; to put my terminal there. Then a simple &lt;code&gt;touch ssh&lt;/code&gt; command will do the trick.&lt;/p&gt;
&lt;p&gt;If you want to connect to your pi via wifi, you've arrived at yet another catch-22 situation. As far as I know, you need to be able to connect to your pi in order to configure the wifi connection. What a nightmare.&lt;/p&gt;
&lt;p&gt;I don't know any tricks here; for the time being, you'll need to connect to the pi via ethernet. I'll run through how to set up wifi later in this post.&lt;/p&gt;
&lt;h2 id="boot-it-up"&gt;Boot it up&lt;/h2&gt;
&lt;p&gt;With SSH enabled, you should be able to connect to the pi remotely via ethernet the very first time you boot it up. You won't know the machine's IP address ahead of time, but you can get that info a bunch of different ways (I use &lt;a href="https://inetapp.de/en/inetx.html"&gt;iNet Network Scanner&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Once you discover the IP address, ssh in. The default username and password are, of course, &lt;code&gt;pi&lt;/code&gt; and &lt;code&gt;raspberry&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh pi@&amp;lt;IP Address&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="step-1-update-upgrade"&gt;Step 1: update / upgrade&lt;/h2&gt;
&lt;p&gt;Get this out of the way. Should take a few mins, depending on the speed of your network.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo apt-get upgrade -y
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="step-2-raspi-config"&gt;Step 2: raspi-config&lt;/h2&gt;
&lt;p&gt;By default the locale for raspberry pis are set to &lt;code&gt;en_GB.UTF8&lt;/code&gt;. I, like many other people, live in NYC. So I need to change the locale (keyboard layout, character set, timezone, etc) for myself. Do this with the built-in GUI tool &lt;code&gt;raspi-config&lt;/code&gt;. Run it under sudo:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo raspi-config
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll get a display that looks like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Raspi Config" class="img-fluid" src="https://nolanbconaway.github.io/blog/2019/main.png"/&gt;&lt;/p&gt;
&lt;p&gt;I end up doing a lot of tinkering in here.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Network Options -&amp;gt; Hostname&lt;/strong&gt;. So that my machine has an informative name on my network.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advanced Options -&amp;gt; Expand Filesystem&lt;/strong&gt;. Gimme &lt;em&gt;all&lt;/em&gt; that sweet, sweet SD memory.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whatever you do, you want to make sure to hit &lt;strong&gt;Localisation Options&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Localisation" class="img-fluid" src="https://nolanbconaway.github.io/blog/2019/localisation.png"/&gt;&lt;/p&gt;
&lt;p&gt;When you hit the "Change Locale" menu, you'll get a very long list of possible locales. Scroll with your arrow keys (it sucks and takes forever) and press spacebar on the to select / deselect.&lt;/p&gt;
&lt;p&gt;Personally, I deselect &lt;code&gt;en_GB.UTF-8 UTF-8&lt;/code&gt; and then select &lt;code&gt;en_US.UTF-8 UTF-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When you're done, hit OK (skip there by pressing TAB). The next screen lets you choose your default locale. Just highlight the one you want and TAB to OK. You should be good to go.&lt;/p&gt;
&lt;p&gt;Next, change your timezone. This works a lot like setting your locale; highlight your region and TAB to OK.&lt;/p&gt;
&lt;h2 id="step-3-set-up-wifi-connection"&gt;Step 3: Set up wifi connection&lt;/h2&gt;
&lt;p&gt;If you want to connect to your raspberry pi on your wifi network, you'll want configure the connection earlier rather than later. I cannot tell you how many times I have lost all ability to connect to my machine (ethernet or otherwise) after messing up the network configuration in an attempt to set up the darn wifi.&lt;/p&gt;
&lt;p&gt;If you might break it all, you'll want to do it &lt;em&gt;before&lt;/em&gt; you invest a lot of time into configuration.&lt;/p&gt;
&lt;p&gt;There are &lt;a href="https://www.google.com/search?q=raspberry+pi+wireless+connection"&gt;like a million posts on how to configure a wifi connection&lt;/a&gt;. The &lt;a href="https://www.raspberrypi.org/documentation/configuration/wireless/wireless-cli.md"&gt;authoritative guide on the subject&lt;/a&gt; can be found on raspberrypi.org, and you might just want to do that.&lt;/p&gt;
&lt;p&gt;As for me, I don't know how &lt;em&gt;not&lt;/em&gt; to break this stuff. So I just copy over config from another, working, raspberry pi. My config looks like:&lt;/p&gt;
&lt;h3 id="etcnetworkinterfaces"&gt;&lt;code&gt;/etc/network/interfaces&lt;/code&gt;&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;source-directory /etc/network/interfaces.d

auto wlan0
iface wlan0 inet manual
wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="etcwpa_supplicantwpa_supplicantconf"&gt;&lt;code&gt;/etc/wpa_supplicant/wpa_supplicant.conf&lt;/code&gt;&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1
country=US

network={
    ssid="network name"
    psk="password"
}
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="reboot"&gt;Reboot&lt;/h3&gt;
&lt;p&gt;This is the moment of truth. Did you break everything? There's no shame in it. If so, hopefully you can still connect via ethernet, in which case you can tinker around until you get a wifi connection.&lt;/p&gt;
&lt;h2 id="step-4-set-up-a-static-ip"&gt;Step 4: Set up a static IP.&lt;/h2&gt;
&lt;p&gt;If you didn't ruin everything in step 3, you should be able to ssh in your pi again.&lt;/p&gt;
&lt;p&gt;At this point, your raspberry pi will be assigned an IP address each time it connects to your network. The address could change across reboots and I don't like having to check the address each time.&lt;/p&gt;
&lt;p&gt;The solution is to configure a static IP address. If you like pain, there is a way to configure the static address on your raspberry pi. But, to me, that has always felt &lt;em&gt;way&lt;/em&gt; too complicated, &lt;a href="https://raspberrypi.stackexchange.com/questions/37920/how-do-i-set-up-networking-wifi-static-ip-address"&gt;see this Stack Exchange post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I find it much easier to do this via my router's admin panel. Most routers will have a section in which you can specify a DHCP reservation for a specific MAC address. This is just a fancy way of making sure your router assigns a specific IP address to your raspberry pi, and that it never assigns any other device to that address.&lt;/p&gt;
&lt;p&gt;I use an Apple Airport Express, here's what that configuration screen looks like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="airport express network options" class="img-fluid" src="https://discussions.apple.com/content/attachment/466201040"/&gt;&lt;/p&gt;
&lt;h2 id="step-5-enable-passwordless-ssh"&gt;Step 5: Enable passwordless ssh.&lt;/h2&gt;
&lt;p&gt;I hate typing my passwords so I make sure to set up passwordless ssh.&lt;/p&gt;
&lt;p&gt;The idea here is that you create a &lt;code&gt;key&lt;/code&gt; on the machine you are using to ssh into the pi &lt;em&gt;from&lt;/em&gt; (that is, the computer you are in front of). Then you paste that key in a special place on the pi. When you try to log in, ssh will compare the keys and let you in without a password if they match.&lt;/p&gt;
&lt;p&gt;First thing, you'll need to make your key. Keys are unique to the machine you are ssh-ing &lt;em&gt;from&lt;/em&gt;, so if you've done this before you don't need to do it again. &lt;a href="https://docs.joyent.com/public-cloud/getting-started/ssh-keys/generating-an-ssh-key-manually/manually-generating-your-ssh-key-in-mac-os-x"&gt;Here's a tutorial on how you'd make an ssh key on a mac&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Your key will probably live in &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; and will look something like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh-rsa &amp;lt;lots of letters and numbers&amp;gt;&lt;span class="o"&gt;==&lt;/span&gt; user@host
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One you have that key copy-pasteable, you can run this command on your raspberry pi:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; mkdir .ssh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; nano .ssh/authorized_keys
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Paste the key from your computer in that bad boy, save the file (&lt;code&gt;ctrl-O&lt;/code&gt;), then log out and log back in. When you log back in, you shouldn't be asked for your password.&lt;/p&gt;
&lt;h2 id="step-6-change-the-username-and-password"&gt;Step 6: Change the username and password.&lt;/h2&gt;
&lt;p&gt;Everybody knows the default login for a raspberry pi (or at least, lots of people). So for security you'll want to change them. Changing the username can be a bit of a hassle since you need to move around the user home, etc, so I only change the username if I plan on opening my firewall to allow outside connections (gotta have that &lt;em&gt;extra&lt;/em&gt; security).&lt;/p&gt;
&lt;p&gt;In either case, &lt;em&gt;definitely&lt;/em&gt; change the password. You can do that with:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;passwd
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So easy.&lt;/p&gt;
&lt;h2 id="step-7-installations"&gt;Step 7: Installations&lt;/h2&gt;
&lt;p&gt;At this point:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Your raspberry pi has updated packages and the right locale.&lt;/li&gt;
&lt;li&gt;The network connectivity is in place.&lt;/li&gt;
&lt;li&gt;The same IP address is assigned to your machine each time you reboot.&lt;/li&gt;
&lt;li&gt;It is at least kind of secure with a nondefault password.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now you get to install the stuff you want to use for your projects. Each project requires different tooling, so below I'll list some of the common items I install each time:&lt;/p&gt;
&lt;h3 id="mailutils"&gt;mailutils&lt;/h3&gt;
&lt;p&gt;I run a lot of stuff in cron and I want to make sure I get notified if a job fails. If you install mailtutils, cron errors will automatically go to mail and you'll get an indicator if there was a failure when you log in.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install mailutils
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="zsh-oh-my-zsh"&gt;zsh, oh-my-zsh&lt;/h3&gt;
&lt;p&gt;I love zsh. &lt;em&gt;Love it&lt;/em&gt;. It's even better when you use &lt;a href="https://github.com/robbyrussell/oh-my-zsh"&gt;oh-my-zsh&lt;/a&gt; as a configuration manager.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install zsh&lt;span class="p"&gt;;&lt;/span&gt;
sh -c &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's all. Have fun spending the next hour &lt;a href="https://github.com/robbyrussell/oh-my-zsh/wiki/Themes"&gt;picking your theme&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="python"&gt;python&lt;/h3&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Dec 2019&lt;/strong&gt;: these days I am all about pyenv and I would probably opt for that instead.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I use python for basically everything. I prefer to keep the raspberry pi pre-installed python clean, so I opt to install my own python.&lt;/p&gt;
&lt;p&gt;There are a bunch if tutorials on how to build a python distribution on a raspberry pi. &lt;a href="https://gist.github.com/SeppPenner/6a5a30ebc8f79936fa136c524417761d"&gt;Here is one&lt;/a&gt;. Even if you know what you're doing, it takes freaking forever to &lt;code&gt;configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install&lt;/code&gt; the thing. If you do not know what you are doing (I do not know what &lt;em&gt;I&lt;/em&gt; am doing), it'll take you that amount of freaking forever to learn something is wrong and that you have to do a different &lt;code&gt;configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Learn from my mistakes. Use &lt;a href="https://github.com/jjhelmus/berryconda"&gt;Berryconda&lt;/a&gt;. Berryconda is pre-built conda distribution for raspberry pi. It only takes a few minutes to set up and it "just works". Follow the instructions on the github page to install. It should be something like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget https://github.com/jjhelmus/berryconda/releases/download/v2.0.0/Berryconda3-2.0.0-Linux-armv7l.sh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
chmod +x Berryconda3-2.0.0-Linux-armv7l.sh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
./Berryconda3-2.0.0-Linux-armv7l.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That'll walk you through the installation.&lt;/p&gt;
&lt;p&gt;If you installed zsh, you'll want to make sure to put the berryconda binary directory on your path (so that zsh knows what to do when you refer to &lt;code&gt;python&lt;/code&gt;). Add a line like this to your &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$PATH&lt;/span&gt;&lt;span class="s2"&gt;:&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;/berryconda3/bin"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where &lt;code&gt;$HOME/berryconda3/bin&lt;/code&gt; is wherever you decided to install berryconda. Then, you should confirm that your python3 command refers to the berryconda install:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;which python3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Should output something like &lt;code&gt;/home/pi/berryconda3/bin/python3&lt;/code&gt; not &lt;code&gt;/usr/bin/python3&lt;/code&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>An update on the softmax function for numpy.</title><link href="https://nolanbconaway.github.io/blog/2019/softmax-numpy-update.html" rel="alternate"></link><published>2019-01-05T00:00:00-05:00</published><updated>2019-01-05T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-01-05:/blog/2019/softmax-numpy-update.html</id><summary type="html">&lt;p&gt;As of early 2019, my post on a &lt;a href="/blog/2017/softmax-numpy"&gt;softmax function for numpy&lt;/a&gt; accounts for 83% of traffic to my website (~16k views in 2018). I recently found that, as of version 1.2.0, scipy has included an implementation of the softmax in its special functions.&lt;/p&gt;
&lt;p&gt;Some info on the ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;As of early 2019, my post on a &lt;a href="/blog/2017/softmax-numpy"&gt;softmax function for numpy&lt;/a&gt; accounts for 83% of traffic to my website (~16k views in 2018). I recently found that, as of version 1.2.0, scipy has included an implementation of the softmax in its special functions.&lt;/p&gt;
&lt;p&gt;Some info on the change:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html#scipy.special.softmax"&gt;SciPy docs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scipy/scipy/pull/8872"&gt;Merged pull request&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scipy/scipy/pull/8556/commits/02d0ac2dea6bd2ad11ddf6c6022b3bae881c961a#diff-86dbed1918e224062ad4239fe5d14041R188"&gt;Feeding my ego&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Originally, the proposed function looked very much like the one I had posted, but then &lt;a href="https://github.com/pv"&gt;a very smart person&lt;/a&gt; realized that you could use existing tooling around &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html#scipy.special.logsumexp"&gt;scipy.special.logsumexp&lt;/a&gt; to make things much cleaner.&lt;/p&gt;
&lt;p&gt;So now the function is &lt;a href="https://github.com/scipy/scipy/blob/master/scipy/special/_logsumexp.py#L215"&gt;a freaking one-liner&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keepdims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I 100% prefer importing a function from scipy over pasting a hand-written one. Especially given the beauty of that puppy. But the two functions are not totally swappable, there are a couple important changes in behavior:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;SciPy computes the softmax over &lt;em&gt;all&lt;/em&gt; array elements by default&lt;/strong&gt;. It will keep the shape but the &lt;em&gt;entire&lt;/em&gt; result will sum to 1. My old function computed the softmax over the first non-singleton dimension like in MATLAB.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;There is no &lt;code&gt;theta&lt;/code&gt; multiplier&lt;/strong&gt;. In my function, a &lt;code&gt;theta&lt;/code&gt; parameter was accepted to control determinism, but that was not implemented in the scipy function. No biggie, you'll just need to do &lt;code&gt;softmax(X*theta)&lt;/code&gt; instead of &lt;code&gt;softmax(X, theta=theta)&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="a-quick-performance-comparison"&gt;A quick performance comparison&lt;/h2&gt;
&lt;p&gt;I figured scipy had some magic stuff going on with &lt;code&gt;logsumexp&lt;/code&gt;, and their function would be both more beautiful &lt;em&gt;and&lt;/em&gt; performant than mine.&lt;/p&gt;
&lt;p&gt;But I ran a quick handful of tests just for kicks. First I wrote up a decorator to print function exec time to the console and wrapped my softmax and scipy softmax in it.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wraps&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.special&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;timeit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Decorator to print function exec time."""&lt;/span&gt;
    &lt;span class="nd"&gt;@wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;te&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{f.__name__}&lt;/span&gt;&lt;span class="s1"&gt; took: {te-ts:2.4f}s'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrap&lt;/span&gt;


&lt;span class="nd"&gt;@timeit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax_nolan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""My func without those pesky comments and docs."""&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;ax_sum&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;


&lt;span class="nd"&gt;@timeit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax_scipy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Scipy softmax with the axis inference and theta included."""&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;special&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I ran each function on 2D arrays of varying sizes and checked for differences in the results.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sizes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sizes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;arr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Running: &lt;/span&gt;&lt;span class="si"&gt;{size}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result_nolan&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;softmax_nolan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result_scipy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;softmax_scipy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# check for differences&lt;/span&gt;
    &lt;span class="n"&gt;max_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result_scipy&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;result_nolan&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;max_diff&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_diff&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://nolanbconaway.github.io/blog/2019/test.py"&gt;Here is the full script&lt;/a&gt;, in case you want to toy around with it.&lt;/p&gt;
&lt;h3 id="results"&gt;Results&lt;/h3&gt;
&lt;p&gt;My function was ... faster?&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0004&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0162&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0210&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0259&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0132&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0233&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0141&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0198&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2186&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2869&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1633&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2740&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1962&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2792&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;Running&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;softmax_nolan&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3774&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;softmax_scipy&lt;/span&gt; &lt;span class="n"&gt;took&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4439&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;My function runs an exponential one time, whereas scipy runs the array through &lt;code&gt;logsumexp&lt;/code&gt; and then exponentializes again. So maybe that's why we get these results? I don't really know what &lt;code&gt;logsumexp&lt;/code&gt; does under the hood.&lt;/p&gt;
&lt;p&gt;Anyway, I still prefer scipy softmax to reduce the dependency on a handwritten function.&lt;/p&gt;
&lt;p&gt;Some notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I get similar results every time I ran the script.&lt;/li&gt;
&lt;li&gt;I use a cruddy pip install on python 3.7.1. I don't know what would happen if those speedy conda optimizations were in play.&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>A flask app for a thermometer in my apartment.</title><link href="https://nolanbconaway.github.io/blog/2018/a-flask-app-for-a-thermometer-in-my-apartment.html" rel="alternate"></link><published>2018-12-05T00:00:00-05:00</published><updated>2018-12-05T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2018-12-05:/blog/2018/a-flask-app-for-a-thermometer-in-my-apartment.html</id><content type="html"></content><category term="app"></category></entry><entry><title>I scraped the text of every Strongbad Email.</title><link href="https://nolanbconaway.github.io/blog/2018/i-scraped-the-text-of-every-strongbad-email.html" rel="alternate"></link><published>2018-11-04T00:00:00-04:00</published><updated>2018-11-04T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2018-11-04:/blog/2018/i-scraped-the-text-of-every-strongbad-email.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Jarjar 3.0 Released.</title><link href="https://nolanbconaway.github.io/blog/2018/jarjar-30-released.html" rel="alternate"></link><published>2018-06-20T00:00:00-04:00</published><updated>2018-06-20T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2018-06-20:/blog/2018/jarjar-30-released.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Jeff Zemla and I developed Python and Bash code for sending notifications to Slack.</title><link href="https://nolanbconaway.github.io/blog/2017/jeff-zemla-and-i-developed-python-and-bash-code-for-sending-notifications-to-slack.html" rel="alternate"></link><published>2017-06-29T00:00:00-04:00</published><updated>2017-06-29T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-06-29:/blog/2017/jeff-zemla-and-i-developed-python-and-bash-code-for-sending-notifications-to-slack.html</id><content type="html"></content><category term="github"></category></entry><entry><title>What I found in 18,000 Pitchfork album reviews.</title><link href="https://nolanbconaway.github.io/blog/2017/pitchfork-roundup.html" rel="alternate"></link><published>2017-06-17T00:00:00-04:00</published><updated>2017-06-17T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-06-17:/blog/2017/pitchfork-roundup.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Over the Winter of 2016-2017, I scraped over 18,000 reviews published on &lt;a href="http://pitchfork.com/"&gt;Pitchfork&lt;/a&gt;. I published the dataset on Github, and wrote &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/author-autocorrelation.ipynb"&gt;several&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb"&gt;Jupyter&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/artist-development.ipynb"&gt;Notebooks&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/review-score-exploration.ipynb"&gt;exploring&lt;/a&gt; the data. This post provides a discussion about what I found. For a more code-driven approach, check out the notebooks on &lt;a href="https://github.com/nolanbconaway/pitchfork-data"&gt;Github&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Over the Winter of 2016-2017, I scraped over 18,000 reviews published on &lt;a href="http://pitchfork.com/"&gt;Pitchfork&lt;/a&gt;. I published the dataset on Github, and wrote &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/author-autocorrelation.ipynb"&gt;several&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb"&gt;Jupyter&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/artist-development.ipynb"&gt;Notebooks&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/review-score-exploration.ipynb"&gt;exploring&lt;/a&gt; the data. This post provides a discussion about what I found. For a more code-driven approach, check out the notebooks on &lt;a href="https://github.com/nolanbconaway/pitchfork-data"&gt;Github&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As a person who reads a fair amount of music criticism, and as a scientist, I've long been curious about the degree of objectivity in reviews of new releases. There is of course no such thing as an &lt;em&gt;objective&lt;/em&gt; assessment for the quality of a piece music, and reviews are based on the personal taste of each reviewer. But also, readers clearly take something away from music reviews (otherwise why would they be read?), and major publications are thought to have wide influence on the behavior of listeners (though, as far as I can tell, there's no data to back up that claim).&lt;/p&gt;
&lt;p&gt;So, while subjective, reviews are at least &lt;em&gt;interpreted&lt;/em&gt; as authoritative. Setting aside routine subjectivity having to do with taste, the scientist in me wonders about sources of bias observable across many authors. To address my curiosity, I collected over 18,000 reviews published on &lt;a href="http://pitchfork.com/"&gt;Pitchfork&lt;/a&gt; between January 1999 and January 2017. I chose Pitchfork because it is widely viewed as authoritative, publishes a large amount of content (25+ reviews per week), and offers a precise scoring (0.0-10.0) of each album. In this post, I'll describe what I found.&lt;/p&gt;
&lt;h2 id="the-reviews"&gt;The Reviews&lt;/h2&gt;
&lt;p&gt;Since 1998, Pitchfork has published five new reviews every weekday. In 2016, Pitchfork added an additional five reviews on Saturdays, as well as a "Sunday Reviews" section containing long-form articles on a classic albums (one review per week, published on Sundays). They've also occasionally published more targeted series of reviews (such as following the deaths of &lt;a href="http://pitchfork.com/artists/438-david-bowie/"&gt;David Bowie&lt;/a&gt; and &lt;a href="http://pitchfork.com/artists/3397-prince/"&gt;Prince&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The typical review addresses a single release by a specific artist, and is attributed to a single author. All reviews are given a numerical score (0.0-10.0), are labeled by genre, report the record label (if any), and contain several paragraphs of text (the review itself). Since 2003, authors have awarded some releases the title "&lt;em&gt;Best New Music&lt;/em&gt;", which means what you'd think. Best New Music albums are displayed prominently on Pitchfork, and even have their own &lt;a href="http://pitchfork.com/reviews/best/albums/"&gt;page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Album scores and Best New Music awards will be the focus of my analysis, so it's worth taking a look at those distributions.&lt;/p&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2017/score-bmn-hist.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Note: Most high scoring albums that are &lt;em&gt;not&lt;/em&gt; Best New Music are from reviews prior to the advent of Best New Music, or are reviews of classic releases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A little more than 5% of reviews are awarded Best New Music. Most scores lie between 6.4 and 7.8, with the average review getting a score of 7.0. Best New Music is typically awarded to albums scoring greater than 8.4. Scores between 8.1 and 8.4 have a decent shot at Best New Music, but many albums in that range are not given the title. &lt;/p&gt;
&lt;h2 id="statistical-heaping"&gt;Statistical Heaping&lt;/h2&gt;
&lt;p&gt;Inspired by &lt;a href="https://gutterstats.wordpress.com/2015/11/03/are-nfl-officials-biased-with-their-ball-placement/"&gt;this really cool blog post&lt;/a&gt;, I thought about what a reviewer might do if they have a &lt;em&gt;general&lt;/em&gt; sense of an album's score, but they need to pick a &lt;em&gt;specific&lt;/em&gt; value. Is this a 7.8? 7.9? 7.7? The above mentioned post found that NFL officials tend to place the line of scrimmage at tidy yard numbers (i.e., 10s and 5s). The technical term for this is &lt;a href="http://ww2.amstat.org/sections/SRMS/Proceedings/y1958/Patterns%20Of%20Heaping%20In%20The%20Reporting%20Of%20Numerical%20Data.pdf"&gt;statistical heaping&lt;/a&gt;, and its observed commonly in survey data. So maybe Pitchfork reviewers behave similarly? I counted the number of scores at each decimal place (e.g., \( &lt;em&gt;.0, &lt;/em&gt;.1... *.9 \)), here's the result:&lt;/p&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2017/score-anchor-points.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Those diamond markers show what you'd expect if Pitchfork reviewers were totally unbiased. The &lt;em&gt;Uniform Sampling&lt;/em&gt; model is what you'd get if you picked scores at random; the &lt;em&gt;Normal Sampling&lt;/em&gt; model shows what you'd get if you picked scores around a normal distribution  based on the observed scores (\(\mu=7.006, \sigma = 1.294\)). *.0 gets a slight bump in the uniform model because 10.0 is a possibility, but 10.1-10.9 are not. &lt;/p&gt;
&lt;p&gt;Obviously, the tidy, round &lt;em&gt;.0 value is much more frequently chosen than you'd expect given either sampling technique: _It's nearly twice as frequent as &lt;/em&gt;.1_. There's also a slight bump at the &lt;em&gt;0.5 and &lt;/em&gt;.8  marks, but those values are a bit weaker. The point is, Pitchfork reviewers absolutely show the heaping behavior: at least in this sense, the review scores are biased. &lt;/p&gt;
&lt;h2 id="borderline-best-new-music-decisions"&gt;Borderline Best New Music Decisions&lt;/h2&gt;
&lt;p&gt;Still, its important to consider that the impact of the heaping is not &lt;em&gt;huge&lt;/em&gt;. We're taking about a reviewer picking between, for example, a score of 7.0 or 7.1. A more impactful difference lies in the choice to award &lt;em&gt;Best New Music&lt;/em&gt;. As I noted above, while most releases scoring 8.5 or better are given the award, releases scoring between 8.1 and 8.4 have a shot but it is not guaranteed. I've long wondered about how these decisions are made, so I looked into it.&lt;/p&gt;
&lt;p&gt;I reduced the dataset to the borderline Best New Music candidates: there are 1223 in all, 269 (22%) of which are Best New Music. I checked out a bunch of possible explanations for why some get Best New Music and some do not; for full disclosure I'll list them here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are "tougher" authors (who give out lower scores) more or less likely to award Best New Music? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Is an artist's first album more likely be awarded Best New Music? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Are authors less likely to grant the award if they have less expertise in the genre? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Are authors less likely to grant the award if they recently awarded it to another album? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To Pitchfork's credit: all of these would be reasonable biases to observe and I found little evidence of each. I &lt;em&gt;did&lt;/em&gt; find that &lt;a href="https://twitter.com/nolanbconaway/status/875568013050658818"&gt;some genres are more likely to be considered Best New Music&lt;/a&gt;, but I'm not sure if that constitutes bias or just Pitchfork's focus.&lt;/p&gt;
&lt;p&gt;But when I looked into whether some record labels where favored over others, the results were striking. I computed the proportion of borderline cases from each label that were Best new Music; most only had one or two borderlines cases, but here are the labels with at least ten:&lt;/p&gt;
&lt;p&gt;&lt;object data="https://nolanbconaway.github.io/blog/2017/borderline-by-label.svg" type="image/svg+xml"&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;The first thing to note is that the labels are all major labels or at least big-name indie labels (with the exception of "self-released"). The second thing to note is the huge degree of differences between labels: &lt;a href="https://en.wikipedia.org/wiki/4AD"&gt;4AD&lt;/a&gt; got Best New Music on 8/14 borderline cases, and &lt;a href="https://en.wikipedia.org/wiki/Thrill_Jockey"&gt;Thrill Jockey&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Relapse_Records"&gt;Relapse&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Nonesuch_Records"&gt;Nonesuch&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Anti-_(record_label)"&gt;Anti-&lt;/a&gt; collectively went 0 out of 44. &lt;/p&gt;
&lt;p&gt;That &lt;em&gt;feels&lt;/em&gt; unlikely, but are those differences routine? I sent each label's data through an unbiased model, where the probability of \(k\) Best New Musics out of \(n\) borderline cases follows a Binomial distribution, with \( p=0.22 \) (the overall proportion of Best New Music among borderline cases). This model (depicted in the figure) shows how likely each label's data is &lt;em&gt;individually&lt;/em&gt;, but it does not really address the question: How probable is the data &lt;em&gt;collectively&lt;/em&gt;?  How likely are we to observe four labels with zero Best New Music? How likely are we to observe four labels with more than 40% Best new Music? How likely are we to observe both scenarios simultaneously?&lt;/p&gt;
&lt;p&gt;So I did some Monte Carlo sampling. I simulated each label's Best New Music record 1 million times using the above Binomial distributions. Out of the 1,000,000 samples, 4523 (0.4523%) contained four or more labels with zero Best New Music, 22469 (2.2469%) contained four or more labels with more than 30% Best New Music, and only &lt;em&gt;42&lt;/em&gt; (&lt;em&gt;0.0042%&lt;/em&gt;) had both. Obviously, the data we have is &lt;em&gt;very unlikely&lt;/em&gt; to occur, assuming each label were awarded Best New Music with equal probability.&lt;/p&gt;
&lt;p&gt;Unfortunately, I do not have a clear idea as to why record labels are treated differently. I certainty wouldn't go as far as to suggest that there is &lt;em&gt;overt&lt;/em&gt; favoritism. My best theory is that maybe authors &lt;em&gt;expect&lt;/em&gt; Best New Music from some labels and they do not expect it from others. These expectations wouldn't usually influence decisions because most releases are clearly Best new Music or they are not, but they emerge in borderline cases. &lt;a href="mailto:nolanbconaway@gmail.com"&gt;Let me know&lt;/a&gt; if you can think of a way to test that theory!&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;The reason why I was interested in revealing these sources of bias is that music reviews carry with them a sense of authority: that the favorability of a review is in some sense objective. But, obviously, authors are people, and &lt;a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases#Decision-making.2C_belief.2C_and_behavioral_biases"&gt;people are biased&lt;/a&gt;. So it's no surprise that, once you get digging into the data, you can find evidence of all sorts of biases. &lt;/p&gt;
&lt;p&gt;To their credit, in this post I did &lt;em&gt;not&lt;/em&gt; report many of the analyses I conducted which uncovered no evidence of bias (like &lt;a href="https://twitter.com/nolanbconaway/status/873754026080436224"&gt;this one&lt;/a&gt;, or &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/reviewer-development.ipynb"&gt;this one&lt;/a&gt;, or &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb"&gt;this one&lt;/a&gt;). Of course, that's not exactly evidence that there is &lt;em&gt;no&lt;/em&gt; bias. But, Pitchfork reviewers are professionals, and my guess is that many of them have considered these sorts of biases before, and may even attempt to combat their influence.&lt;/p&gt;
&lt;p&gt;As always, feel free to &lt;a href="mailto:nolanbconaway@gmail.com"&gt;get in touch&lt;/a&gt; if you have comments/questions, or even if you're just curious about some aspect of the data and you don't want to do the analysis yourself.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>I logged the stats for every ride I made in RollerCoaster Tycoon for iPad.</title><link href="https://nolanbconaway.github.io/blog/2017/i-logged-the-stats-for-every-ride-i-made-in-rollercoaster-tycoon-for-ipad.html" rel="alternate"></link><published>2017-05-23T00:00:00-04:00</published><updated>2017-05-23T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-05-23:/blog/2017/i-logged-the-stats-for-every-ride-i-made-in-rollercoaster-tycoon-for-ipad.html</id><content type="html"></content><category term="github"></category></entry><entry><title>A softmax function for numpy.</title><link href="https://nolanbconaway.github.io/blog/2017/softmax-numpy.html" rel="alternate"></link><published>2017-03-15T00:00:00-04:00</published><updated>2017-03-15T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-03-15:/blog/2017/softmax-numpy.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;h4 id="update-jan-2019-scipy-120-now-includes-the-softmax-as-a-special-function-its-really-slick-use-it-here-are-some-notes"&gt;Update (Jan 2019): SciPy (1.2.0) now includes the softmax as a special function. It's really slick. Use it. &lt;a href="/blog/2019/softmax-numpy-update"&gt;Here are some notes&lt;/a&gt;.&lt;/h4&gt;
&lt;/blockquote&gt;
&lt;hr/&gt;
&lt;p&gt;I use the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; function &lt;em&gt;constantly&lt;/em&gt;. It's handy anytime I need to model choice among a set of mutually exclusive options. In the canonical example, you ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;h4 id="update-jan-2019-scipy-120-now-includes-the-softmax-as-a-special-function-its-really-slick-use-it-here-are-some-notes"&gt;Update (Jan 2019): SciPy (1.2.0) now includes the softmax as a special function. It's really slick. Use it. &lt;a href="/blog/2019/softmax-numpy-update"&gt;Here are some notes&lt;/a&gt;.&lt;/h4&gt;
&lt;/blockquote&gt;
&lt;hr/&gt;
&lt;p&gt;I use the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; function &lt;em&gt;constantly&lt;/em&gt;. It's handy anytime I need to model choice among a set of mutually exclusive options. In the canonical example, you have some metric of evidence, \(X = \{ X_1, X_2, ... X_n\} \), that an item belongs to each of \(N\) classes: \( C = \{C_1, C_2, ... C_n\} \). \(X\) can only belong to one class, and larger values indicate more evidence for class membership. So you need to convert the relative amounts of evidence into probabilities of membership within each of the classes.&lt;/p&gt;
&lt;p&gt;That's what the softmax function is for. Below I have written the mathematics, but idea is simple: you divide each element of \(X\) by the sum of all the elements:&lt;/p&gt;
&lt;p&gt;$$
p(C_n) =
\frac{ \exp{\theta \cdot X_n} }
{ \sum_{i=1}^{N}{\exp {\theta \cdot X_i} } }
$$&lt;/p&gt;
&lt;p&gt;The use of exponentials serves to normalize \(X\), and it also allows the function to be parameterized. In the above equation, I threw in a free parameter, \(\theta\) (\(\theta \geq 0\)), that broadly controls &lt;em&gt;determinism&lt;/em&gt;. Within the exponentiation, \(\theta\) makes larger values of  \(X\) larger-er, so if you set \(\theta\) to a large value, the softmax really squashes things: Elements of \(X\) with anything but the largest values will have a probability very close to zero. When \(\theta = 1\), it's as if the parameter was never there.&lt;/p&gt;
&lt;p&gt;I use this sort of function all the time to simulate how people make decisions based on evidence. But unfortunately, there is no built in &lt;code&gt;numpy&lt;/code&gt; function to compute the softmax. For years I have been writing code like this:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# evidence for each choice&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;                         &lt;span class="c1"&gt;# determinism parameter&lt;/span&gt;

&lt;span class="n"&gt;ps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ps&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Of course, usually &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;theta&lt;/code&gt; come from somewhere else. This works well if you are only simulating one decision: the softmax requires literally two lines of code and its easily readable. But things get thornier if you want to simulate many choices. For example, what if &lt;code&gt;X&lt;/code&gt; is a matrix where rows correspond to the different choices, and the columns correspond to the options?&lt;/p&gt;
&lt;p&gt;In the 2D case, you can either run a loop through the rows of &lt;code&gt;X&lt;/code&gt; or use &lt;code&gt;numpy&lt;/code&gt; matrix &lt;a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"&gt;broadcasting&lt;/a&gt;. Here's the loop solution:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;6.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;9.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;])&lt;/span&gt;  

&lt;span class="c1"&gt;# looping through rows of X&lt;/span&gt;
&lt;span class="n"&gt;ps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's not &lt;em&gt;terrible&lt;/em&gt;, but you can imagine that it's annoying to write one of those &lt;em&gt;every time&lt;/em&gt; you need to softmax. Likewise, you'd have to change up the code if you wanted to softmax over columns rather than rows. Or for that matter, what if &lt;code&gt;X&lt;/code&gt; was a 3D-array, and you wanted to compute softmax over the third dimension?&lt;/p&gt;
&lt;p&gt;At this point it feels more useful to write a generalized softmax function.&lt;/p&gt;
&lt;h2 id="my-softmax-function"&gt;My softmax function&lt;/h2&gt;
&lt;p&gt;After years of copying one-off softmax code between scripts, I decided to make things a little &lt;a href="https://en.wikipedia.org/wiki/Don't_repeat_yourself"&gt;dry&lt;/a&gt;-er: I sat down and wrote a darn softmax function. The goal was to support \(X\) of any dimensionality, and to allow the user to softmax over an arbitrary axis. Here's the function:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Compute the softmax of each element along an axis of X.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    X: ND-Array. Probably should be floats.&lt;/span&gt;
&lt;span class="sd"&gt;    theta (optional): float parameter, used as a multiplier&lt;/span&gt;
&lt;span class="sd"&gt;        prior to exponentiation. Default = 1.0&lt;/span&gt;
&lt;span class="sd"&gt;    axis (optional): axis to compute values along. Default is the&lt;/span&gt;
&lt;span class="sd"&gt;        first non-singleton axis.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns an array the same size as X. The result will sum to 1&lt;/span&gt;
&lt;span class="sd"&gt;    along the specified axis.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;

    &lt;span class="c1"&gt;# make X at least 2d&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# find axis&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# multiply y against the theta parameter,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# subtract the max for numerical stability&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# exponentiate y&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# take the sum along the specified axis&lt;/span&gt;
    &lt;span class="n"&gt;ax_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# finally: divide elementwise&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;ax_sum&lt;/span&gt;

    &lt;span class="c1"&gt;# flatten if X was 1D&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="whats-up-with-that-max-subtraction"&gt;What's up with that max subtraction?&lt;/h2&gt;
&lt;p&gt;The only aspect of this function that does not directly correspond to something in the softmax equation is the subtraction of the maximum from each of the elements of &lt;code&gt;X&lt;/code&gt;. This is done for stability reasons: when you exponentiate even large-ish numbers, the result can be quite large. &lt;code&gt;numpy&lt;/code&gt; will return &lt;code&gt;inf&lt;/code&gt; when you exponentiate values over 710 or so. So if values of &lt;code&gt;X&lt;/code&gt; aren't limited to some fixed range (e.g., \([0...1]\)), or even if you let &lt;code&gt;theta&lt;/code&gt; take on any value, you run the distinct possibility of hitting the &lt;code&gt;inf&lt;/code&gt; ceiling.&lt;/p&gt;
&lt;p&gt;But! If you subtract the maximum value from each element, the largest pre-exponential value will be zero, thus avoiding numerical instability.&lt;/p&gt;
&lt;p&gt;So that solves the numerical stability problem, but is it &lt;em&gt;mathematically&lt;/em&gt; correct? To clear this up, let's write out the softmax equation with the subtraction terms in there. To keep it simple, I've also removed the \(\theta\) parameter:&lt;/p&gt;
&lt;p&gt;$$
p(C_n) =
\frac{ \exp{ X_n - \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i - \max(X) } } }
$$&lt;/p&gt;
&lt;p&gt;Subtracting within an exponent is the same as dividing between exponents (&lt;a href="http://www.rapidtables.com/math/number/exponent.htm"&gt;remember&lt;/a&gt;? \(e^{a-b} = e^a / e^b\)), so:&lt;/p&gt;
&lt;p&gt;$$
\frac{ \exp{ X_n - \max(X) }  }
{ \sum_{i=1}^{N}{\exp { X_i - \max(X) } } }
= \frac{ \exp { X_n  } \div \exp { \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i } \div \exp { \max(X) } } }
$$&lt;/p&gt;
&lt;p&gt;Then you just cancel out the maximum terms, and you're left with the original equation:&lt;/p&gt;
&lt;p&gt;$$
\frac{ \exp { X_n  } \div \exp { \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i } \div \exp { \max(X) } } } =
\frac{ \exp{ X_n } }
{ \sum_{i=1}^{N}{\exp { X_i } } }
$$&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;The function works beautifully and has a nice safeguard against overflow in the exponential. And, if you're like me, including it will prevent you from writing a handful of one-off implementations of the softmax. I'll round this out with a few examples of its usage:&lt;/p&gt;
&lt;h3 id="2d-usage"&gt;2D Usage&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;6.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;9.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# softmax over rows&lt;/span&gt;
&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.055&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.407&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.015&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.413&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.822&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.165&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.395&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.152&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.428&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.59&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.435&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="c1"&gt;# softmax over columns&lt;/span&gt;
&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.031&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.22&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.054&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.695&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.204&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.039&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.645&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.112&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.022&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.072&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.68&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.226&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="c1"&gt;# softmax over columns, and squash it!&lt;/span&gt;
&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;500.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="3d-and-beyond"&gt;3D (and beyond!)&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mf"&gt;0.844&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.237&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.364&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.768&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.811&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.959&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.511&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.06&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.594&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.029&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.963&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.292&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.463&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.869&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.704&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.786&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.173&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.89&lt;/span&gt; &lt;span class="p"&gt;]]])&lt;/span&gt;


&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mf"&gt;0.575&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.425&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.45&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.55&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.482&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.518&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

           &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.556&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.444&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.57&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.43&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.583&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.417&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

           &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.449&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.551&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.49&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.51&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.411&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.589&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="blog"></category></entry><entry><title>Solving nonlinearly separable classifications in a single layer neural network.</title><link href="https://nolanbconaway.github.io/blog/2017/solving-nls.html" rel="alternate"></link><published>2017-01-15T00:00:00-05:00</published><updated>2017-01-15T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-01-15:/blog/2017/solving-nls.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Recently, &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt; (my graduate advisor) and I figured out a unique solution to the famous limitation that single-layer neural networks cannot solve nonlinearly separable classifications. We published our findings in &lt;a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00931"&gt;Neural Computation&lt;/a&gt;. This post is intended to provide a more introductory-level description of our solution. Read the paper for ‚Ä¶&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Recently, &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt; (my graduate advisor) and I figured out a unique solution to the famous limitation that single-layer neural networks cannot solve nonlinearly separable classifications. We published our findings in &lt;a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00931"&gt;Neural Computation&lt;/a&gt;. This post is intended to provide a more introductory-level description of our solution. Read the paper for a more formal report!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In an &lt;a href="https://en.wikipedia.org/wiki/Perceptrons_(book)"&gt;influential book&lt;/a&gt; published in 1969,  Marvin Minsky and Seymour Papert proved that the conventional neural networks of the day could not solve nonlinearly separable (NLS) classifications.&lt;/p&gt;
&lt;p&gt;Their conclusions spurred a decline in research on neural network models during the following two decades. It wasn't until the popularization of the &lt;a href="https://en.wikipedia.org/wiki/Backpropagation"&gt;backpropagation algorithm&lt;/a&gt; in the 1980s, which enabled models to solve NLS classifications through learning an additional layer of weights, that interest picked back up. But even after backprop, and after the advent of methods to train up &lt;a href="https://en.wikipedia.org/wiki/Deep_learning"&gt;deep networks&lt;/a&gt;, the conventional wisdom has been that the only way to solve a NLS classification is by &lt;em&gt;adding layers of weights&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Recently, &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt; (my graduate advisor) and I figured out how you can solve NLS classifications with only a single layer of weights. In this post, I'll explain how!&lt;/p&gt;
&lt;h2 id="what-is-a-nonlinearly-separable-classification"&gt;What is a nonlinearly separable classification?&lt;/h2&gt;
&lt;p&gt;Nonlinearly separable classifications are most straightforwardly understood through contrast with linearly separable ones: if a classification is linearly separable, you can draw a line to separate the classes. &lt;/p&gt;
&lt;p&gt;Below is an example of each. Imagine you are trying to discriminate between two classes, A and B, on the basis of two input dimensions (\(D_1\) and \(D_2\)):&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" src="https://nolanbconaway.github.io/blog/2017/ls-nls.png"/&gt;&lt;/p&gt;
&lt;p&gt;The NLS problem above is the ubiquitous &lt;a href="https://en.wikipedia.org/wiki/Exclusive_or"&gt;Exclusive-Or&lt;/a&gt; (XOR) problem. Whereas you can easily separate the LS classes with a line, this task is not possible for the NLS problem. &lt;/p&gt;
&lt;h2 id="why-cant-a-single-layer-network-solve-nls-classifications"&gt;Why &lt;em&gt;can't&lt;/em&gt; a single layer network solve NLS classifications?&lt;/h2&gt;
&lt;p&gt;Drawing a line between the classes is like solving the classification with regression: the aim is to find the boundary the best separates the members of the two classes. And as it turns out, the traditional single-layer classifier is identical to a regression model:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="150" src="https://nolanbconaway.github.io/blog/2017/traditional-perceptron.png"/&gt;&lt;/p&gt;
&lt;p&gt;The network has two input units, \(D_1\) and \(D_2\), a bias unit, and a single output unit coding the response (say, 0 codes for A, and 1 codes for B). The inputs are connected to the outputs with weights, \(W_1\) and \(W_2\). So the model's output is given by:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
output &amp;amp; = D_1W_1 + D_2W_2 + bias
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Of course, that's also the canonical formula for regression. \(W_1\) and \(W_2\) are the slopes, and the bias is the intercept. The learning objective is to find the values of \(W_1\), \(W_2\), and bias that produce values of 0 for items in category A, and 1 for items in category B.&lt;/p&gt;
&lt;p&gt;The above architecture is just a specific case of the more general multi-class network:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="150" src="https://nolanbconaway.github.io/blog/2017/traditional-multiclass-perceptron.png"/&gt;&lt;/p&gt;
&lt;p&gt;Where each class (again, A and B) gets its own output node. I've color-coded the weights so you can see how this is just two versions of the earlier network, put next to each other. So now output for each unit \(c\) is the sum of the product of each dimension \(D_k\) multiplied against the dimension's weight to the unit, \(W_{ck}\):&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
output_c &amp;amp; = bias + \Sigma_k{D_k W_{ck}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;In this more general case, it's useful to store the weights in an array to take advantage of matrix algebra to compute the output:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

                    &lt;span class="c1"&gt;# bias  D_1  D_2  # Correct Category&lt;/span&gt;
&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# A&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# A&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# B&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# B&lt;/span&gt;
    &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# A perfect solution to the LS problem!&lt;/span&gt;
&lt;span class="c1"&gt;#                  A    B&lt;/span&gt;
&lt;span class="n"&gt;wts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c1"&gt;# bias&lt;/span&gt;
                &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c1"&gt;# W_1&lt;/span&gt;
                &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c1"&gt;# W_2&lt;/span&gt;
            &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# output = &lt;/span&gt;
&lt;span class="c1"&gt;#      A   B&lt;/span&gt;
&lt;span class="c1"&gt;#   [[ 1.  0.]&lt;/span&gt;
&lt;span class="c1"&gt;#    [ 1.  0.]&lt;/span&gt;
&lt;span class="c1"&gt;#    [ 0.  1.]&lt;/span&gt;
&lt;span class="c1"&gt;#    [ 0.  1.]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: for simplicity, bias is handled as an extra input unit that always has value 1. The real value of bias is specified in its weights. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But, again, this is identical to the simpler case, we're just doing it twice (once for &lt;code&gt;A&lt;/code&gt; and once for &lt;code&gt;B&lt;/code&gt;), and using matrix algebra for the computation in the equation above. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anyway&lt;/em&gt;, the goal, of course, is to find values for \(W\) that correctly predict the category. But since this is akin to separating the categories with a line (which is what regression does), you'll never find useful values for an NLS problem: by definition, you cannot separate the classes with a line.&lt;/p&gt;
&lt;h2 id="how-to-solve-the-nls-classification"&gt;How to solve the NLS classification&lt;/h2&gt;
&lt;p&gt;You definitely can't solve the NLS problem with any of the architectures above. So, the solution we came up with involves a re-envisioning of the problem, which is realized in a change to the architecture. Instead of the traditional perceptron architecture with dimensions-as-inputs and categories-as-outputs, we designed an autoassociator-based classifier:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="200" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc.png"/&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: That architecture is actually a single-layer version of Ken's &lt;a href="https://link.springer.com/article/10.3758/BF03196806"&gt;Divergent Autoencoder&lt;/a&gt; model, which is also very cool!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've color-coded the weights so that it doesn't look like a tangled mess. Instead of having one output unit per category, the network has one output unit per category, per dimension. The output units are split into two, category-specific, channels, which is why we call it a "&lt;em&gt;Divergent Autoassociator&lt;/em&gt;".&lt;/p&gt;
&lt;p&gt;You could also imagine that network as two separate, regular autoassociators, which is formally the same (it just doesn't &lt;em&gt;look&lt;/em&gt; divergent):&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="200" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc-split.png"/&gt;&lt;/p&gt;
&lt;p&gt;The network's goal is to "reconstruct" everything it sees in the correct category. So if \(D_1=0\) and \(D_2=1\) (i.e., &lt;code&gt;input = [0,1]&lt;/code&gt;), and the network is told that belongs to category A, then the goal is for \(A_1=0\) and \(A_2=1\). The units on the opposite-category channel (\(B_1\) and \(B_2\)) are left alone.&lt;/p&gt;
&lt;p&gt;Based on that learning objective, the network needs to learn weights that accurately reconstruct each category's items on the correct channel. This produces a network which learns how dimensions are correlated within each category, rather than a network which learns how dimensions predict categories.&lt;/p&gt;
&lt;h3 id="classification-with-the-divergent-autoassociator"&gt;Classification with the Divergent Autoassociator&lt;/h3&gt;
&lt;p&gt;Getting the model to classify things is about comparing the amount of &lt;em&gt;error&lt;/em&gt; on each category channel. If, for example, the category A channel does a good job reconstructing &lt;code&gt;[0, 1]&lt;/code&gt;, and category B does a poor job, then the model will classify the item into category A.&lt;/p&gt;
&lt;p&gt;Just to provide a sense of how the classification rule works, I calculated the optimal weights for the LS problem, and I copied over the network's output using those weights. Then, based on the output from each category, I computed the error and classification response.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# outputs copied from an optimal LS solution&lt;/span&gt;
&lt;span class="n"&gt;output_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;output_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;error_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;output_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;error_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;output_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;classification&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;error_B&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;error_A&lt;/span&gt;
&lt;span class="c1"&gt;# classification = &lt;/span&gt;
&lt;span class="c1"&gt;#   [0 0 1 1]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="the-single-layer-nls-solution"&gt;The Single-Layer NLS Solution&lt;/h3&gt;
&lt;p&gt;So "solving" a classification in the Divergent Autoassociator involves finding a set of weights that allow each class to reconstruct its own members, but poorly reconstruct members of the other class. Here's how it's done:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="275" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc-split-solution.png"/&gt;&lt;/p&gt;
&lt;p&gt;To prevent things from looking like a tangled mess, I've used the two-separate-autoassociators figure. But remember, this is just a single network! I've put the optimal weight values next to each weight.&lt;/p&gt;
&lt;p&gt;Those weights result in the following outputs:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;\(D_1\)&lt;/th&gt;
&lt;th&gt;\(D_2\)&lt;/th&gt;
&lt;th&gt;\(A_1\)&lt;/th&gt;
&lt;th&gt;\(A_2\)&lt;/th&gt;
&lt;th&gt;\(B_1\)&lt;/th&gt;
&lt;th&gt;\(B_2\)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can see that the network has "solved" the NLS problem: items in category A are perfectly reconstructed by the A channel, but not in the B channel, and vice-versa. Here's the math for the final line: \(D_1\)  = 1, \(D_2\) = 0&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
A_1 = D_1 \cdot 0.5 + D_2 \cdot 0.5 + bias \\
    = 1.0 \cdot 0.5 + 0.0 \cdot 0.5 + 0\\
    = 0.5 + 0.0 + 0\\
    = 0.5 \\
    \\
A_2 = D_1 \cdot 0.5 + D_2 \cdot 0.5 + bias\\
    = 1.0 \cdot 0.5 + 0.0 \cdot 0.5 + 0\\
    = 0.5 + 0.0 + 0\\
    = 0.5 \\
\end{align}
$$&lt;/p&gt;
&lt;p&gt;So the weights going to each category A output are identical. This shows that the network learned a key within-category regularity: \(D_1\) = \(D_2\). That is, you can always predict the value of \(D_1\) based on \(D_2\), because within category A the dimensions are positively correlated.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
B_1 = D_1 \cdot 0.67 + D_2 \cdot -0.33 + bias\\
    = 1.0 \cdot 0.67 + 0.0 \cdot -0.33 + 0.33\\
    = 0.67 + 0.0 + 0.33\\
    = 1.0 \\
\\
B_2 = D_1 \cdot -0.33 + D_2 \cdot 0.67 + bias\\
    = 1.0 \cdot -0.33 + 0.0 \cdot 0.67 + 0.33\\
    = -0.33 + 0.0 + 0.33\\
    = 0.0\\
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The key difference in the category B channel is that the &lt;em&gt;cross-dimension&lt;/em&gt; weights (e.g., going from \(D_1\) to \(B_2\)) are negative, showing that the network has learned the opposite pattern in category B: \(D_1\) will be the opposite of \(D_2\).&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;The Divergent Autoassociator solves the NLS classification without actually learning how to classify anything! Really, the model sidesteps the problem: first, it learns how to predict dimensions within each category, then it classifies items based on their consistency with what has been learned.&lt;/p&gt;
&lt;p&gt;It doesn't do anything &lt;em&gt;that&lt;/em&gt; differently from the traditional architecture: we didn't add a new computational step, we didn't add preprocessing, we didn't add a hidden layer. But by looking at the computational problem from a different angle, a new solution arises.&lt;/p&gt;
&lt;h1 id="explore-it-yourself"&gt;Explore it yourself!&lt;/h1&gt;
&lt;p&gt;Along with this post, I've written a couple simple python classes to explore learning in the traditional perceptron and divergent autoassociator models. They're probably not what you want to use for any rigorous study (they don't even &lt;em&gt;classify&lt;/em&gt; things), but they're enough to explore what kinds of weight solutions are learned.&lt;/p&gt;
&lt;p&gt;The classes can be found &lt;a href="https://nolanbconaway.github.io/blog/2017/single_layer_nets.zip"&gt;here&lt;/a&gt;. You can download them and import as a module like so:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_printoptions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;precision&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# import network objects&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;single_layer_nets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;perceptron&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;single_layer_nets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;autoassociator&lt;/span&gt;

&lt;span class="c1"&gt;# function to make 3d array printing prettier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;single_layer_nets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;printattr&lt;/span&gt; 

&lt;span class="c1"&gt;# define classes&lt;/span&gt;
&lt;span class="n"&gt;NLS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;description&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'NLS'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;LS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;  &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;description&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'LS'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NLS&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;' ----------- '&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'description'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Inputs:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perceptron&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;autoassociator&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;

        &lt;span class="c1"&gt;# initialize network model&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;' output:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;printattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;' weights:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;printattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="blog"></category></entry><entry><title>I'll be at MathPsych and CogSci 2016</title><link href="https://nolanbconaway.github.io/blog/2016/summer-conferences.html" rel="alternate"></link><published>2016-07-15T00:00:00-04:00</published><updated>2016-07-15T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2016-07-15:/blog/2016/summer-conferences.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here's what I'll be up to! Also check out my &lt;a href="https://nolan.shinyapps.io/whos-at-cogsci/"&gt;Shiny App&lt;/a&gt; to see who else is presenting at CogSci.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="mathpsych-new-brunswick-nj"&gt;MathPsych (New Brunswick, NJ)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I'm giving a talk on some joint work with &lt;a href="https://alab.psych.wisc.edu"&gt;Joe Austerweil&lt;/a&gt; (with whom I'll be working as a postdoc!) and &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="cogsci-philadelphia-pa"&gt;CogSci (Philadelphia, PA)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/manuscripts/conaway-kurtz-cogsci2016.pdf"&gt;PDF ‚Ä¶&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here's what I'll be up to! Also check out my &lt;a href="https://nolan.shinyapps.io/whos-at-cogsci/"&gt;Shiny App&lt;/a&gt; to see who else is presenting at CogSci.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="mathpsych-new-brunswick-nj"&gt;MathPsych (New Brunswick, NJ)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I'm giving a talk on some joint work with &lt;a href="https://alab.psych.wisc.edu"&gt;Joe Austerweil&lt;/a&gt; (with whom I'll be working as a postdoc!) and &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="cogsci-philadelphia-pa"&gt;CogSci (Philadelphia, PA)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/manuscripts/conaway-kurtz-cogsci2016.pdf"&gt;PDF&lt;/a&gt;&lt;/strong&gt;] I'm giving a talk on a paper I coauthored with Ken Kurtz.&lt;/li&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/manuscripts/honke-conaway-kurtz-cogsci2016.pdf"&gt;PDF&lt;/a&gt;&lt;/strong&gt;] &lt;a href="http://bingweb.binghamton.edu/~ghonke1/"&gt;Garrett Honke&lt;/a&gt; is giving a talk on a paper we coauthored with Ken Kurtz.&lt;/li&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/posters/cogsci2016.pdf"&gt;PDF&lt;/a&gt;&lt;/strong&gt;] &lt;a href="https://www.marist.edu/sbs/facviewer.html?uid=487"&gt;Kimery Levering&lt;/a&gt; and I are presenting a poster on some research we conducted with Ken Kurtz.&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Who's At Cogsci? 2016.</title><link href="https://nolanbconaway.github.io/blog/2016/whos-at-cogsci-2016.html" rel="alternate"></link><published>2016-07-01T00:00:00-04:00</published><updated>2016-07-01T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2016-07-01:/blog/2016/whos-at-cogsci-2016.html</id><content type="html"></content><category term="app"></category></entry></feed>