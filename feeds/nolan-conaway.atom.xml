<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Nolan Conaway's Blog - Nolan Conaway</title><link href="https://nolanbconaway.github.io/" rel="alternate"></link><link href="https://nolanbconaway.github.io/feeds/nolan-conaway.atom.xml" rel="self"></link><id>https://nolanbconaway.github.io/</id><updated>2022-02-01T00:00:00-05:00</updated><entry><title>I wrote another scraper for pitchfork reviews</title><link href="https://nolanbconaway.github.io/blog/2022/i-wrote-another-scraper-for-pitchfork-reviews.html" rel="alternate"></link><published>2022-02-01T00:00:00-05:00</published><updated>2022-02-01T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2022-02-01:/blog/2022/i-wrote-another-scraper-for-pitchfork-reviews.html</id><content type="html"></content><category term="github"></category></entry><entry><title>2021: The high and low temperatures</title><link href="https://nolanbconaway.github.io/blog/2022/2021-the-high-and-low-temperatures.html" rel="alternate"></link><published>2022-01-29T00:00:00-05:00</published><updated>2022-01-29T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2022-01-29:/blog/2022/2021-the-high-and-low-temperatures.html</id><summary type="html">&lt;p&gt;Three years ago I hooked up a thermometer to a Raspberry Pi, and I set up a cron job to save the temperature in my postgres database every minute. Since then, I've posted year-in-review style articles once annually (&lt;a href="/blog/2020/2019-the-high-and-low-temperatures"&gt;2019&lt;/a&gt;, &lt;a href="/blog/2021/2020-the-high-and-low-temperatures"&gt;2020&lt;/a&gt;) describing the data I had obtained from running the cron ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;Three years ago I hooked up a thermometer to a Raspberry Pi, and I set up a cron job to save the temperature in my postgres database every minute. Since then, I've posted year-in-review style articles once annually (&lt;a href="/blog/2020/2019-the-high-and-low-temperatures"&gt;2019&lt;/a&gt;, &lt;a href="/blog/2021/2020-the-high-and-low-temperatures"&gt;2020&lt;/a&gt;) describing the data I had obtained from running the cron job for a full calendar year. &lt;/p&gt;
&lt;p&gt;This is another one of those posts!&lt;/p&gt;
&lt;p&gt;(FYI- You can still check out a real time view of the data via &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/"&gt;my heroku webapp&lt;/a&gt;.)&lt;/p&gt;
&lt;h2 id="the-warmest-day"&gt;üî• the warmest day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2022/warmest.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;Usually I record the warmest daily temperatures in early July, when summer is in full swing and we've shut off the AC over the July 4 weekend. This year we stuck around NYC for July 4, and instead are looking at June 7 for the warmest day  üíÅ.&lt;/p&gt;
&lt;h2 id="the-coldest-day"&gt;‚ùÑ the coldest day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2022/coldest.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;2020's coldest day in the apartment was December 31. Like with the warmest day, we had left town for the holiday and shut off the heat. Interestingly (??), the coldest day in 2021 was &lt;em&gt;the very next day&lt;/em&gt;: January 1, 2021.&lt;/p&gt;
&lt;h2 id="the-most-variable-day"&gt;üìà The most variable day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2022/variable.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;Our apartment is southwestern facing, and has floor-to-ceiling windows. We get &lt;em&gt;a lot&lt;/em&gt; of sunlight. As a result, I find that the days with the most variance in temperature are sunny days in Spring/Autumn, when temperatures start out cold but the sun quickly heats up the apartment.&lt;/p&gt;
&lt;p&gt;March 7, the most variable day of 2021, is one of those such cases: the day started out at a chilly 57‚Ñâ but rose to a toasty 81‚Ñâ in the span of five hours!&lt;/p&gt;
&lt;h2 id="all-together"&gt;All together!&lt;/h2&gt;
&lt;p&gt;I found it interesting to plot these days on the same axis so that the changes in temperature over time can be appreciated. Take a look!&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2022/everybody.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;üëã cya in 2023!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>I switched telecom providers and have the data</title><link href="https://nolanbconaway.github.io/blog/2021/i-switched-telecom-providers-and-have-the-data.html" rel="alternate"></link><published>2021-10-09T00:00:00-04:00</published><updated>2021-10-09T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2021-10-09:/blog/2021/i-switched-telecom-providers-and-have-the-data.html</id><summary type="html">&lt;p&gt;In 2019 I started using the python &lt;a href="https://github.com/sivel/speedtest-cli"&gt;speedtest-cli&lt;/a&gt; to log network stats to a MySQL database. The original purpose was to visualize the data in a &lt;a href="https://nolans-network-speed.herokuapp.com/today"&gt;home network speed webapp&lt;/a&gt; so that I could check on my network every now and again.&lt;/p&gt;
&lt;p&gt;Charter Spectrum had been my service provider for ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;In 2019 I started using the python &lt;a href="https://github.com/sivel/speedtest-cli"&gt;speedtest-cli&lt;/a&gt; to log network stats to a MySQL database. The original purpose was to visualize the data in a &lt;a href="https://nolans-network-speed.herokuapp.com/today"&gt;home network speed webapp&lt;/a&gt; so that I could check on my network every now and again.&lt;/p&gt;
&lt;p&gt;Charter Spectrum had been my service provider for a long time, and casually viewing the app frequently revealed service throttling. My contract was for 200mbits download, and there were often hours during the day where my service was throttled beyond 50% of the contract speed.&lt;/p&gt;
&lt;p&gt;Here's an example:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/spectrum.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;Deeply uncool. üòÅ&lt;/p&gt;
&lt;p&gt;So, I switched providers as soon as Verizon Fios set up shop in my neighborhood. This post raises the question: &lt;em&gt;Is Fios any better?&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="the-data"&gt;The data&lt;/h2&gt;
&lt;p&gt;I append speedtest-cli stats to my database every 15 minutes within an airflow job which runs on my raspberry pi. The raspberry pi is connected directly to my router via ethernet and ought to have speeds as fast as I'll get on my network.&lt;/p&gt;
&lt;p&gt;I aggregated the speeds to the hour (so each point is the average of four samples) and plotted the timeseries:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/raw.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;You can notice several things from this view:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;My contract speed went up to 300 mbits when I switched to Fios (&lt;em&gt;and&lt;/em&gt; I am paying less üòâ).&lt;/li&gt;
&lt;li&gt;Even Fios does not provide its full speed at all times&lt;/li&gt;
&lt;li&gt;There are obviously fewer throttled periods in the Fios section.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fios is visibly more reliable, but &lt;em&gt;how much more reliable&lt;/em&gt;? I calculated average speeds per provider and compared those to the contract speeds:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Provider&lt;/th&gt;
&lt;th align="right"&gt;Download (mbits)&lt;/th&gt;
&lt;th align="right"&gt;Contract Speed&lt;/th&gt;
&lt;th align="right"&gt;Download / Contract&lt;/th&gt;
&lt;th align="right"&gt;Download - Contract&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Fios&lt;/td&gt;
&lt;td align="right"&gt;282.3&lt;/td&gt;
&lt;td align="right"&gt;300&lt;/td&gt;
&lt;td align="right"&gt;94.1%&lt;/td&gt;
&lt;td align="right"&gt;-17.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;Spectrum&lt;/td&gt;
&lt;td align="right"&gt;173.4&lt;/td&gt;
&lt;td align="right"&gt;200&lt;/td&gt;
&lt;td align="right"&gt;86.7%&lt;/td&gt;
&lt;td align="right"&gt;-26.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I think that goes a long way in terms of quantifying the improvement in service reliability, but the reliability question itself is better posed as the probability that a provider will perform at some X% of the contract value. I won't notice anything if I'm at 95%, but might notice some sluggishness when it gets down to 50%.&lt;/p&gt;
&lt;p&gt;That makes the question a little more specific: &lt;em&gt;what is the % of hours in which each provider realized service at X% of the contracted value?&lt;/em&gt;&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/auc.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;Both providers provide &amp;gt;0% service 100% of the time (duh), and both provide &amp;gt;100% of the contract service some amount of the time (you can see values above the contract speed above; I am not sure if this is noise or not). &lt;/p&gt;
&lt;p&gt;Crucially, there is a large swath in which Fios outperforms Spectrum. At the most extreme, Fios provided &amp;gt; 47% of service in 99% of hours, whereas Spectrum provided the same level of service only 77% of the time.&lt;/p&gt;
&lt;p&gt;This is an extremely Petty analysis and, to be clear, I &lt;strong&gt;do not work for Verizon&lt;/strong&gt;; I'm just one of many people happy to enjoy improved network reliability üòá. You can find my analysis in &lt;a href="https://deepnote.com/project/Spectrum-Vs-FIOS-BD1Vf-OLRbqIDpZyVI1T9Q/%2Fnotebook.ipynb/#00008-fc4d1b26-157c-40d6-afc5-e694ca06987e"&gt;this deepnote&lt;/a&gt; (BTW, I üíñ deepnote). I'm happy to send the data to whomever asks, so ask if you want it!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Watching spring arrive from my home office.</title><link href="https://nolanbconaway.github.io/blog/2021/watching-spring-arrive-from-my-home-office.html" rel="alternate"></link><published>2021-06-19T00:00:00-04:00</published><updated>2021-06-19T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2021-06-19:/blog/2021/watching-spring-arrive-from-my-home-office.html</id><summary type="html">&lt;p&gt;During the winter of 2020, I hooked up a camera to a Raspberry Pi and started writing &lt;a href="https://github.com/nolanbconaway/rpi-camera"&gt;python modules&lt;/a&gt; to do time lapse photography. Early on, I made some films of &lt;a href="https://www.youtube.com/watch?v=8ykslLDXHdA"&gt;winter storms&lt;/a&gt; coming in over the course of 4-12 hour periods.&lt;/p&gt;
&lt;p&gt;At the start of spring, I set up ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;During the winter of 2020, I hooked up a camera to a Raspberry Pi and started writing &lt;a href="https://github.com/nolanbconaway/rpi-camera"&gt;python modules&lt;/a&gt; to do time lapse photography. Early on, I made some films of &lt;a href="https://www.youtube.com/watch?v=8ykslLDXHdA"&gt;winter storms&lt;/a&gt; coming in over the course of 4-12 hour periods.&lt;/p&gt;
&lt;p&gt;At the start of spring, I set up for a longer time lapse; I wanted to film the start of spring as seen from the window of my home office. There's a lot of greenery outside so I was hoping to quantify the proportion of the color space occupied by green over time.&lt;/p&gt;
&lt;p&gt;Here's what I got:&lt;/p&gt;
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="405" src="https://www.youtube.com/embed/TQsMR8QJxsw" title="YouTube video player" width="720"&gt;&lt;/iframe&gt;
&lt;p&gt;It can be a little difficult to see the greenery come in, so I made another version which clips to the focal regions:&lt;/p&gt;
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="405" src="https://www.youtube.com/embed/S1OscbAJsFM" title="YouTube video player" width="720"&gt;&lt;/iframe&gt;
&lt;p&gt;It can't hurt to look at a little before/after action. These two photos were taken at Noon.&lt;/p&gt;
&lt;table class="table-sm table table-hover" style=""&gt;
&lt;tr&gt;
&lt;th&gt;April 7&lt;/th&gt;&lt;th&gt;April 25&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img class="img-fluid" src="https://nolanbconaway.github.io/blog/2021/2021-04-07.jpg"/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img class="img-fluid" src="https://nolanbconaway.github.io/blog/2021/2021-04-25.jpg"/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Even here we're still only using Human vision to see how much green there is. I used opencv to do a really dumb calculation: the proportion of the RBG color volume that lies in the G channel each day:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/timeline.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;NOTE: I do not know very much about computer vision; don't @ me.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The green channel gains dominance over time but it's not crazy evident. Probably what I am seeing as "green" is actually a mixture of lots of colors in the RGB space. &lt;/p&gt;
&lt;p&gt;In any case, if time lapse content is your jam, be on the lookout for another film of my tomatoes coming in!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Notifications for unknown SSH logins on your headless machines.</title><link href="https://nolanbconaway.github.io/blog/2021/notifications-for-unknown-ssh-logins-on-your-headless-machines.html" rel="alternate"></link><published>2021-04-20T00:00:00-04:00</published><updated>2021-04-20T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2021-04-20:/blog/2021/notifications-for-unknown-ssh-logins-on-your-headless-machines.html</id><summary type="html">&lt;p&gt;I have four Linux servers (raspberry pi or otherwise) scattered around my apartment, and the thought of someone breaking into them can be pretty unsettling üòÖ. Besides all the usual steps one takes to secure machines like this (firewalls, disabling password authentication, port obfuscation, etc) I thought of one last precaution ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have four Linux servers (raspberry pi or otherwise) scattered around my apartment, and the thought of someone breaking into them can be pretty unsettling üòÖ. Besides all the usual steps one takes to secure machines like this (firewalls, disabling password authentication, port obfuscation, etc) I thought of one last precaution to help myself sleep better at night: &lt;strong&gt;send an email notification whenever there's an SSH login from an unknown IP address&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;My various servers should only be logged into by me on my personal computer, or by each other in a cronjob situation. I've assigned each of these a static IP address using my router, so there's a very finite number of addresses that should be used to SSH into each.&lt;/p&gt;
&lt;h2 id="how-it-works"&gt;How it works&lt;/h2&gt;
&lt;p&gt;I add a shell script to the home directory of each server called &lt;code&gt;check-ssh-address&lt;/code&gt;. The script uses a text file to remember all the historical addresses that have been used to log in via SSH.&lt;/p&gt;
&lt;p&gt;If the current session is an SSH session, the script checks the IP address of the client against the known IP addresses. If the client is new, it sends an email and appends a new record to the text file. If the client is known, the script exits early.&lt;/p&gt;
&lt;p&gt;Here's what that looks like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="c1"&gt;# set these&lt;/span&gt;
&lt;span class="nv"&gt;EMAIL_ADDR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;
&lt;span class="nv"&gt;EMAIL_PASS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt; &lt;span class="c1"&gt;# you need an app password. NOT your gmail password.&lt;/span&gt;
&lt;span class="nv"&gt;KNOWN_ADDRESSES_FILE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;/.ssh/known_addresses"&lt;/span&gt;

&lt;span class="c1"&gt;# exit if not ssh&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -z &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$SSH_CONNECTION&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="c1"&gt;# get IP address&lt;/span&gt;
&lt;span class="nv"&gt;IP_ADDRESS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$SSH_CONNECTION&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; cut -d &lt;span class="s2"&gt;" "&lt;/span&gt; -f &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;

&lt;span class="c1"&gt;# make sure file exists, then read it&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt; ! -f &lt;span class="nv"&gt;$KNOWN_ADDRESSES_FILE&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; touch &lt;span class="nv"&gt;$KNOWN_ADDRESSES_FILE&lt;/span&gt;
&lt;span class="nv"&gt;KNOWN_IPS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;cat &lt;span class="nv"&gt;$KNOWN_ADDRESSES_FILE&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# check if the user IP is known&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$KNOWN_IPS&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; *&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$IP_ADDRESS&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;* &lt;span class="o"&gt;]]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="c1"&gt;# add IP to known addresses&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$IP_ADDRESS&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="nv"&gt;$KNOWN_ADDRESSES_FILE&lt;/span&gt;

&lt;span class="c1"&gt;# send an email&lt;/span&gt;
&lt;span class="nv"&gt;EMAIL_TEXT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"\&lt;/span&gt;
&lt;span class="s2"&gt;From: &lt;/span&gt;&lt;span class="nv"&gt;$EMAIL_ADDR&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;
&lt;span class="s2"&gt;To: &lt;/span&gt;&lt;span class="nv"&gt;$EMAIL_ADDR&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;
&lt;span class="s2"&gt;Subject: SSH Session with unknown IP Address&lt;/span&gt;

&lt;span class="s2"&gt;A new IP address was used to log into &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;hostname&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;!&lt;/span&gt;

&lt;span class="s2"&gt;User: &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;whoami&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;
&lt;span class="s2"&gt;Date: &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;date&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;
&lt;span class="s2"&gt;Address: &lt;/span&gt;&lt;span class="nv"&gt;$IP_ADDRESS&lt;/span&gt;&lt;span class="s2"&gt;&lt;/span&gt;

&lt;span class="s2"&gt;If it is you, that is great! Otherwise LOCK DOWN THAT MACHINE.&lt;/span&gt;
&lt;span class="s2"&gt;"&lt;/span&gt;

curl &lt;span class="se"&gt;\&lt;/span&gt;
    --url &lt;span class="s2"&gt;"smtps://smtp.gmail.com:465"&lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
    --silent --show-error&lt;span class="se"&gt;\&lt;/span&gt;
    --ssl-reqd &lt;span class="se"&gt;\&lt;/span&gt;
    --mail-from &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$EMAIL_ADDR&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --mail-rcpt &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$EMAIL_ADDR&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    --user &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$EMAIL_ADDR&lt;/span&gt;&lt;span class="s2"&gt;:&lt;/span&gt;&lt;span class="nv"&gt;$EMAIL_PASS&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    -T &amp;lt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$EMAIL_TEXT&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The last trick is to get that check to run each time there is an SSH login. You could add a call to &lt;code&gt;.bash_profile&lt;/code&gt; or whatever, but I think it makes sense to put this in &lt;code&gt;~/.ssh/rc&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ touch &lt;span class="nv"&gt;$HOME&lt;/span&gt;/check-ssh-address
&lt;span class="c1"&gt;# ...&lt;/span&gt;
&lt;span class="c1"&gt;# Paste that file in, edit your email and APP PASSWORD (not your real password).&lt;/span&gt;
&lt;span class="c1"&gt;# ...&lt;/span&gt;
$ chmod +x &lt;span class="nv"&gt;$HOME&lt;/span&gt;/check-ssh-address
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then add this line to your &lt;code&gt;~/.ssh/rc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/check-ssh-address
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To this date I have never been informed of a new login that was &lt;em&gt;not&lt;/em&gt; me, and here's hoping that I never do! üôè.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>A flask server for iMessage on computers without iCloud.</title><link href="https://nolanbconaway.github.io/blog/2021/a-flask-server-for-imessage-on-computers-without-icloud.html" rel="alternate"></link><published>2021-04-19T00:00:00-04:00</published><updated>2021-04-19T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2021-04-19:/blog/2021/a-flask-server-for-imessage-on-computers-without-icloud.html</id><content type="html"></content><category term="github"></category></entry><entry><title>2020: The high and low temperatures</title><link href="https://nolanbconaway.github.io/blog/2021/2020-the-high-and-low-temperatures.html" rel="alternate"></link><published>2021-01-31T00:00:00-05:00</published><updated>2021-01-31T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2021-01-31:/blog/2021/2020-the-high-and-low-temperatures.html</id><summary type="html">&lt;p&gt;Two years ago, I hooked up a DS18B20 thermometer to a Raspberry Pi, and I set up a cron job to save the temperature in my postgres database every minute.&lt;/p&gt;
&lt;p&gt;A year later, in early 2020, I posted a &lt;a href="/blog/2020/2019-the-high-and-low-temperatures"&gt;year-in-review style article&lt;/a&gt; describing the data I had obtained from running ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;Two years ago, I hooked up a DS18B20 thermometer to a Raspberry Pi, and I set up a cron job to save the temperature in my postgres database every minute.&lt;/p&gt;
&lt;p&gt;A year later, in early 2020, I posted a &lt;a href="/blog/2020/2019-the-high-and-low-temperatures"&gt;year-in-review style article&lt;/a&gt; describing the data I had obtained from running the cron job for a full calendar year. Well, we have another year of data and so here I am describing the latest in temperatures from my apartment!&lt;/p&gt;
&lt;p&gt;(FYI- You can still check out a real time view of the data via &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/"&gt;my heroku webapp&lt;/a&gt;.)&lt;/p&gt;
&lt;h2 id="the-warmest-day"&gt;üî• the warmest day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/warmest.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;In 2019, July 5 was the warmest day I recorded, with temperatures in the ~88‚Ñâ range. We were out of town that day and had shut off our air conditioning. This year is a similar story, with high temps in early July at about ~88‚Ñâ. Interestingly (? ü§î), we were &lt;em&gt;not&lt;/em&gt; out of town that day, so we must've roasted pretty good in the apartment.&lt;/p&gt;
&lt;h2 id="the-coldest-day"&gt;‚ùÑ the coldest day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/coldest.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;In 2019 our apartment's heat came from old-school radiators, and we had no control over the temperature. The coldest day (Feb 1 2029, for those keeping track) was the coldest day because the heat simply did not come on as much.&lt;/p&gt;
&lt;p&gt;We moved out of that apartment though! Now, we control our own heat (+ and pay for it üòè). We were lucky enough to be able to spend New Years Eve with family this year, so we shut off our heat while we were gone. That makes Dec 31 the coldest day I recorded. &lt;/p&gt;
&lt;h2 id="the-most-variable-day"&gt;üìà The most variable day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/variable.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;In 2019, the most variable day was due to a burglar who &lt;strong&gt;literally left the window open&lt;/strong&gt;. We were üôè &lt;strong&gt;not burgled in 2020&lt;/strong&gt; üôè, so the most variable day was one in which the temperature moved a lot naturally. Our new apartment gets more sun, so the temperature much more strongly follows a daily cycle on sunny days.&lt;/p&gt;
&lt;h2 id="all-together"&gt;All together!&lt;/h2&gt;
&lt;p&gt;I found it interesting to plot these days on the same axis so that the changes in temperature over time can be appreciated. Take a look!&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/everybody.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;üëã cya in 2022!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Binoculars, a python package for Binomial confidence intervals.</title><link href="https://nolanbconaway.github.io/blog/2021/binoculars.html" rel="alternate"></link><published>2021-01-01T00:00:00-05:00</published><updated>2021-01-01T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2021-01-01:/blog/2021/binoculars.html</id><summary type="html">&lt;p&gt;I recently published a package (&lt;a href="https://pypi.org/project/binoculars/"&gt;üì¶ click&lt;/a&gt;) called "binoculars", and all it does is calculate confidence intervals for binomial proportions. You can &lt;code&gt;pip install binoculars&lt;/code&gt; or contribute via &lt;a href="https://github.com/nolanbconaway/binoculars"&gt;GitHub&lt;/a&gt;, etc. &lt;/p&gt;
&lt;p&gt;Usage is pretty simple:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;binoculars&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;binomial_confidence&lt;/span&gt;

&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;

&lt;span class="n"&gt;binomial_confidence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'jeffrey'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# (0 ‚Ä¶&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I recently published a package (&lt;a href="https://pypi.org/project/binoculars/"&gt;üì¶ click&lt;/a&gt;) called "binoculars", and all it does is calculate confidence intervals for binomial proportions. You can &lt;code&gt;pip install binoculars&lt;/code&gt; or contribute via &lt;a href="https://github.com/nolanbconaway/binoculars"&gt;GitHub&lt;/a&gt;, etc. &lt;/p&gt;
&lt;p&gt;Usage is pretty simple:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;binoculars&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;binomial_confidence&lt;/span&gt;

&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;

&lt;span class="n"&gt;binomial_confidence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'jeffrey'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# (0.1307892803998113, 0.28628125447599173)&lt;/span&gt;
&lt;span class="n"&gt;binomial_confidence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'normal'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (0.12160000000000001, 0.2784)&lt;/span&gt;
&lt;span class="n"&gt;binomial_confidence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'wilson'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (0.1333659225590988, 0.28883096192650237)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Et-cetra.&lt;/p&gt;
&lt;p&gt;‚ùì‚ùì‚ùì But &lt;em&gt;why the heck did I write a whole package to calculate confidence intervals for binomial proportions?&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="reason-1-binomial-certainty-comes-up-constantly"&gt;Reason 1: Binomial certainty comes up constantly.&lt;/h2&gt;
&lt;p&gt;Lots of statistics problems come around to the issue of certainty in the estimation of a binomial proportion. We all have data in which there are some \(N\) observations, out of which some \(k\) successfully do something, and so you want to understand the success rate \(\hat{p}= k \div N\). In the simplest case, you have some reference \(p\) value and you want to know if your \(\hat{p}\) is very different from that. Let's be concrete with some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You've got an ecommerce site and you want to determine if the conversion rate, \(\text{purchases} \div \text{visits}\), this month is different from last year.&lt;/li&gt;
&lt;li&gt;You want to know if the employment rate of a subpopulation, \(\text{employed} \div \text{people}\), is different from the national average.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This comes up entirely too often in my work and it probably does in yours. It &lt;em&gt;ought to be&lt;/em&gt; super easy to calculate a confidence interval on \(\text{Binomial}(p, N )\), where \(p\) is set to your reference proportion and \(N\) is the number of observations in your sample. Then you check if your \(\hat{p}\) is within the confidence interval and report those numbers. But it is &lt;em&gt;not&lt;/em&gt; that.&lt;/p&gt;
&lt;h2 id="reason-2-lots-of-people-get-this-wrong"&gt;Reason 2: Lots of people get this wrong.&lt;/h2&gt;
&lt;p&gt;A casual google search for "binomial confidence interval" will invariably lead you to this &lt;a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"&gt;&lt;em&gt;outstanding wikipedia page&lt;/em&gt;&lt;/a&gt;. I have returned to this page more times than I can count.&lt;/p&gt;
&lt;p&gt;Most people end up implementing the &lt;strong&gt;"Normal Approximation"&lt;/strong&gt; which is super easy and super terrible.&lt;/p&gt;
&lt;p&gt;$$
p \pm z \sqrt{\frac{p (1 - p)}{N}}
$$&lt;/p&gt;
&lt;p&gt;That math is not a lot, so you can do one-liners in python and SQL. Also, it is &lt;em&gt;symmetrical&lt;/em&gt;, which means you write clean code like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;ci&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.96&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;ci&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ci&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But the &lt;em&gt;symmetry&lt;/em&gt; is the problem! By way of demonstrating the terribleness here, I calculated (&lt;a href="https://deepnote.com/project/e17fa473-51c6-45aa-8de0-980be7d2dc5f"&gt;üìì click&lt;/a&gt;) the lower bound of 95% confidence with the Normal approximation as a function of \(N\) with a static \(\hat{p}\) of 0.01, 0.05, and 0.1:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2021/Normal.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;I took the liberty of annotating exactly where it is terrible. You can see that in &lt;em&gt;many real world cases&lt;/em&gt; the Normal approximation produces a lower bound &amp;lt; 0. We'd also see upper bounds &amp;gt; 1 if we had looked at \(\hat{p} = 0.99, 0.95, 0.9\), etc, because of the &lt;em&gt;symmetry&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;That's troubling not only because it should be impossible, but also because it suggests an &lt;strong&gt;underestimation&lt;/strong&gt; of the uncertainty at the upper bound. Using the normal approximation in these cases might lead you to a fully incorrect conclusion üò¨. &lt;/p&gt;
&lt;p&gt;I don't know about y'all, but I commonly deal with Binomials at \(N&amp;lt;100\) and \(\hat{p}&amp;lt;0.1\), so this is super concerning for me.&lt;/p&gt;
&lt;h2 id="back-to-binoculars"&gt;Back to Binoculars...&lt;/h2&gt;
&lt;p&gt;We have a situation in which the easiest method for computing Binomial uncertainty is both the most popular and is patently &lt;em&gt;wrong&lt;/em&gt; in many common cases. It &lt;em&gt;ought to be&lt;/em&gt; just as easy to choose a more accurate method, and with Binoculars, it is!&lt;/p&gt;
&lt;p&gt;So, what method should &lt;em&gt;you&lt;/em&gt; use?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Do &lt;strong&gt;not&lt;/strong&gt; use the normal approximation if you have any other option.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are using Python, you have other options! (thanks to Binoculars üòÄ). I like Jeffrey's interval because I am partial to Bayes but the Wilson interval is also very good.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="in-sql"&gt;In SQL?&lt;/h3&gt;
&lt;p&gt;Binoculars won't help if you're doing this in SQL. But I often find myself needing a quick confidence interval for a dashboard, etc. If this sounds like you, the Wilson interval is your jam. Jeffrey's interval involves a lot of Beta distribution math which gets &lt;em&gt;narsty&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;The Wilson interval isn't exactly &lt;em&gt;pretty&lt;/em&gt;, but you can probably implement it as a user-defined-function on your database without a lot of hassle. &lt;/p&gt;
&lt;p&gt;If you're using DBT, you can also implement it as a macro!&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cp"&gt;{%&lt;/span&gt;- &lt;span class="k"&gt;macro&lt;/span&gt; &lt;span class="nv"&gt;wilson_interval&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;col_p&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1.96&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; -&lt;span class="cp"&gt;%}&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;

&lt;span class="cp"&gt;{%&lt;/span&gt;- &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nv"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'lower'&lt;/span&gt; -&lt;span class="cp"&gt;%}&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt; (&lt;/span&gt;
&lt;span class="x"&gt;   &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_p&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; + &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; / (2 * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;) &lt;/span&gt;
&lt;span class="x"&gt;   - &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * sqrt((&lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_p&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * (1 - &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_p&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;) + &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; / (4 * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;)) / &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;)&lt;/span&gt;
&lt;span class="x"&gt;  ) / (1 + &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; / &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;)&lt;/span&gt;

&lt;span class="cp"&gt;{%&lt;/span&gt;- &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nv"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;'upper'&lt;/span&gt; -&lt;span class="cp"&gt;%}&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt; (&lt;/span&gt;
&lt;span class="x"&gt;   &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_p&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; + &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; / (2 * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;) &lt;/span&gt;
&lt;span class="x"&gt;   + &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * sqrt((&lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_p&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * (1 - &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_p&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;) + &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; / (4 * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;)) / &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;)&lt;/span&gt;
&lt;span class="x"&gt;  ) / (1 + &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; * &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;z&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; / &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt;&lt;span class="nv"&gt;col_n&lt;/span&gt;&lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt;)&lt;/span&gt;

&lt;span class="cp"&gt;{%&lt;/span&gt;- &lt;span class="k"&gt;endif&lt;/span&gt; -&lt;span class="cp"&gt;%}&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="cp"&gt;{%&lt;/span&gt;- &lt;span class="k"&gt;endmacro&lt;/span&gt; -&lt;span class="cp"&gt;%}&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which gets called like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="x"&gt;with t as ( select 500 as n, 0.2 as p )&lt;/span&gt;
&lt;span class="x"&gt;select &lt;/span&gt;
&lt;span class="x"&gt;  t.*,&lt;/span&gt;
&lt;span class="x"&gt;  &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt; &lt;span class="nv"&gt;wilson_interval&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'n'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'lower'&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; as ci_lower,&lt;/span&gt;
&lt;span class="x"&gt;  &lt;/span&gt;&lt;span class="cp"&gt;{{&lt;/span&gt; &lt;span class="nv"&gt;wilson_interval&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'p'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'n'&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'upper'&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="cp"&gt;}}&lt;/span&gt;&lt;span class="x"&gt; as ci_upper&lt;/span&gt;
&lt;span class="x"&gt;from t&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That sort of math is is exactly what you'd &lt;em&gt;want&lt;/em&gt; to hide in a macro, anyway üòâ.&lt;/p&gt;
&lt;h3 id="please-contribute"&gt;üôè Please contribute!&lt;/h3&gt;
&lt;p&gt;Binoculars currently focuses on estimating uncertainty given \(\text{Binomial}(\hat{p}, N)\). It's missing several well-defined methods on that &lt;a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"&gt;awesome wiki page&lt;/a&gt;, and there are a lot of similar questions to be asked! Such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Uncertainty about the odds-ratio between two binomial samples?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\text{odds} = \frac{k_1}{N_1} \div \frac{k_2}{N_2}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Uncertainty about the difference between two binomial samples?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\text{diff} = \frac{k_1}{N_1} - \frac{k_2}{N_2}
$$&lt;/p&gt;
&lt;p&gt;I've found some &lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK431098/"&gt;Normal Approximation-looking math&lt;/a&gt; for this stuff, and it'd be great to work out a function to make this available generally!&lt;/p&gt;
&lt;p&gt;Binoculars is open sourced on &lt;a href="https://github.com/nolanbconaway/binoculars"&gt;GitHub&lt;/a&gt;, submit issues with your complaints or PRs with your solutions üòó.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>In which I demonstrate that they say "Oh My God" a lot in the show "Friends"</title><link href="https://nolanbconaway.github.io/blog/2020/friends-omg.html" rel="alternate"></link><published>2020-09-13T00:00:00-04:00</published><updated>2020-09-13T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-09-13:/blog/2020/friends-omg.html</id><summary type="html">&lt;p&gt;We were rewatching some old Friends episodes when I took notice that the phrase &lt;em&gt;"oh my god"&lt;/em&gt; comes up a lot in that show. I am a &lt;em&gt;professional data scientist&lt;/em&gt; üìà üìä , so I set to quantify exactly just how much more often they say "oh my god" in Friends compared to ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;We were rewatching some old Friends episodes when I took notice that the phrase &lt;em&gt;"oh my god"&lt;/em&gt; comes up a lot in that show. I am a &lt;em&gt;professional data scientist&lt;/em&gt; üìà üìä , so I set to quantify exactly just how much more often they say "oh my god" in Friends compared to other shows.&lt;/p&gt;
&lt;p&gt;I ended up building a whole website so that it would be easy to ask arbitrary questions about phrase frequency in different TV shows. I &lt;a href="https://friends-omg.herokuapp.com/"&gt;deployed it on heroku&lt;/a&gt; so you can ask these questions too. &lt;/p&gt;
&lt;p&gt;This post describes the source data I'm using and looks into a few of my own curiosities! Enjoy,  friends üîé .&lt;/p&gt;
&lt;h2 id="source-data"&gt;Source data&lt;/h2&gt;
&lt;p&gt;Once you have well-structured data, answering the question "How much do they say phrase &lt;em&gt;X&lt;/em&gt; in each show?" is fairly easy. In the simplest case, you can count the number of lines containing a string (thats what I did). &lt;/p&gt;
&lt;p&gt;But as usual, obtaining said well-structured data is not easy.&lt;/p&gt;
&lt;p&gt;Luckily, other smart people are asking similar questions of TV show script data, so it wasn't a nightmare to cobble together a script database.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://quotennial.github.io/friends-engineering/"&gt;Yusuf Sohoye&lt;/a&gt; had already set up some slick regex to parse through every Friends script.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/colinpollock/seinfeld-scripts"&gt;Colin Pollock&lt;/a&gt; had a ready-to-go sqlite database with every Seinfeld script.&lt;/li&gt;
&lt;li&gt;I found a Sex and the City script CSV on &lt;a href="https://www.kaggle.com/snapcrack/every-sex-and-the-city-script"&gt;Kaggle&lt;/a&gt;. It needed some manual cleaning and so now I'm pretty sure that I &lt;a href="http://nolanc.heliohost.org/omg-data/satc.csv"&gt;host&lt;/a&gt; the cleanest Sex and the City database online.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wrote out &lt;a href="https://github.com/nolanbconaway/friends-omg/tree/master/build"&gt;a python module&lt;/a&gt; to transfer data from each of these sources into a single sqlite database. Each show has its own &lt;a href="https://github.com/nolanbconaway/friends-omg/blob/master/build/ddl.sql"&gt;table&lt;/a&gt;, and its own python script to transform the raw source data into a schema composed of these four columns:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Column&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;episode_id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;varchar&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;"0105"&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;character_name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;varchar&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;"monica"&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;episode_line_number&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;integer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;17&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;line_text&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;varchar&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;"Oh my god!"&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Once those data are in place, it's easy to do a count of lines containing any phrase:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;select&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;case&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;when&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;like&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'%{phrase}%'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;then&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;"{show}"&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;select&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can run that query once per show to get all the counts you want!&lt;/p&gt;
&lt;p&gt;This approach doesn't really work with smaller phrases (e.g., if you want to count lines containing "dog", you'll also get lines containing "hotdog"). It also fails to account for how TV scripts use punctuation to express things differently (e.g., "Oh... My... God..." is not "Oh My God"). But whatever this is fast and easy.&lt;/p&gt;
&lt;p&gt;You can download the full (gzipped) SQLite3 database &lt;a href="http://nolanc.heliohost.org/omg-data/data.db.gz"&gt;here&lt;/a&gt; (it's not very large) üòú .&lt;/p&gt;
&lt;h2 id="show-me-the-numbers-already"&gt;Show Me the Numbers Already&lt;/h2&gt;
&lt;p&gt;You can compute the frequencies of any phrase you'd like using the &lt;a href="https://friends-omg.herokuapp.com/"&gt;app&lt;/a&gt; but I've also taken the liberty of exploring some of my own curiosities!&lt;/p&gt;
&lt;h3 id="oh-my-god-click"&gt;Oh My God (&lt;a href="https://friends-omg.herokuapp.com/?q=oh+my+god"&gt;click&lt;/a&gt;)&lt;/h3&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Show&lt;/th&gt;
&lt;th&gt;Total Lines&lt;/th&gt;
&lt;th&gt;Lines including "oh my god"&lt;/th&gt;
&lt;th&gt;Probability of "oh my god"&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Friends&lt;/td&gt;
&lt;td&gt;61161&lt;/td&gt;
&lt;td&gt;920&lt;/td&gt;
&lt;td&gt;1.504%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sex And The City&lt;/td&gt;
&lt;td&gt;39687&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;td&gt;0.108%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seinfeld&lt;/td&gt;
&lt;td&gt;52865&lt;/td&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;0.212%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Think about this: &lt;em&gt;More than 1 in 100 lines of Friends includes the phrase "oh my god"&lt;/em&gt;. Less than &lt;strong&gt;0.25%&lt;/strong&gt; of lines in Seinfeld and Sex and the City contain the same phrase.  So they're at least &lt;strong&gt;6x&lt;/strong&gt; more likely to say Oh My God in Friends.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=qSmp1ZSvelY"&gt;Janice&lt;/a&gt; isn't even the top offender:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rachel: 221 lines&lt;/li&gt;
&lt;li&gt;Monica: 204 lines&lt;/li&gt;
&lt;li&gt;Phoebe: 164 lines&lt;/li&gt;
&lt;li&gt;Ross: 108 lines&lt;/li&gt;
&lt;li&gt;Chandler: 71 lines&lt;/li&gt;
&lt;li&gt;Joey: 63 lines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Together, the Friends account for 723/920 occurrences, which is 1.2% of all lines in the show. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Oh My God&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="talking-about-new-york-click"&gt;Talking about New York (&lt;a href="https://friends-omg.herokuapp.com/?q=new+york"&gt;click&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;All three shows are set around the same time period in New York City. But characters in Sex and the City talk about New York the most:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Show&lt;/th&gt;
&lt;th&gt;Total Lines&lt;/th&gt;
&lt;th&gt;Lines including "new york"&lt;/th&gt;
&lt;th&gt;Probability of "new york"&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sex And The City&lt;/td&gt;
&lt;td&gt;39687&lt;/td&gt;
&lt;td&gt;337&lt;/td&gt;
&lt;td&gt;0.849%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seinfeld&lt;/td&gt;
&lt;td&gt;52865&lt;/td&gt;
&lt;td&gt;83&lt;/td&gt;
&lt;td&gt;0.157%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Friends&lt;/td&gt;
&lt;td&gt;61161&lt;/td&gt;
&lt;td&gt;68&lt;/td&gt;
&lt;td&gt;0.111%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The majority of occurrences in Sex and the City are via Carrie, who says the phrase in 220 of 14107 lines. That's &lt;strong&gt;1.56%&lt;/strong&gt; of her lines, which is more often than they say "oh my god" in Friends! &lt;/p&gt;
&lt;h3 id="talking-about-food"&gt;Talking about Food&lt;/h3&gt;
&lt;p&gt;I checked on the line rates for different kinds of food and drink. Below is a bar chart of the probability that any line will contain the name of the food/drink:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/food.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;Not a lot to observe as far as I can tell, but it is a little surprising that Seinfeld talks about "coffee" more than Friends given that so much of Friends takes place in the coffee shop ü§î .&lt;/p&gt;
&lt;h2 id="more-info"&gt;More Info&lt;/h2&gt;
&lt;h3 id="what-about-some-other-tv-show-nolan"&gt;"What about [some other TV show], Nolan?"&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Please, &lt;em&gt;please&lt;/em&gt; point me to the data for the show you'd like added!&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;The data needs to be at the line granularity, so that the &lt;code&gt;episode_id, character_name, episode_line_number, line_text&lt;/code&gt; schema can be fulfilled. So long as the data can fit into that schema, there's no reason not to add it!&lt;/p&gt;
&lt;h3 id="the-normal-approximation-is-for-proportions-close-to-0"&gt;The Normal Approximation is üôÅ for proportions close to 0!&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Please, &lt;em&gt;please&lt;/em&gt; submit a PR implementing the &lt;a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval"&gt;wilson interval&lt;/a&gt;!&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Or literally ask me once and I'll do it. I am a &lt;em&gt;professional data scientist&lt;/em&gt; üìà üìä üìà üìä .&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Format SQL using SQLFluff in a webpp.</title><link href="https://nolanbconaway.github.io/blog/2020/format-sql-using-sqlfluff-in-a-webpp.html" rel="alternate"></link><published>2020-06-28T00:00:00-04:00</published><updated>2020-06-28T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-06-28:/blog/2020/format-sql-using-sqlfluff-in-a-webpp.html</id><content type="html"></content><category term="app"></category></entry><entry><title>An apriori recommender for music.</title><link href="https://nolanbconaway.github.io/blog/2020/an-apriori-recommender-for-music.html" rel="alternate"></link><published>2020-06-14T00:00:00-04:00</published><updated>2020-06-14T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-06-14:/blog/2020/an-apriori-recommender-for-music.html</id><content type="html"></content><category term="app"></category></entry><entry><title>Shabadoo, a Bayesian regression library built on numpyro and jax.</title><link href="https://nolanbconaway.github.io/blog/2020/shabadoo-a-bayesian-regression-library-built-on-numpyro-and-jax.html" rel="alternate"></link><published>2020-03-01T00:00:00-05:00</published><updated>2020-03-01T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-03-01:/blog/2020/shabadoo-a-bayesian-regression-library-built-on-numpyro-and-jax.html</id><content type="html"></content><category term="github"></category></entry><entry><title>2019: The high and low temperatures</title><link href="https://nolanbconaway.github.io/blog/2020/2019-the-high-and-low-temperatures.html" rel="alternate"></link><published>2020-01-05T00:00:00-05:00</published><updated>2020-01-05T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-01-05:/blog/2020/2019-the-high-and-low-temperatures.html</id><summary type="html">&lt;p&gt;Last winter I hooked up a &lt;a href="https://www.adafruit.com/product/374"&gt;DS18B20&lt;/a&gt; thermometer to a Raspberry Pi. I set up a &lt;a href="https://github.com/nolanbconaway/thermometer"&gt;python library&lt;/a&gt; to read the temperature, and a cron job to save the temperature in my postgres database every minute. Then I built a &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/"&gt;webapp&lt;/a&gt; to display the last 24 hours of data. The ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last winter I hooked up a &lt;a href="https://www.adafruit.com/product/374"&gt;DS18B20&lt;/a&gt; thermometer to a Raspberry Pi. I set up a &lt;a href="https://github.com/nolanbconaway/thermometer"&gt;python library&lt;/a&gt; to read the temperature, and a cron job to save the temperature in my postgres database every minute. Then I built a &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/"&gt;webapp&lt;/a&gt; to display the last 24 hours of data. The first stable readings were stored on Jan 5 2019, so we have arrived at one full year of readings from my thermometer project üéâ!&lt;/p&gt;
&lt;p&gt;The data are probably uninteresting to anyone who does not live in my apartment. Actually, they're not even interesting to all the people who &lt;em&gt;do&lt;/em&gt; live here ü§î.&lt;/p&gt;
&lt;p&gt;But I think the data can tell a story. At least some of it. This post tells the stories of unusual days in my apartment, from the perspective of temperature measurements.&lt;/p&gt;
&lt;h2 id="describing-a-days-temperatures"&gt;Describing a day's temperatures&lt;/h2&gt;
&lt;p&gt;Early on, I wrote out a SQL view that computes summary statistics on each day's temperature readings. It shows me descriptives like:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;readings&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;th&gt;range&lt;/th&gt;
&lt;th&gt;avg&lt;/th&gt;
&lt;th&gt;stddev&lt;/th&gt;
&lt;th&gt;avg_minute_delta&lt;/th&gt;
&lt;th&gt;day_delta&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-05&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;67.9&lt;/td&gt;
&lt;td&gt;74.9&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;71.5&lt;/td&gt;
&lt;td&gt;1.7&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;-3.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-06&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;71.2&lt;/td&gt;
&lt;td&gt;74.8&lt;/td&gt;
&lt;td&gt;3.6&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;-0.001&lt;/td&gt;
&lt;td&gt;1.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-07&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;68.9&lt;/td&gt;
&lt;td&gt;73.5&lt;/td&gt;
&lt;td&gt;4.6&lt;/td&gt;
&lt;td&gt;70.7&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-0.003&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-08&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;67.8&lt;/td&gt;
&lt;td&gt;72.5&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;td&gt;70.3&lt;/td&gt;
&lt;td&gt;1.1&lt;/td&gt;
&lt;td&gt;0.002&lt;/td&gt;
&lt;td&gt;-2.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019-01-09&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;69.5&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;3.5&lt;/td&gt;
&lt;td&gt;71.6&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Most of those columns have sensible names. &lt;code&gt;avg_minute_delta&lt;/code&gt; describes the average difference in temperature per minute across adjacent readings. &lt;code&gt;day_delta&lt;/code&gt; is the difference between the first and last reading in the day.&lt;/p&gt;
&lt;p&gt;I looked into these summary stats for days which stand out, days like:&lt;/p&gt;
&lt;h2 id="the-warmest-day"&gt;üî• the warmest day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/warmest.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;The day with the highest average temperature was &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-07-05"&gt;July 5&lt;/a&gt;. We were in Baltimore to celebrate July 4 with my family, so we closed up the windows and shut off the AC and the apartment absolutely &lt;em&gt;roasted&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="the-coldest-day"&gt;‚ùÑ the coldest day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/coldest.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;Per NYC regulation, landlords are required to maintain a temperature of 68‚Ñâ during the day in the winter, but on &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-02-01"&gt;Feb 1&lt;/a&gt; the temperature hung below 66‚Ñâ all day long. &lt;/p&gt;
&lt;h2 id="the-most-variable-day"&gt;The most variable day&lt;/h2&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/most_stddev.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;On December 16 we were burgled (&lt;strong&gt;everyone was ok!&lt;/strong&gt;). The burglar entered through the fire escape window, which is near where the Raspberry Pi thermometer is tucked away. That huge dip in the temperature is exactly the time the burglar entered the apartment. The detective that caught our case thought this was &lt;em&gt;awesome&lt;/em&gt;!&lt;/p&gt;
&lt;h2 id="anomaly-detection"&gt;Anomaly Detection&lt;/h2&gt;
&lt;p&gt;We can also use statistics to identify days with unusual properties. I queried my database with Python like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sqlalchemy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;

&lt;span class="n"&gt;engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_engine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'SQLALCHEMY_DATABASE_URI'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;daily_stats&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_sql_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"daily_stats"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'dt'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;daily_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="c1"&gt;#             readings    min    max  range    avg  stddev  avg_minute_delta  day_delta&lt;/span&gt;
&lt;span class="c1"&gt;# dt                                                                                   &lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-05      1440  67.89  74.86   6.98  71.54    1.65               0.0      -3.38&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-06      1440  71.15  74.75   3.60  72.96    0.75              -0.0       1.24&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-07      1440  68.90  73.51   4.61  70.65    0.95              -0.0       3.71&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-08      1440  67.78  72.50   4.72  70.28    1.12               0.0      -2.36&lt;/span&gt;
&lt;span class="c1"&gt;# 2019-01-09      1440  69.46  72.95   3.49  71.62    0.79              -0.0       0.34&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we assume all days are drawn from a multivariate normal distribution across the summary stat metrics, \(x \sim \mathcal{N}\left(\mu, \Sigma\right) \), outliers would be considered dates with very low \( p\left(x \vert  \mathcal{N}\left(\mu, \Sigma\right)\right) \).&lt;/p&gt;
&lt;p&gt;I normalized the stats and calculated the empirical parameters for the multivariate Normal distribution. Then I used &lt;code&gt;scipy.stats&lt;/code&gt; to do the heavy lifting.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt;

&lt;span class="n"&gt;zscores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;daily_stats&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'readings'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# I don't care about reading count&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;zscores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zscores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;density&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multivariate_normal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zscores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then I picked out the days with the lowest densities! Here are those dates:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;readings&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;th&gt;range&lt;/th&gt;
&lt;th&gt;avg&lt;/th&gt;
&lt;th&gt;stddev&lt;/th&gt;
&lt;th&gt;avg_minute_delta&lt;/th&gt;
&lt;th&gt;day_delta&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-12-16"&gt;2019-12-16&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;50.787&lt;/td&gt;
&lt;td&gt;71.15&lt;/td&gt;
&lt;td&gt;20.363&lt;/td&gt;
&lt;td&gt;65.003&lt;/td&gt;
&lt;td&gt;6.334&lt;/td&gt;
&lt;td&gt;-0.003&lt;/td&gt;
&lt;td&gt;3.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-09-12"&gt;2019-09-12&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;74.862&lt;/td&gt;
&lt;td&gt;86.9&lt;/td&gt;
&lt;td&gt;12.038&lt;/td&gt;
&lt;td&gt;83.073&lt;/td&gt;
&lt;td&gt;3.715&lt;/td&gt;
&lt;td&gt;-0.008&lt;/td&gt;
&lt;td&gt;11.925&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-09-19"&gt;2019-09-19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;65.187&lt;/td&gt;
&lt;td&gt;77.9&lt;/td&gt;
&lt;td&gt;12.713&lt;/td&gt;
&lt;td&gt;73.059&lt;/td&gt;
&lt;td&gt;4.709&lt;/td&gt;
&lt;td&gt;0.005&lt;/td&gt;
&lt;td&gt;-7.987&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-09-11"&gt;2019-09-11&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;77.9&lt;/td&gt;
&lt;td&gt;87.125&lt;/td&gt;
&lt;td&gt;9.225&lt;/td&gt;
&lt;td&gt;82.538&lt;/td&gt;
&lt;td&gt;3.736&lt;/td&gt;
&lt;td&gt;0.006&lt;/td&gt;
&lt;td&gt;-8.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-10-01"&gt;2019-10-01&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;73.287&lt;/td&gt;
&lt;td&gt;82.737&lt;/td&gt;
&lt;td&gt;9.45&lt;/td&gt;
&lt;td&gt;78.11&lt;/td&gt;
&lt;td&gt;3.643&lt;/td&gt;
&lt;td&gt;0.005&lt;/td&gt;
&lt;td&gt;-7.313&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The first anomaly is an obvious one: the most variable day of the year which I wrote about above. See below for plots of the other four:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-09-12.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-09-19.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-09-11.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/anomaly-2019-10-01.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;And for good measure, &lt;a href="https://temp-in-nolans-apartment.herokuapp.com/date/2019-03-22"&gt;here&lt;/a&gt; is the least anomalous date according to my very naive model.&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2020/nonanomaly.svg" type="image/svg+xml"&gt;&lt;/object&gt;</content><category term="blog"></category></entry><entry><title>Deploying my Pelican website to Github Pages.</title><link href="https://nolanbconaway.github.io/blog/2020/deploying-my-pelican-website-to-github-pages.html" rel="alternate"></link><published>2020-01-01T00:00:00-05:00</published><updated>2020-01-01T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2020-01-01:/blog/2020/deploying-my-pelican-website-to-github-pages.html</id><summary type="html">&lt;p&gt;üéâ If you're reading this, you're looking at my new website! üéâ&lt;/p&gt;
&lt;p&gt;I ditched Jekyll because I didn't want to have to maintain a ruby installation on my machine. My new site runs on Pelican and I hacked together a deployment flow to Github pages. Here's that hack:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Main Workflow&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;on ‚Ä¶&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;üéâ If you're reading this, you're looking at my new website! üéâ&lt;/p&gt;
&lt;p&gt;I ditched Jekyll because I didn't want to have to maintain a ruby installation on my machine. My new site runs on Pelican and I hacked together a deployment flow to Github pages. Here's that hack:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Main Workflow&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;push&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;branches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;dev&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;jobs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;build-and-deploy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;runs-on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ubuntu-latest&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;actions/checkout@v1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;actions/setup-python@v1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;with&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;python-version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"3.7"&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install Dependencies&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;|&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;python -m pip install --upgrade pip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;pip install -r requirements.txt&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Build Production Site&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;|&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;make publish&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;cp readme.md output/readme.md&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;touch output/.nojekyll&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Deploy To Master Branch&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;peaceiris/actions-gh-pages@v3&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;with&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;publish_branch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;master&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;publish_dir&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;./output&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;personal_token&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ secrets.PERSONAL_TOKEN }}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="translation"&gt;Translation...&lt;/h3&gt;
&lt;p&gt;On push to the &lt;code&gt;dev&lt;/code&gt; branch, Github will set up a python 3.7 environment with my requirements.txt. It'll build the static site into an &lt;code&gt;output&lt;/code&gt; directory using &lt;code&gt;make publish&lt;/code&gt;. I added a &lt;code&gt;.nojekyll&lt;/code&gt; file so that Github knows it's not working with a Jekyll page.&lt;/p&gt;
&lt;p&gt;I found an &lt;a href="https://github.com/peaceiris/actions-gh-pages"&gt;action&lt;/a&gt; that will publish a directory to github pages. I set that up to send the static site directory to the &lt;code&gt;master&lt;/code&gt; branch. Github will then automatically deploy the site since i am using my &lt;code&gt;username.github.io&lt;/code&gt; repo! üéä&lt;/p&gt;
&lt;p&gt;So the flow is that I make all updates to &lt;code&gt;dev&lt;/code&gt; which propagate automatically to &lt;code&gt;master&lt;/code&gt; and then deployment on push. Beautiful! You can check out the source code on &lt;a href="https://github.com/nolanbconaway/nolanbconaway.github.io/tree/dev"&gt;github&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Underground, A python library for processing realtime NYC subway data.</title><link href="https://nolanbconaway.github.io/blog/2019/underground-a-python-library-for-processing-realtime-nyc-subway-data.html" rel="alternate"></link><published>2019-10-06T00:00:00-04:00</published><updated>2019-10-06T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-10-06:/blog/2019/underground-a-python-library-for-processing-realtime-nyc-subway-data.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Shlack, yet another command line interface for slack.</title><link href="https://nolanbconaway.github.io/blog/2019/shlack-yet-another-command-line-interface-for-slack.html" rel="alternate"></link><published>2019-10-03T00:00:00-04:00</published><updated>2019-10-03T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-10-03:/blog/2019/shlack-yet-another-command-line-interface-for-slack.html</id><content type="html"></content><category term="github"></category></entry><entry><title>I set up a webapp to monitor my home network performance.</title><link href="https://nolanbconaway.github.io/blog/2019/i-set-up-a-webapp-to-monitor-my-home-network-performance.html" rel="alternate"></link><published>2019-06-06T00:00:00-04:00</published><updated>2019-06-06T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-06-06:/blog/2019/i-set-up-a-webapp-to-monitor-my-home-network-performance.html</id><content type="html"></content><category term="app"></category></entry><entry><title>A single sample poisson test.</title><link href="https://nolanbconaway.github.io/blog/2019/a-single-sample-poisson-test.html" rel="alternate"></link><published>2019-05-15T00:00:00-04:00</published><updated>2019-05-15T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-05-15:/blog/2019/a-single-sample-poisson-test.html</id><summary type="html">&lt;p&gt;I am working with a lot of Poisson data recently. The more I encounter these data, the more I realize that data scientists often incorrectly treat Poisson data as normally or linearly distributed variables.&lt;/p&gt;
&lt;p&gt;I recently added a new tool to my Poisson kit, which I think of as a ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am working with a lot of Poisson data recently. The more I encounter these data, the more I realize that data scientists often incorrectly treat Poisson data as normally or linearly distributed variables.&lt;/p&gt;
&lt;p&gt;I recently added a new tool to my Poisson kit, which I think of as a "Single Sample Poisson test". The question being asked is:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What is the probability of drawing \(N\) samples with mean \(\bar{x}=\sum_i{N_i} \div N\) from a Poisson distribution with mean \(\mu\)?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An example to make things more concrete. Search platforms like google/bing/yahoo allow you to bid a certain amount to show an ad when someone searches for a given keyword. You only pay the search engine if the ad is clicked on. They usually won't charge you the full price of the bid, but some unknown lower amount that we refer to as the cost-per-click (CPC).&lt;/p&gt;
&lt;p&gt;Imagine we are hoping to pay a 100 cent ($1 USD) CPC.  We can measure how much we actually paid for each click, but as clicks come in they have variable prices:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Price (cents)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;9:01am&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:01am&lt;/td&gt;
&lt;td&gt;95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:01am&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:02am&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9:02am&lt;/td&gt;
&lt;td&gt;98&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Above are 5 samples with an average CPC of 102.6 cents. How likely are we to obtain 5 samples with an average of 102.6, assuming that the true CPC is 100? If \(N=5\) samples with \(\bar{x}=102.6\) is unlikely given a distribution with \(\mu=100\), then clearly we are not correctly targeting the CPC value.&lt;/p&gt;
&lt;h2 id="the-test"&gt;The Test&lt;/h2&gt;
&lt;p&gt;A traditional approach to this question might treat the samples as normally distributed and use a Student's T-Test or a Sign test. This works a lot of the time due to the fact that &lt;a href="http://socr.ucla.edu/Applets.dir/NormalApprox2PoissonApplet.html"&gt;Poisson data are approximately normal when \(\mu &amp;gt; 20\)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want a "pure-Poisson" approach, you need to think about it another way. I took part in a &lt;a href="https://stats.stackexchange.com/questions/399803/compute-sample-probabilities-given-a-poisson-distribution"&gt;very interesting SO discussion&lt;/a&gt; in which a &lt;a href="https://stats.stackexchange.com/users/85665/bruceet"&gt;Very Smart Person&lt;/a&gt; basically gave me the answer. I will relate that answer here.&lt;/p&gt;
&lt;p&gt;In short, drawing \(N\) samples with an average of \(\bar{x}\) is like drawing a single sample from a higher distribution, which has the value \(\sum_i{x_i} = N\bar{x}\). We can think about that value within the context of a distribution \(\mathsf{Poisson}(N \mu)\), which would tell us how unlikely our samples are:&lt;/p&gt;
&lt;p&gt;$$
N \bar x \sim \mathsf{Poisson}(N \mu).
$$&lt;/p&gt;
&lt;p&gt;For example, the CDF of \(\mathsf{Poisson}(N \mu)\) would tell us the proportion of values which are greater than or less than \(N\bar{x}\).&lt;/p&gt;
&lt;p&gt;If only a small proportion of values from the CPC distribution \(\mathsf{Poisson}(5 \cdot 100)\) are greater than \(5 \cdot 102.6\), this would indicate that it is unlikely to have obtained \(\bar{x}=102.6\) by chance and thus maybe there's something wrong with the CPC targeting.&lt;/p&gt;
&lt;h3 id="in-python"&gt;In Python&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;xbar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;102.6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;xbar&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mf"&gt;0.7285633495908114&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That shows that 72.9% of \(N=5\) samples from a distribution with \(\mu=100\) would have an average value of less than \(\bar{x}=102.6\). Conversely, 27.1% of samples would have a greater value. So our samples aren't very unlikely given a CPC of 100.&lt;/p&gt;
&lt;p&gt;As you increase the number of samples you'd expect \(\bar{x}=102.6\) to become more and more unlikely.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;     &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cdf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;xbar&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="mf"&gt;0.7994310510662929&lt;/span&gt;
&lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="mf"&gt;0.8795150478303122&lt;/span&gt;
&lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="mf"&gt;0.9236492476438214&lt;/span&gt;
&lt;span class="mi"&gt;40&lt;/span&gt; &lt;span class="mf"&gt;0.9503028216143891&lt;/span&gt;
&lt;span class="mi"&gt;50&lt;/span&gt; &lt;span class="mf"&gt;0.9671124727069668&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When \(N=50\), 96.7% of samples from the distribution would have a lower average value (conversely, only 3.3% of samples would have a greater value). We're much less likely to obtain \(\bar{x}=102.6\) by chance, and I might suggest looking into how CPCs are being targeted.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>A python implementation of the Poisson exact test (e-test).</title><link href="https://nolanbconaway.github.io/blog/2019/a-python-implementation-of-the-poisson-exact-test-e-test.html" rel="alternate"></link><published>2019-03-24T00:00:00-04:00</published><updated>2019-03-24T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-03-24:/blog/2019/a-python-implementation-of-the-poisson-exact-test-e-test.html</id><summary type="html">&lt;p&gt;At work we are doing tests on different paid search optimization tools. We wanted to see if a new tool offered improvements over what we had been using. In reality we are comparing the two on all sorts of metrics, but as an example we can focus on comparisons with ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work we are doing tests on different paid search optimization tools. We wanted to see if a new tool offered improvements over what we had been using. In reality we are comparing the two on all sorts of metrics, but as an example we can focus on comparisons with respect to the cost paid per click, &lt;em&gt;CPC&lt;/em&gt;, measured in USD.&lt;/p&gt;
&lt;p&gt;Cost per click is an example of a Poisson distributed variable. We started measuring the CPC during some control period using the current tools, then switched to the new tool in a test period.&lt;/p&gt;
&lt;p&gt;Those data might look something like:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Control&lt;/th&gt;
&lt;th&gt;Test&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CPC&lt;/td&gt;
&lt;td&gt;$10&lt;/td&gt;
&lt;td&gt;$9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cost&lt;/td&gt;
&lt;td&gt;$400&lt;/td&gt;
&lt;td&gt;$450&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Clicks&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The question is: How likely are we to obtain those data under the assumption that there was no change in overall CPC?&lt;/p&gt;
&lt;p&gt;That question specifies the null hypothesis: the two periods of data were generated from the same distribution. Under the alternative hypothesis the test period data might have a greater or lesser CPC.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://www.ucs.louisiana.edu/~kxk4695/JSPI-04.pdf"&gt;Poisson Exact Test&lt;/a&gt; (E-Test) is a hypothesis test for answering this kind of question. In R, you can use &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/poisson.test.html"&gt;&lt;code&gt;poisson.test&lt;/code&gt;&lt;/a&gt;, which implements the similar but inexact Poisson C-Test. But in Python, no such implementation exists.&lt;/p&gt;
&lt;p&gt;I dug up the original &lt;a href="http://www.ucs.louisiana.edu/~kxk4695/statcalc/pois2pval.for"&gt;Fortran code&lt;/a&gt; posted to the academic website belonging to one of the authors of the test. I edited it very minimally so that it could be wrapped by the Numpy &lt;a href="https://docs.scipy.org/doc/numpy/f2py/index.html"&gt;Fortran to Python interface generator&lt;/a&gt;. Then I packaged it and posted on pypi, so you can use it too!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nolanbconaway/poisson-etest"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.org/project/poisson-etest/0.0/"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://travis-ci.org/nolanbconaway/poisson-etest"&gt;Travis CI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="usage"&gt;Usage&lt;/h2&gt;
&lt;p&gt;Just plug in the numbers to get your answer!&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;poisson_etest&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;poisson_etest&lt;/span&gt;

&lt;span class="n"&gt;control_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;
&lt;span class="n"&gt;control_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;
&lt;span class="n"&gt;test_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;450&lt;/span&gt;
&lt;span class="n"&gt;test_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;poisson_etest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;control_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;control_clicks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_clicks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 0.12732580695256054&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We believe more in the estimate of each period's CPC if we have more clicks. That is, I wouldn't say we know much about the CPC of the Test period after only obtaining a single click. Conversely, we have a strong estimate of the CPC after many hundreds.&lt;/p&gt;
&lt;p&gt;This test is useful in that it accounts for the sample size accordingly. Keeping the CPC's the same, lets check on the results if we had over 100 clicks in each period:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;control_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;105&lt;/span&gt;
&lt;span class="n"&gt;test_clicks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;span class="n"&gt;control_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;control_clicks&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;test_cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_clicks&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, the probability that the two were generated from the same distribution is much smaller:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;poisson_etest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;control_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;control_clicks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_clicks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# 0.0034409891789582165&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="blog"></category></entry><entry><title>A freezable dictionary for python.</title><link href="https://nolanbconaway.github.io/blog/2019/a-freezable-dictionary-for-python.html" rel="alternate"></link><published>2019-03-12T00:00:00-04:00</published><updated>2019-03-12T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-03-12:/blog/2019/a-freezable-dictionary-for-python.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Configuring a headless raspberry pi on your home network.</title><link href="https://nolanbconaway.github.io/blog/2019/rpi-setup.html" rel="alternate"></link><published>2019-03-06T00:00:00-05:00</published><updated>2019-03-06T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-03-06:/blog/2019/rpi-setup.html</id><summary type="html">&lt;p&gt;I do a lot of projects that involve raspberry pis. At this point, there are three tucked away in different corners of my apartment (one runs an online &lt;a href="/blog/2018/apartment-temp"&gt;temperature sensor&lt;/a&gt;). They all run headless and I SSH into them occasionally to see if anything has broken.&lt;/p&gt;
&lt;p&gt;I recently set up ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;I do a lot of projects that involve raspberry pis. At this point, there are three tucked away in different corners of my apartment (one runs an online &lt;a href="/blog/2018/apartment-temp"&gt;temperature sensor&lt;/a&gt;). They all run headless and I SSH into them occasionally to see if anything has broken.&lt;/p&gt;
&lt;p&gt;I recently set up a fresh Raspberry Pi 3 B+. The project was to hook up a &lt;a href="https://www.adafruit.com/product/2097"&gt;PiTFT display&lt;/a&gt; and use it to show upcoming Manhattan-bound Q trains.&lt;/p&gt;
&lt;p&gt;The initial setup for a headless raspberry pi can be tricky. Since I've done this more than a handful of times, I thought I'd record the steps I take to configure my new machine.&lt;/p&gt;
&lt;h2 id="install-raspbian"&gt;Install Raspbian&lt;/h2&gt;
&lt;p&gt;I have never been able to remember the syntax to &lt;code&gt;dd&lt;/code&gt;, so I just head over to raspberrypi.org and follow &lt;a href="https://www.raspberrypi.org/documentation/installation/installing-images/README.md"&gt;their instructions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As for the Raspbian distribution, I choose the latest "lite" version (right now it is "Raspbian Stretch Lite") because I don't use the desktop environment. But you do you.&lt;/p&gt;
&lt;h3 id="before-you-proceed"&gt;Before you proceed ...&lt;/h3&gt;
&lt;p&gt;Consider the catch-22 situation you're in. In order to connect to a headless pi, you need to have set up ssh. But you also need to connect to it &lt;em&gt;in order to set up ssh&lt;/em&gt;. Luckily, there's a little trick to set up ssh before the first boot.&lt;/p&gt;
&lt;p&gt;Simply add a blank file, &lt;code&gt;ssh&lt;/code&gt;, to the root of the SD card. I never can remember where my SD card mount lives in the mac filesystem, so I navigate my finder to the mounted SD card and use &lt;a href="https://zipzapmac.com/Go2Shell"&gt;Go2Shell&lt;/a&gt; to put my terminal there. Then a simple &lt;code&gt;touch ssh&lt;/code&gt; command will do the trick.&lt;/p&gt;
&lt;p&gt;If you want to connect to your pi via wifi, you've arrived at yet another catch-22 situation. As far as I know, you need to be able to connect to your pi in order to configure the wifi connection. What a nightmare.&lt;/p&gt;
&lt;p&gt;I don't know any tricks here; for the time being, you'll need to connect to the pi via ethernet. I'll run through how to set up wifi later in this post.&lt;/p&gt;
&lt;h2 id="boot-it-up"&gt;Boot it up&lt;/h2&gt;
&lt;p&gt;With SSH enabled, you should be able to connect to the pi remotely via ethernet the very first time you boot it up. You won't know the machine's IP address ahead of time, but you can get that info a bunch of different ways (I use &lt;a href="https://inetapp.de/en/inetx.html"&gt;iNet Network Scanner&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Once you discover the IP address, ssh in. The default username and password are, of course, &lt;code&gt;pi&lt;/code&gt; and &lt;code&gt;raspberry&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ssh pi@&amp;lt;IP Address&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="step-1-update-upgrade"&gt;Step 1: update / upgrade&lt;/h2&gt;
&lt;p&gt;Get this out of the way. Should take a few mins, depending on the speed of your network.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo apt-get upgrade -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="step-2-raspi-config"&gt;Step 2: raspi-config&lt;/h2&gt;
&lt;p&gt;By default the locale for raspberry pis are set to &lt;code&gt;en_GB.UTF8&lt;/code&gt;. I, like many other people, live in NYC. So I need to change the locale (keyboard layout, character set, timezone, etc) for myself. Do this with the built-in GUI tool &lt;code&gt;raspi-config&lt;/code&gt;. Run it under sudo:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo raspi-config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll get a display that looks like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Raspi Config" class="img-fluid" src="https://nolanbconaway.github.io/blog/2019/main.png"/&gt;&lt;/p&gt;
&lt;p&gt;I end up doing a lot of tinkering in here.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Network Options -&amp;gt; Hostname&lt;/strong&gt;. So that my machine has an informative name on my network.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advanced Options -&amp;gt; Expand Filesystem&lt;/strong&gt;. Gimme &lt;em&gt;all&lt;/em&gt; that sweet, sweet SD memory.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whatever you do, you want to make sure to hit &lt;strong&gt;Localisation Options&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Localisation" class="img-fluid" src="https://nolanbconaway.github.io/blog/2019/localisation.png"/&gt;&lt;/p&gt;
&lt;p&gt;When you hit the "Change Locale" menu, you'll get a very long list of possible locales. Scroll with your arrow keys (it sucks and takes forever) and press spacebar on the to select / deselect.&lt;/p&gt;
&lt;p&gt;Personally, I deselect &lt;code&gt;en_GB.UTF-8 UTF-8&lt;/code&gt; and then select &lt;code&gt;en_US.UTF-8 UTF-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When you're done, hit OK (skip there by pressing TAB). The next screen lets you choose your default locale. Just highlight the one you want and TAB to OK. You should be good to go.&lt;/p&gt;
&lt;p&gt;Next, change your timezone. This works a lot like setting your locale; highlight your region and TAB to OK.&lt;/p&gt;
&lt;h2 id="step-3-set-up-wifi-connection"&gt;Step 3: Set up wifi connection&lt;/h2&gt;
&lt;p&gt;If you want to connect to your raspberry pi on your wifi network, you'll want configure the connection earlier rather than later. I cannot tell you how many times I have lost all ability to connect to my machine (ethernet or otherwise) after messing up the network configuration in an attempt to set up the darn wifi.&lt;/p&gt;
&lt;p&gt;If you might break it all, you'll want to do it &lt;em&gt;before&lt;/em&gt; you invest a lot of time into configuration.&lt;/p&gt;
&lt;p&gt;There are &lt;a href="https://www.google.com/search?q=raspberry+pi+wireless+connection"&gt;like a million posts on how to configure a wifi connection&lt;/a&gt;. The &lt;a href="https://www.raspberrypi.org/documentation/configuration/wireless/wireless-cli.md"&gt;authoritative guide on the subject&lt;/a&gt; can be found on raspberrypi.org, and you might just want to do that.&lt;/p&gt;
&lt;p&gt;As for me, I don't know how &lt;em&gt;not&lt;/em&gt; to break this stuff. So I just copy over config from another, working, raspberry pi. My config looks like:&lt;/p&gt;
&lt;h3 id="etcnetworkinterfaces"&gt;&lt;code&gt;/etc/network/interfaces&lt;/code&gt;&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;source-directory /etc/network/interfaces.d

auto wlan0
iface wlan0 inet manual
wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="etcwpa_supplicantwpa_supplicantconf"&gt;&lt;code&gt;/etc/wpa_supplicant/wpa_supplicant.conf&lt;/code&gt;&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1
country=US

network={
    ssid="network name"
    psk="password"
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="reboot"&gt;Reboot&lt;/h3&gt;
&lt;p&gt;This is the moment of truth. Did you break everything? There's no shame in it. If so, hopefully you can still connect via ethernet, in which case you can tinker around until you get a wifi connection.&lt;/p&gt;
&lt;h2 id="step-4-set-up-a-static-ip"&gt;Step 4: Set up a static IP.&lt;/h2&gt;
&lt;p&gt;If you didn't ruin everything in step 3, you should be able to ssh in your pi again.&lt;/p&gt;
&lt;p&gt;At this point, your raspberry pi will be assigned an IP address each time it connects to your network. The address could change across reboots and I don't like having to check the address each time.&lt;/p&gt;
&lt;p&gt;The solution is to configure a static IP address. If you like pain, there is a way to configure the static address on your raspberry pi. But, to me, that has always felt &lt;em&gt;way&lt;/em&gt; too complicated, &lt;a href="https://raspberrypi.stackexchange.com/questions/37920/how-do-i-set-up-networking-wifi-static-ip-address"&gt;see this Stack Exchange post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I find it much easier to do this via my router's admin panel. Most routers will have a section in which you can specify a DHCP reservation for a specific MAC address. This is just a fancy way of making sure your router assigns a specific IP address to your raspberry pi, and that it never assigns any other device to that address.&lt;/p&gt;
&lt;p&gt;I use an Apple Airport Express, here's what that configuration screen looks like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="airport express network options" class="img-fluid" src="https://discussions.apple.com/content/attachment/466201040"/&gt;&lt;/p&gt;
&lt;h2 id="step-5-enable-passwordless-ssh"&gt;Step 5: Enable passwordless ssh.&lt;/h2&gt;
&lt;p&gt;I hate typing my passwords so I make sure to set up passwordless ssh.&lt;/p&gt;
&lt;p&gt;The idea here is that you create a &lt;code&gt;key&lt;/code&gt; on the machine you are using to ssh into the pi &lt;em&gt;from&lt;/em&gt; (that is, the computer you are in front of). Then you paste that key in a special place on the pi. When you try to log in, ssh will compare the keys and let you in without a password if they match.&lt;/p&gt;
&lt;p&gt;First thing, you'll need to make your key. Keys are unique to the machine you are ssh-ing &lt;em&gt;from&lt;/em&gt;, so if you've done this before you don't need to do it again. &lt;a href="https://docs.joyent.com/public-cloud/getting-started/ssh-keys/generating-an-ssh-key-manually/manually-generating-your-ssh-key-in-mac-os-x"&gt;Here's a tutorial on how you'd make an ssh key on a mac&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Your key will probably live in &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; and will look something like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ssh-rsa &amp;lt;lots of letters and numbers&amp;gt;&lt;span class="o"&gt;==&lt;/span&gt; user@host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One you have that key copy-pasteable, you can run this command on your raspberry pi:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; mkdir .ssh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; nano .ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Paste the key from your computer in that bad boy, save the file (&lt;code&gt;ctrl-O&lt;/code&gt;), then log out and log back in. When you log back in, you shouldn't be asked for your password.&lt;/p&gt;
&lt;h2 id="step-6-change-the-username-and-password"&gt;Step 6: Change the username and password.&lt;/h2&gt;
&lt;p&gt;Everybody knows the default login for a raspberry pi (or at least, lots of people). So for security you'll want to change them. Changing the username can be a bit of a hassle since you need to move around the user home, etc, so I only change the username if I plan on opening my firewall to allow outside connections (gotta have that &lt;em&gt;extra&lt;/em&gt; security).&lt;/p&gt;
&lt;p&gt;In either case, &lt;em&gt;definitely&lt;/em&gt; change the password. You can do that with:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;passwd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So easy.&lt;/p&gt;
&lt;h2 id="step-7-installations"&gt;Step 7: Installations&lt;/h2&gt;
&lt;p&gt;At this point:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Your raspberry pi has updated packages and the right locale.&lt;/li&gt;
&lt;li&gt;The network connectivity is in place.&lt;/li&gt;
&lt;li&gt;The same IP address is assigned to your machine each time you reboot.&lt;/li&gt;
&lt;li&gt;It is at least kind of secure with a nondefault password.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now you get to install the stuff you want to use for your projects. Each project requires different tooling, so below I'll list some of the common items I install each time:&lt;/p&gt;
&lt;h3 id="mailutils"&gt;mailutils&lt;/h3&gt;
&lt;p&gt;I run a lot of stuff in cron and I want to make sure I get notified if a job fails. If you install mailtutils, cron errors will automatically go to mail and you'll get an indicator if there was a failure when you log in.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get install mailutils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="zsh-oh-my-zsh"&gt;zsh, oh-my-zsh&lt;/h3&gt;
&lt;p&gt;I love zsh. &lt;em&gt;Love it&lt;/em&gt;. It's even better when you use &lt;a href="https://github.com/robbyrussell/oh-my-zsh"&gt;oh-my-zsh&lt;/a&gt; as a configuration manager.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get install zsh&lt;span class="p"&gt;;&lt;/span&gt;
sh -c &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's all. Have fun spending the next hour &lt;a href="https://github.com/robbyrussell/oh-my-zsh/wiki/Themes"&gt;picking your theme&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="python"&gt;python&lt;/h3&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Dec 2019&lt;/strong&gt;: these days I am all about pyenv and I would probably opt for that instead.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I use python for basically everything. I prefer to keep the raspberry pi pre-installed python clean, so I opt to install my own python.&lt;/p&gt;
&lt;p&gt;There are a bunch if tutorials on how to build a python distribution on a raspberry pi. &lt;a href="https://gist.github.com/SeppPenner/6a5a30ebc8f79936fa136c524417761d"&gt;Here is one&lt;/a&gt;. Even if you know what you're doing, it takes freaking forever to &lt;code&gt;configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install&lt;/code&gt; the thing. If you do not know what you are doing (I do not know what &lt;em&gt;I&lt;/em&gt; am doing), it'll take you that amount of freaking forever to learn something is wrong and that you have to do a different &lt;code&gt;configure &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Learn from my mistakes. Use &lt;a href="https://github.com/jjhelmus/berryconda"&gt;Berryconda&lt;/a&gt;. Berryconda is pre-built conda distribution for raspberry pi. It only takes a few minutes to set up and it "just works". Follow the instructions on the github page to install. It should be something like:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wget https://github.com/jjhelmus/berryconda/releases/download/v2.0.0/Berryconda3-2.0.0-Linux-armv7l.sh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
chmod +x Berryconda3-2.0.0-Linux-armv7l.sh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
./Berryconda3-2.0.0-Linux-armv7l.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That'll walk you through the installation.&lt;/p&gt;
&lt;p&gt;If you installed zsh, you'll want to make sure to put the berryconda binary directory on your path (so that zsh knows what to do when you refer to &lt;code&gt;python&lt;/code&gt;). Add a line like this to your &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$PATH&lt;/span&gt;&lt;span class="s2"&gt;:&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;&lt;span class="s2"&gt;/berryconda3/bin"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where &lt;code&gt;$HOME/berryconda3/bin&lt;/code&gt; is wherever you decided to install berryconda. Then, you should confirm that your python3 command refers to the berryconda install:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;which python3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Should output something like &lt;code&gt;/home/pi/berryconda3/bin/python3&lt;/code&gt; not &lt;code&gt;/usr/bin/python3&lt;/code&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>An update on the softmax function for numpy.</title><link href="https://nolanbconaway.github.io/blog/2019/softmax-numpy-update.html" rel="alternate"></link><published>2019-01-05T00:00:00-05:00</published><updated>2019-01-05T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2019-01-05:/blog/2019/softmax-numpy-update.html</id><summary type="html">&lt;p&gt;As of early 2019, my post on a &lt;a href="/blog/2017/softmax-numpy"&gt;softmax function for numpy&lt;/a&gt; accounts for 83% of traffic to my website (~16k views in 2018). I recently found that, as of version 1.2.0, scipy has included an implementation of the softmax in its special functions.&lt;/p&gt;
&lt;p&gt;Some info on the ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;p&gt;As of early 2019, my post on a &lt;a href="/blog/2017/softmax-numpy"&gt;softmax function for numpy&lt;/a&gt; accounts for 83% of traffic to my website (~16k views in 2018). I recently found that, as of version 1.2.0, scipy has included an implementation of the softmax in its special functions.&lt;/p&gt;
&lt;p&gt;Some info on the change:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html#scipy.special.softmax"&gt;SciPy docs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scipy/scipy/pull/8872"&gt;Merged pull request&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scipy/scipy/pull/8556/commits/02d0ac2dea6bd2ad11ddf6c6022b3bae881c961a#diff-86dbed1918e224062ad4239fe5d14041R188"&gt;Feeding my ego&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Originally, the proposed function looked very much like the one I had posted, but then &lt;a href="https://github.com/pv"&gt;a very smart person&lt;/a&gt; realized that you could use existing tooling around &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html#scipy.special.logsumexp"&gt;scipy.special.logsumexp&lt;/a&gt; to make things much cleaner.&lt;/p&gt;
&lt;p&gt;So now the function is &lt;a href="https://github.com/scipy/scipy/blob/master/scipy/special/_logsumexp.py#L215"&gt;a freaking one-liner&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keepdims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I 100% prefer importing a function from scipy over pasting a hand-written one. Especially given the beauty of that puppy. But the two functions are not totally swappable, there are a couple important changes in behavior:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;SciPy computes the softmax over &lt;em&gt;all&lt;/em&gt; array elements by default&lt;/strong&gt;. It will keep the shape but the &lt;em&gt;entire&lt;/em&gt; result will sum to 1. My old function computed the softmax over the first non-singleton dimension like in MATLAB.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;There is no &lt;code&gt;theta&lt;/code&gt; multiplier&lt;/strong&gt;. In my function, a &lt;code&gt;theta&lt;/code&gt; parameter was accepted to control determinism, but that was not implemented in the scipy function. No biggie, you'll just need to do &lt;code&gt;softmax(X*theta)&lt;/code&gt; instead of &lt;code&gt;softmax(X, theta=theta)&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="a-quick-performance-comparison"&gt;A quick performance comparison&lt;/h2&gt;
&lt;p&gt;I figured scipy had some magic stuff going on with &lt;code&gt;logsumexp&lt;/code&gt;, and their function would be both more beautiful &lt;em&gt;and&lt;/em&gt; performant than mine.&lt;/p&gt;
&lt;p&gt;But I ran a quick handful of tests just for kicks. First I wrote up a decorator to print function exec time to the console and wrapped my softmax and scipy softmax in it.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;wraps&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;scipy.special&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;timeit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Decorator to print function exec time."""&lt;/span&gt;
    &lt;span class="nd"&gt;@wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;te&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; took: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;te&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ts&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s1"&gt;2.4f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;s'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrap&lt;/span&gt;


&lt;span class="nd"&gt;@timeit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax_nolan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""My func without those pesky comments and docs."""&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;ax_sum&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;


&lt;span class="nd"&gt;@timeit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax_scipy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Scipy softmax with the axis inference and theta included."""&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;special&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I ran each function on 2D arrays of varying sizes and checked for differences in the results.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sizes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10000000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sizes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;arr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'Running: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result_nolan&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;softmax_nolan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;result_scipy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;softmax_scipy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# check for differences&lt;/span&gt;
    &lt;span class="n"&gt;max_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result_scipy&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;result_nolan&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;max_diff&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_diff&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://nolanbconaway.github.io/blog/2019/test.py"&gt;Here is the full script&lt;/a&gt;, in case you want to toy around with it.&lt;/p&gt;
&lt;h3 id="results"&gt;Results&lt;/h3&gt;
&lt;p&gt;My function was ... faster?&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Running: (10000,)
softmax_nolan took: 0.0004s
softmax_scipy took: 0.0162s

Running: (1000, 1000)
softmax_nolan took: 0.0210s
softmax_scipy took: 0.0259s

Running: (10000, 100)
softmax_nolan took: 0.0132s
softmax_scipy took: 0.0233s

Running: (100, 10000)
softmax_nolan took: 0.0141s
softmax_scipy took: 0.0198s

Running: (100000, 100)
softmax_nolan took: 0.2186s
softmax_scipy took: 0.2869s

Running: (100, 100000)
softmax_nolan took: 0.1633s
softmax_scipy took: 0.2740s

Running: (10, 1000000)
softmax_nolan took: 0.1962s
softmax_scipy took: 0.2792s

Running: (10, 10000000)
softmax_nolan took: 3.3774s
softmax_scipy took: 4.4439s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;My function runs an exponential one time, whereas scipy runs the array through &lt;code&gt;logsumexp&lt;/code&gt; and then exponentializes again. So maybe that's why we get these results? I don't really know what &lt;code&gt;logsumexp&lt;/code&gt; does under the hood.&lt;/p&gt;
&lt;p&gt;Anyway, I still prefer scipy softmax to reduce the dependency on a handwritten function.&lt;/p&gt;
&lt;p&gt;Some notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I get similar results every time I ran the script.&lt;/li&gt;
&lt;li&gt;I use a cruddy pip install on python 3.7.1. I don't know what would happen if those speedy conda optimizations were in play.&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>A flask app for a thermometer in my apartment.</title><link href="https://nolanbconaway.github.io/blog/2018/a-flask-app-for-a-thermometer-in-my-apartment.html" rel="alternate"></link><published>2018-12-05T00:00:00-05:00</published><updated>2018-12-05T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2018-12-05:/blog/2018/a-flask-app-for-a-thermometer-in-my-apartment.html</id><content type="html"></content><category term="app"></category></entry><entry><title>I scraped the text of every Strongbad Email.</title><link href="https://nolanbconaway.github.io/blog/2018/i-scraped-the-text-of-every-strongbad-email.html" rel="alternate"></link><published>2018-11-04T00:00:00-04:00</published><updated>2018-11-04T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2018-11-04:/blog/2018/i-scraped-the-text-of-every-strongbad-email.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Jarjar 3.0 Released.</title><link href="https://nolanbconaway.github.io/blog/2018/jarjar-30-released.html" rel="alternate"></link><published>2018-06-20T00:00:00-04:00</published><updated>2018-06-20T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2018-06-20:/blog/2018/jarjar-30-released.html</id><content type="html"></content><category term="github"></category></entry><entry><title>Jeff Zemla and I developed Python and Bash code for sending notifications to Slack.</title><link href="https://nolanbconaway.github.io/blog/2017/jeff-zemla-and-i-developed-python-and-bash-code-for-sending-notifications-to-slack.html" rel="alternate"></link><published>2017-06-29T00:00:00-04:00</published><updated>2017-06-29T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-06-29:/blog/2017/jeff-zemla-and-i-developed-python-and-bash-code-for-sending-notifications-to-slack.html</id><content type="html"></content><category term="github"></category></entry><entry><title>What I found in 18,000 Pitchfork album reviews.</title><link href="https://nolanbconaway.github.io/blog/2017/pitchfork-roundup.html" rel="alternate"></link><published>2017-06-17T00:00:00-04:00</published><updated>2017-06-17T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-06-17:/blog/2017/pitchfork-roundup.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Over the Winter of 2016-2017, I scraped over 18,000 reviews published on &lt;a href="http://pitchfork.com/"&gt;Pitchfork&lt;/a&gt;. I published the dataset on Github, and wrote &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/author-autocorrelation.ipynb"&gt;several&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb"&gt;Jupyter&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/artist-development.ipynb"&gt;Notebooks&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/review-score-exploration.ipynb"&gt;exploring&lt;/a&gt; the data. This post provides a discussion about what I found. For a more code-driven approach, check out the notebooks on &lt;a href="https://github.com/nolanbconaway/pitchfork-data"&gt;Github&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Over the Winter of 2016-2017, I scraped over 18,000 reviews published on &lt;a href="http://pitchfork.com/"&gt;Pitchfork&lt;/a&gt;. I published the dataset on Github, and wrote &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/author-autocorrelation.ipynb"&gt;several&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb"&gt;Jupyter&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/artist-development.ipynb"&gt;Notebooks&lt;/a&gt; &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/review-score-exploration.ipynb"&gt;exploring&lt;/a&gt; the data. This post provides a discussion about what I found. For a more code-driven approach, check out the notebooks on &lt;a href="https://github.com/nolanbconaway/pitchfork-data"&gt;Github&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As a person who reads a fair amount of music criticism, and as a scientist, I've long been curious about the degree of objectivity in reviews of new releases. There is of course no such thing as an &lt;em&gt;objective&lt;/em&gt; assessment for the quality of a piece music, and reviews are based on the personal taste of each reviewer. But also, readers clearly take something away from music reviews (otherwise why would they be read?), and major publications are thought to have wide influence on the behavior of listeners (though, as far as I can tell, there's no data to back up that claim).&lt;/p&gt;
&lt;p&gt;So, while subjective, reviews are at least &lt;em&gt;interpreted&lt;/em&gt; as authoritative. Setting aside routine subjectivity having to do with taste, the scientist in me wonders about sources of bias observable across many authors. To address my curiosity, I collected over 18,000 reviews published on &lt;a href="http://pitchfork.com/"&gt;Pitchfork&lt;/a&gt; between January 1999 and January 2017. I chose Pitchfork because it is widely viewed as authoritative, publishes a large amount of content (25+ reviews per week), and offers a precise scoring (0.0-10.0) of each album. In this post, I'll describe what I found.&lt;/p&gt;
&lt;h2 id="the-reviews"&gt;The Reviews&lt;/h2&gt;
&lt;p&gt;Since 1998, Pitchfork has published five new reviews every weekday. In 2016, Pitchfork added an additional five reviews on Saturdays, as well as a "Sunday Reviews" section containing long-form articles on a classic albums (one review per week, published on Sundays). They've also occasionally published more targeted series of reviews (such as following the deaths of &lt;a href="http://pitchfork.com/artists/438-david-bowie/"&gt;David Bowie&lt;/a&gt; and &lt;a href="http://pitchfork.com/artists/3397-prince/"&gt;Prince&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The typical review addresses a single release by a specific artist, and is attributed to a single author. All reviews are given a numerical score (0.0-10.0), are labeled by genre, report the record label (if any), and contain several paragraphs of text (the review itself). Since 2003, authors have awarded some releases the title "&lt;em&gt;Best New Music&lt;/em&gt;", which means what you'd think. Best New Music albums are displayed prominently on Pitchfork, and even have their own &lt;a href="http://pitchfork.com/reviews/best/albums/"&gt;page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Album scores and Best New Music awards will be the focus of my analysis, so it's worth taking a look at those distributions.&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2017/score-bmn-hist.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Note: Most high scoring albums that are &lt;em&gt;not&lt;/em&gt; Best New Music are from reviews prior to the advent of Best New Music, or are reviews of classic releases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A little more than 5% of reviews are awarded Best New Music. Most scores lie between 6.4 and 7.8, with the average review getting a score of 7.0. Best New Music is typically awarded to albums scoring greater than 8.4. Scores between 8.1 and 8.4 have a decent shot at Best New Music, but many albums in that range are not given the title. &lt;/p&gt;
&lt;h2 id="statistical-heaping"&gt;Statistical Heaping&lt;/h2&gt;
&lt;p&gt;Inspired by &lt;a href="https://gutterstats.wordpress.com/2015/11/03/are-nfl-officials-biased-with-their-ball-placement/"&gt;this really cool blog post&lt;/a&gt;, I thought about what a reviewer might do if they have a &lt;em&gt;general&lt;/em&gt; sense of an album's score, but they need to pick a &lt;em&gt;specific&lt;/em&gt; value. Is this a 7.8? 7.9? 7.7? The above mentioned post found that NFL officials tend to place the line of scrimmage at tidy yard numbers (i.e., 10s and 5s). The technical term for this is &lt;a href="http://ww2.amstat.org/sections/SRMS/Proceedings/y1958/Patterns%20Of%20Heaping%20In%20The%20Reporting%20Of%20Numerical%20Data.pdf"&gt;statistical heaping&lt;/a&gt;, and its observed commonly in survey data. So maybe Pitchfork reviewers behave similarly? I counted the number of scores at each decimal place (e.g., \( &lt;em&gt;.0, &lt;/em&gt;.1... *.9 \)), here's the result:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2017/score-anchor-points.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;Those diamond markers show what you'd expect if Pitchfork reviewers were totally unbiased. The &lt;em&gt;Uniform Sampling&lt;/em&gt; model is what you'd get if you picked scores at random; the &lt;em&gt;Normal Sampling&lt;/em&gt; model shows what you'd get if you picked scores around a normal distribution  based on the observed scores (\(\mu=7.006, \sigma = 1.294\)). *.0 gets a slight bump in the uniform model because 10.0 is a possibility, but 10.1-10.9 are not. &lt;/p&gt;
&lt;p&gt;Obviously, the tidy, round &lt;em&gt;.0 value is much more frequently chosen than you'd expect given either sampling technique: _It's nearly twice as frequent as &lt;/em&gt;.1_. There's also a slight bump at the &lt;em&gt;0.5 and &lt;/em&gt;.8  marks, but those values are a bit weaker. The point is, Pitchfork reviewers absolutely show the heaping behavior: at least in this sense, the review scores are biased. &lt;/p&gt;
&lt;h2 id="borderline-best-new-music-decisions"&gt;Borderline Best New Music Decisions&lt;/h2&gt;
&lt;p&gt;Still, its important to consider that the impact of the heaping is not &lt;em&gt;huge&lt;/em&gt;. We're taking about a reviewer picking between, for example, a score of 7.0 or 7.1. A more impactful difference lies in the choice to award &lt;em&gt;Best New Music&lt;/em&gt;. As I noted above, while most releases scoring 8.5 or better are given the award, releases scoring between 8.1 and 8.4 have a shot but it is not guaranteed. I've long wondered about how these decisions are made, so I looked into it.&lt;/p&gt;
&lt;p&gt;I reduced the dataset to the borderline Best New Music candidates: there are 1223 in all, 269 (22%) of which are Best New Music. I checked out a bunch of possible explanations for why some get Best New Music and some do not; for full disclosure I'll list them here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are "tougher" authors (who give out lower scores) more or less likely to award Best New Music? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Is an artist's first album more likely be awarded Best New Music? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Are authors less likely to grant the award if they have less expertise in the genre? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Are authors less likely to grant the award if they recently awarded it to another album? &lt;em&gt;No&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To Pitchfork's credit: all of these would be reasonable biases to observe and I found little evidence of each. I &lt;em&gt;did&lt;/em&gt; find that &lt;a href="https://twitter.com/nolanbconaway/status/875568013050658818"&gt;some genres are more likely to be considered Best New Music&lt;/a&gt;, but I'm not sure if that constitutes bias or just Pitchfork's focus.&lt;/p&gt;
&lt;p&gt;But when I looked into whether some record labels where favored over others, the results were striking. I computed the proportion of borderline cases from each label that were Best new Music; most only had one or two borderlines cases, but here are the labels with at least ten:&lt;/p&gt;
&lt;object data="https://nolanbconaway.github.io/blog/2017/borderline-by-label.svg" type="image/svg+xml"&gt;&lt;/object&gt;
&lt;p&gt;The first thing to note is that the labels are all major labels or at least big-name indie labels (with the exception of "self-released"). The second thing to note is the huge degree of differences between labels: &lt;a href="https://en.wikipedia.org/wiki/4AD"&gt;4AD&lt;/a&gt; got Best New Music on 8/14 borderline cases, and &lt;a href="https://en.wikipedia.org/wiki/Thrill_Jockey"&gt;Thrill Jockey&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Relapse_Records"&gt;Relapse&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Nonesuch_Records"&gt;Nonesuch&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Anti-_(record_label)"&gt;Anti-&lt;/a&gt; collectively went 0 out of 44. &lt;/p&gt;
&lt;p&gt;That &lt;em&gt;feels&lt;/em&gt; unlikely, but are those differences routine? I sent each label's data through an unbiased model, where the probability of \(k\) Best New Musics out of \(n\) borderline cases follows a Binomial distribution, with \( p=0.22 \) (the overall proportion of Best New Music among borderline cases). This model (depicted in the figure) shows how likely each label's data is &lt;em&gt;individually&lt;/em&gt;, but it does not really address the question: How probable is the data &lt;em&gt;collectively&lt;/em&gt;?  How likely are we to observe four labels with zero Best New Music? How likely are we to observe four labels with more than 40% Best new Music? How likely are we to observe both scenarios simultaneously?&lt;/p&gt;
&lt;p&gt;So I did some Monte Carlo sampling. I simulated each label's Best New Music record 1 million times using the above Binomial distributions. Out of the 1,000,000 samples, 4523 (0.4523%) contained four or more labels with zero Best New Music, 22469 (2.2469%) contained four or more labels with more than 30% Best New Music, and only &lt;em&gt;42&lt;/em&gt; (&lt;em&gt;0.0042%&lt;/em&gt;) had both. Obviously, the data we have is &lt;em&gt;very unlikely&lt;/em&gt; to occur, assuming each label were awarded Best New Music with equal probability.&lt;/p&gt;
&lt;p&gt;Unfortunately, I do not have a clear idea as to why record labels are treated differently. I certainty wouldn't go as far as to suggest that there is &lt;em&gt;overt&lt;/em&gt; favoritism. My best theory is that maybe authors &lt;em&gt;expect&lt;/em&gt; Best New Music from some labels and they do not expect it from others. These expectations wouldn't usually influence decisions because most releases are clearly Best new Music or they are not, but they emerge in borderline cases. &lt;a href="mailto:nolanbconaway@gmail.com"&gt;Let me know&lt;/a&gt; if you can think of a way to test that theory!&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;The reason why I was interested in revealing these sources of bias is that music reviews carry with them a sense of authority: that the favorability of a review is in some sense objective. But, obviously, authors are people, and &lt;a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases#Decision-making.2C_belief.2C_and_behavioral_biases"&gt;people are biased&lt;/a&gt;. So it's no surprise that, once you get digging into the data, you can find evidence of all sorts of biases. &lt;/p&gt;
&lt;p&gt;To their credit, in this post I did &lt;em&gt;not&lt;/em&gt; report many of the analyses I conducted which uncovered no evidence of bias (like &lt;a href="https://twitter.com/nolanbconaway/status/873754026080436224"&gt;this one&lt;/a&gt;, or &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/reviewer-development.ipynb"&gt;this one&lt;/a&gt;, or &lt;a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb"&gt;this one&lt;/a&gt;). Of course, that's not exactly evidence that there is &lt;em&gt;no&lt;/em&gt; bias. But, Pitchfork reviewers are professionals, and my guess is that many of them have considered these sorts of biases before, and may even attempt to combat their influence.&lt;/p&gt;
&lt;p&gt;As always, feel free to &lt;a href="mailto:nolanbconaway@gmail.com"&gt;get in touch&lt;/a&gt; if you have comments/questions, or even if you're just curious about some aspect of the data and you don't want to do the analysis yourself.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>I logged the stats for every ride I made in RollerCoaster Tycoon for iPad.</title><link href="https://nolanbconaway.github.io/blog/2017/i-logged-the-stats-for-every-ride-i-made-in-rollercoaster-tycoon-for-ipad.html" rel="alternate"></link><published>2017-05-23T00:00:00-04:00</published><updated>2017-05-23T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-05-23:/blog/2017/i-logged-the-stats-for-every-ride-i-made-in-rollercoaster-tycoon-for-ipad.html</id><content type="html"></content><category term="github"></category></entry><entry><title>A softmax function for numpy.</title><link href="https://nolanbconaway.github.io/blog/2017/softmax-numpy.html" rel="alternate"></link><published>2017-03-15T00:00:00-04:00</published><updated>2017-03-15T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-03-15:/blog/2017/softmax-numpy.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;h4 id="update-jan-2019-scipy-120-now-includes-the-softmax-as-a-special-function-its-really-slick-use-it-here-are-some-notes"&gt;Update (Jan 2019): SciPy (1.2.0) now includes the softmax as a special function. It's really slick. Use it. &lt;a href="/blog/2019/softmax-numpy-update"&gt;Here are some notes&lt;/a&gt;.&lt;/h4&gt;
&lt;/blockquote&gt;
&lt;hr/&gt;
&lt;p&gt;I use the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; function &lt;em&gt;constantly&lt;/em&gt;. It's handy anytime I need to model choice among a set of mutually exclusive options. In the canonical example, you ‚Ä¶&lt;/p&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;h4 id="update-jan-2019-scipy-120-now-includes-the-softmax-as-a-special-function-its-really-slick-use-it-here-are-some-notes"&gt;Update (Jan 2019): SciPy (1.2.0) now includes the softmax as a special function. It's really slick. Use it. &lt;a href="/blog/2019/softmax-numpy-update"&gt;Here are some notes&lt;/a&gt;.&lt;/h4&gt;
&lt;/blockquote&gt;
&lt;hr/&gt;
&lt;p&gt;I use the &lt;a href="https://en.wikipedia.org/wiki/Softmax_function"&gt;softmax&lt;/a&gt; function &lt;em&gt;constantly&lt;/em&gt;. It's handy anytime I need to model choice among a set of mutually exclusive options. In the canonical example, you have some metric of evidence, \(X = \{ X_1, X_2, ... X_n\} \), that an item belongs to each of \(N\) classes: \( C = \{C_1, C_2, ... C_n\} \). \(X\) can only belong to one class, and larger values indicate more evidence for class membership. So you need to convert the relative amounts of evidence into probabilities of membership within each of the classes.&lt;/p&gt;
&lt;p&gt;That's what the softmax function is for. Below I have written the mathematics, but idea is simple: you divide each element of \(X\) by the sum of all the elements:&lt;/p&gt;
&lt;p&gt;$$
p(C_n) =
\frac{ \exp{\theta \cdot X_n} }
{ \sum_{i=1}^{N}{\exp {\theta \cdot X_i} } }
$$&lt;/p&gt;
&lt;p&gt;The use of exponentials serves to normalize \(X\), and it also allows the function to be parameterized. In the above equation, I threw in a free parameter, \(\theta\) (\(\theta \geq 0\)), that broadly controls &lt;em&gt;determinism&lt;/em&gt;. Within the exponentiation, \(\theta\) makes larger values of  \(X\) larger-er, so if you set \(\theta\) to a large value, the softmax really squashes things: Elements of \(X\) with anything but the largest values will have a probability very close to zero. When \(\theta = 1\), it's as if the parameter was never there.&lt;/p&gt;
&lt;p&gt;I use this sort of function all the time to simulate how people make decisions based on evidence. But unfortunately, there is no built in &lt;code&gt;numpy&lt;/code&gt; function to compute the softmax. For years I have been writing code like this:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# evidence for each choice&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;                         &lt;span class="c1"&gt;# determinism parameter&lt;/span&gt;

&lt;span class="n"&gt;ps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ps&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Of course, usually &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;theta&lt;/code&gt; come from somewhere else. This works well if you are only simulating one decision: the softmax requires literally two lines of code and its easily readable. But things get thornier if you want to simulate many choices. For example, what if &lt;code&gt;X&lt;/code&gt; is a matrix where rows correspond to the different choices, and the columns correspond to the options?&lt;/p&gt;
&lt;p&gt;In the 2D case, you can either run a loop through the rows of &lt;code&gt;X&lt;/code&gt; or use &lt;code&gt;numpy&lt;/code&gt; matrix &lt;a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"&gt;broadcasting&lt;/a&gt;. Here's the loop solution:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;6.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;9.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;])&lt;/span&gt;  

&lt;span class="c1"&gt;# looping through rows of X&lt;/span&gt;
&lt;span class="n"&gt;ps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's not &lt;em&gt;terrible&lt;/em&gt;, but you can imagine that it's annoying to write one of those &lt;em&gt;every time&lt;/em&gt; you need to softmax. Likewise, you'd have to change up the code if you wanted to softmax over columns rather than rows. Or for that matter, what if &lt;code&gt;X&lt;/code&gt; was a 3D-array, and you wanted to compute softmax over the third dimension?&lt;/p&gt;
&lt;p&gt;At this point it feels more useful to write a generalized softmax function.&lt;/p&gt;
&lt;h2 id="my-softmax-function"&gt;My softmax function&lt;/h2&gt;
&lt;p&gt;After years of copying one-off softmax code between scripts, I decided to make things a little &lt;a href="https://en.wikipedia.org/wiki/Don't_repeat_yourself"&gt;dry&lt;/a&gt;-er: I sat down and wrote a darn softmax function. The goal was to support \(X\) of any dimensionality, and to allow the user to softmax over an arbitrary axis. Here's the function:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Compute the softmax of each element along an axis of X.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    X: ND-Array. Probably should be floats.&lt;/span&gt;
&lt;span class="sd"&gt;    theta (optional): float parameter, used as a multiplier&lt;/span&gt;
&lt;span class="sd"&gt;        prior to exponentiation. Default = 1.0&lt;/span&gt;
&lt;span class="sd"&gt;    axis (optional): axis to compute values along. Default is the&lt;/span&gt;
&lt;span class="sd"&gt;        first non-singleton axis.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns an array the same size as X. The result will sum to 1&lt;/span&gt;
&lt;span class="sd"&gt;    along the specified axis.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;

    &lt;span class="c1"&gt;# make X at least 2d&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;atleast_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# find axis&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# multiply y against the theta parameter,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# subtract the max for numerical stability&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# exponentiate y&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# take the sum along the specified axis&lt;/span&gt;
    &lt;span class="n"&gt;ax_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# finally: divide elementwise&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;ax_sum&lt;/span&gt;

    &lt;span class="c1"&gt;# flatten if X was 1D&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="whats-up-with-that-max-subtraction"&gt;What's up with that max subtraction?&lt;/h2&gt;
&lt;p&gt;The only aspect of this function that does not directly correspond to something in the softmax equation is the subtraction of the maximum from each of the elements of &lt;code&gt;X&lt;/code&gt;. This is done for stability reasons: when you exponentiate even large-ish numbers, the result can be quite large. &lt;code&gt;numpy&lt;/code&gt; will return &lt;code&gt;inf&lt;/code&gt; when you exponentiate values over 710 or so. So if values of &lt;code&gt;X&lt;/code&gt; aren't limited to some fixed range (e.g., \([0...1]\)), or even if you let &lt;code&gt;theta&lt;/code&gt; take on any value, you run the distinct possibility of hitting the &lt;code&gt;inf&lt;/code&gt; ceiling.&lt;/p&gt;
&lt;p&gt;But! If you subtract the maximum value from each element, the largest pre-exponential value will be zero, thus avoiding numerical instability.&lt;/p&gt;
&lt;p&gt;So that solves the numerical stability problem, but is it &lt;em&gt;mathematically&lt;/em&gt; correct? To clear this up, let's write out the softmax equation with the subtraction terms in there. To keep it simple, I've also removed the \(\theta\) parameter:&lt;/p&gt;
&lt;p&gt;$$
p(C_n) =
\frac{ \exp{ X_n - \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i - \max(X) } } }
$$&lt;/p&gt;
&lt;p&gt;Subtracting within an exponent is the same as dividing between exponents (&lt;a href="http://www.rapidtables.com/math/number/exponent.htm"&gt;remember&lt;/a&gt;? \(e^{a-b} = e^a / e^b\)), so:&lt;/p&gt;
&lt;p&gt;$$
\frac{ \exp{ X_n - \max(X) }  }
{ \sum_{i=1}^{N}{\exp { X_i - \max(X) } } }
= \frac{ \exp { X_n  } \div \exp { \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i } \div \exp { \max(X) } } }
$$&lt;/p&gt;
&lt;p&gt;Then you just cancel out the maximum terms, and you're left with the original equation:&lt;/p&gt;
&lt;p&gt;$$
\frac{ \exp { X_n  } \div \exp { \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i } \div \exp { \max(X) } } } =
\frac{ \exp{ X_n } }
{ \sum_{i=1}^{N}{\exp { X_i } } }
$$&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;The function works beautifully and has a nice safeguard against overflow in the exponential. And, if you're like me, including it will prevent you from writing a handful of one-off implementations of the softmax. I'll round this out with a few examples of its usage:&lt;/p&gt;
&lt;h3 id="2d-usage"&gt;2D Usage&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;6.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;3.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;8.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;9.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;7.4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# softmax over rows&lt;/span&gt;
&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.055&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.407&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.015&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.413&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.822&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.165&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.395&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.152&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.123&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.428&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.59&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.435&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="c1"&gt;# softmax over columns&lt;/span&gt;
&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.031&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.22&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.054&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.695&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.204&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.039&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.645&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.112&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.022&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.072&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.68&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.226&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="c1"&gt;# softmax over columns, and squash it!&lt;/span&gt;
&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;500.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="3d-and-beyond"&gt;3D (and beyond!)&lt;/h3&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mf"&gt;0.844&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.237&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.364&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.768&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.811&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.959&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.511&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.06&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.594&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.029&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.963&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.292&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.463&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.869&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.704&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.786&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.173&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.89&lt;/span&gt; &lt;span class="p"&gt;]]])&lt;/span&gt;


&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mf"&gt;0.575&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.425&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.45&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.55&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.482&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.518&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

           &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.556&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.444&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.57&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.43&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.583&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.417&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

           &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;0.449&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.551&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.49&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.51&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.411&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.589&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="blog"></category></entry><entry><title>Solving nonlinearly separable classifications in a single layer neural network.</title><link href="https://nolanbconaway.github.io/blog/2017/solving-nls.html" rel="alternate"></link><published>2017-01-15T00:00:00-05:00</published><updated>2017-01-15T00:00:00-05:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2017-01-15:/blog/2017/solving-nls.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Recently, &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt; (my graduate advisor) and I figured out a unique solution to the famous limitation that single-layer neural networks cannot solve nonlinearly separable classifications. We published our findings in &lt;a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00931"&gt;Neural Computation&lt;/a&gt;. This post is intended to provide a more introductory-level description of our solution. Read the paper for ‚Ä¶&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Recently, &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt; (my graduate advisor) and I figured out a unique solution to the famous limitation that single-layer neural networks cannot solve nonlinearly separable classifications. We published our findings in &lt;a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00931"&gt;Neural Computation&lt;/a&gt;. This post is intended to provide a more introductory-level description of our solution. Read the paper for a more formal report!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In an &lt;a href="https://en.wikipedia.org/wiki/Perceptrons_(book)"&gt;influential book&lt;/a&gt; published in 1969,  Marvin Minsky and Seymour Papert proved that the conventional neural networks of the day could not solve nonlinearly separable (NLS) classifications.&lt;/p&gt;
&lt;p&gt;Their conclusions spurred a decline in research on neural network models during the following two decades. It wasn't until the popularization of the &lt;a href="https://en.wikipedia.org/wiki/Backpropagation"&gt;backpropagation algorithm&lt;/a&gt; in the 1980s, which enabled models to solve NLS classifications through learning an additional layer of weights, that interest picked back up. But even after backprop, and after the advent of methods to train up &lt;a href="https://en.wikipedia.org/wiki/Deep_learning"&gt;deep networks&lt;/a&gt;, the conventional wisdom has been that the only way to solve a NLS classification is by &lt;em&gt;adding layers of weights&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Recently, &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt; (my graduate advisor) and I figured out how you can solve NLS classifications with only a single layer of weights. In this post, I'll explain how!&lt;/p&gt;
&lt;h2 id="what-is-a-nonlinearly-separable-classification"&gt;What is a nonlinearly separable classification?&lt;/h2&gt;
&lt;p&gt;Nonlinearly separable classifications are most straightforwardly understood through contrast with linearly separable ones: if a classification is linearly separable, you can draw a line to separate the classes. &lt;/p&gt;
&lt;p&gt;Below is an example of each. Imagine you are trying to discriminate between two classes, A and B, on the basis of two input dimensions (\(D_1\) and \(D_2\)):&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" src="https://nolanbconaway.github.io/blog/2017/ls-nls.png"/&gt;&lt;/p&gt;
&lt;p&gt;The NLS problem above is the ubiquitous &lt;a href="https://en.wikipedia.org/wiki/Exclusive_or"&gt;Exclusive-Or&lt;/a&gt; (XOR) problem. Whereas you can easily separate the LS classes with a line, this task is not possible for the NLS problem. &lt;/p&gt;
&lt;h2 id="why-cant-a-single-layer-network-solve-nls-classifications"&gt;Why &lt;em&gt;can't&lt;/em&gt; a single layer network solve NLS classifications?&lt;/h2&gt;
&lt;p&gt;Drawing a line between the classes is like solving the classification with regression: the aim is to find the boundary the best separates the members of the two classes. And as it turns out, the traditional single-layer classifier is identical to a regression model:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="150" src="https://nolanbconaway.github.io/blog/2017/traditional-perceptron.png"/&gt;&lt;/p&gt;
&lt;p&gt;The network has two input units, \(D_1\) and \(D_2\), a bias unit, and a single output unit coding the response (say, 0 codes for A, and 1 codes for B). The inputs are connected to the outputs with weights, \(W_1\) and \(W_2\). So the model's output is given by:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
output &amp;amp; = D_1W_1 + D_2W_2 + bias
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Of course, that's also the canonical formula for regression. \(W_1\) and \(W_2\) are the slopes, and the bias is the intercept. The learning objective is to find the values of \(W_1\), \(W_2\), and bias that produce values of 0 for items in category A, and 1 for items in category B.&lt;/p&gt;
&lt;p&gt;The above architecture is just a specific case of the more general multi-class network:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="150" src="https://nolanbconaway.github.io/blog/2017/traditional-multiclass-perceptron.png"/&gt;&lt;/p&gt;
&lt;p&gt;Where each class (again, A and B) gets its own output node. I've color-coded the weights so you can see how this is just two versions of the earlier network, put next to each other. So now output for each unit \(c\) is the sum of the product of each dimension \(D_k\) multiplied against the dimension's weight to the unit, \(W_{ck}\):&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
output_c &amp;amp; = bias + \Sigma_k{D_k W_{ck}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;In this more general case, it's useful to store the weights in an array to take advantage of matrix algebra to compute the output:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

                    &lt;span class="c1"&gt;# bias  D_1  D_2  # Correct Category&lt;/span&gt;
&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# A&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# A&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# B&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;  &lt;span class="c1"&gt;# B&lt;/span&gt;
    &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# A perfect solution to the LS problem!&lt;/span&gt;
&lt;span class="c1"&gt;#                  A    B&lt;/span&gt;
&lt;span class="n"&gt;wts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c1"&gt;# bias&lt;/span&gt;
                &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c1"&gt;# W_1&lt;/span&gt;
                &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="c1"&gt;# W_2&lt;/span&gt;
            &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# output = &lt;/span&gt;
&lt;span class="c1"&gt;#      A   B&lt;/span&gt;
&lt;span class="c1"&gt;#   [[ 1.  0.]&lt;/span&gt;
&lt;span class="c1"&gt;#    [ 1.  0.]&lt;/span&gt;
&lt;span class="c1"&gt;#    [ 0.  1.]&lt;/span&gt;
&lt;span class="c1"&gt;#    [ 0.  1.]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: for simplicity, bias is handled as an extra input unit that always has value 1. The real value of bias is specified in its weights. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But, again, this is identical to the simpler case, we're just doing it twice (once for &lt;code&gt;A&lt;/code&gt; and once for &lt;code&gt;B&lt;/code&gt;), and using matrix algebra for the computation in the equation above. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anyway&lt;/em&gt;, the goal, of course, is to find values for \(W\) that correctly predict the category. But since this is akin to separating the categories with a line (which is what regression does), you'll never find useful values for an NLS problem: by definition, you cannot separate the classes with a line.&lt;/p&gt;
&lt;h2 id="how-to-solve-the-nls-classification"&gt;How to solve the NLS classification&lt;/h2&gt;
&lt;p&gt;You definitely can't solve the NLS problem with any of the architectures above. So, the solution we came up with involves a re-envisioning of the problem, which is realized in a change to the architecture. Instead of the traditional perceptron architecture with dimensions-as-inputs and categories-as-outputs, we designed an autoassociator-based classifier:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="200" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc.png"/&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: That architecture is actually a single-layer version of Ken's &lt;a href="https://link.springer.com/article/10.3758/BF03196806"&gt;Divergent Autoencoder&lt;/a&gt; model, which is also very cool!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've color-coded the weights so that it doesn't look like a tangled mess. Instead of having one output unit per category, the network has one output unit per category, per dimension. The output units are split into two, category-specific, channels, which is why we call it a "&lt;em&gt;Divergent Autoassociator&lt;/em&gt;".&lt;/p&gt;
&lt;p&gt;You could also imagine that network as two separate, regular autoassociators, which is formally the same (it just doesn't &lt;em&gt;look&lt;/em&gt; divergent):&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="200" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc-split.png"/&gt;&lt;/p&gt;
&lt;p&gt;The network's goal is to "reconstruct" everything it sees in the correct category. So if \(D_1=0\) and \(D_2=1\) (i.e., &lt;code&gt;input = [0,1]&lt;/code&gt;), and the network is told that belongs to category A, then the goal is for \(A_1=0\) and \(A_2=1\). The units on the opposite-category channel (\(B_1\) and \(B_2\)) are left alone.&lt;/p&gt;
&lt;p&gt;Based on that learning objective, the network needs to learn weights that accurately reconstruct each category's items on the correct channel. This produces a network which learns how dimensions are correlated within each category, rather than a network which learns how dimensions predict categories.&lt;/p&gt;
&lt;h3 id="classification-with-the-divergent-autoassociator"&gt;Classification with the Divergent Autoassociator&lt;/h3&gt;
&lt;p&gt;Getting the model to classify things is about comparing the amount of &lt;em&gt;error&lt;/em&gt; on each category channel. If, for example, the category A channel does a good job reconstructing &lt;code&gt;[0, 1]&lt;/code&gt;, and category B does a poor job, then the model will classify the item into category A.&lt;/p&gt;
&lt;p&gt;Just to provide a sense of how the classification rule works, I calculated the optimal weights for the LS problem, and I copied over the network's output using those weights. Then, based on the output from each category, I computed the error and classification response.&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# outputs copied from an optimal LS solution&lt;/span&gt;
&lt;span class="n"&gt;output_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;output_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;error_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;output_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;error_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;output_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;classification&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;error_B&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;error_A&lt;/span&gt;
&lt;span class="c1"&gt;# classification = &lt;/span&gt;
&lt;span class="c1"&gt;#   [0 0 1 1]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="the-single-layer-nls-solution"&gt;The Single-Layer NLS Solution&lt;/h3&gt;
&lt;p&gt;So "solving" a classification in the Divergent Autoassociator involves finding a set of weights that allow each class to reconstruct its own members, but poorly reconstruct members of the other class. Here's how it's done:&lt;/p&gt;
&lt;p&gt;&lt;img class="img-fluid" height="275" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc-split-solution.png"/&gt;&lt;/p&gt;
&lt;p&gt;To prevent things from looking like a tangled mess, I've used the two-separate-autoassociators figure. But remember, this is just a single network! I've put the optimal weight values next to each weight.&lt;/p&gt;
&lt;p&gt;Those weights result in the following outputs:&lt;/p&gt;
&lt;table class="table-sm table table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Class&lt;/th&gt;
&lt;th&gt;\(D_1\)&lt;/th&gt;
&lt;th&gt;\(D_2\)&lt;/th&gt;
&lt;th&gt;\(A_1\)&lt;/th&gt;
&lt;th&gt;\(A_2\)&lt;/th&gt;
&lt;th&gt;\(B_1\)&lt;/th&gt;
&lt;th&gt;\(B_2\)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;td&gt;0.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can see that the network has "solved" the NLS problem: items in category A are perfectly reconstructed by the A channel, but not in the B channel, and vice-versa. Here's the math for the final line: \(D_1\)  = 1, \(D_2\) = 0&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
A_1 = D_1 \cdot 0.5 + D_2 \cdot 0.5 + bias \\
    = 1.0 \cdot 0.5 + 0.0 \cdot 0.5 + 0\\
    = 0.5 + 0.0 + 0\\
    = 0.5 \\
    \\
A_2 = D_1 \cdot 0.5 + D_2 \cdot 0.5 + bias\\
    = 1.0 \cdot 0.5 + 0.0 \cdot 0.5 + 0\\
    = 0.5 + 0.0 + 0\\
    = 0.5 \\
\end{align}
$$&lt;/p&gt;
&lt;p&gt;So the weights going to each category A output are identical. This shows that the network learned a key within-category regularity: \(D_1\) = \(D_2\). That is, you can always predict the value of \(D_1\) based on \(D_2\), because within category A the dimensions are positively correlated.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
B_1 = D_1 \cdot 0.67 + D_2 \cdot -0.33 + bias\\
    = 1.0 \cdot 0.67 + 0.0 \cdot -0.33 + 0.33\\
    = 0.67 + 0.0 + 0.33\\
    = 1.0 \\
\\
B_2 = D_1 \cdot -0.33 + D_2 \cdot 0.67 + bias\\
    = 1.0 \cdot -0.33 + 0.0 \cdot 0.67 + 0.33\\
    = -0.33 + 0.0 + 0.33\\
    = 0.0\\
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The key difference in the category B channel is that the &lt;em&gt;cross-dimension&lt;/em&gt; weights (e.g., going from \(D_1\) to \(B_2\)) are negative, showing that the network has learned the opposite pattern in category B: \(D_1\) will be the opposite of \(D_2\).&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;The Divergent Autoassociator solves the NLS classification without actually learning how to classify anything! Really, the model sidesteps the problem: first, it learns how to predict dimensions within each category, then it classifies items based on their consistency with what has been learned.&lt;/p&gt;
&lt;p&gt;It doesn't do anything &lt;em&gt;that&lt;/em&gt; differently from the traditional architecture: we didn't add a new computational step, we didn't add preprocessing, we didn't add a hidden layer. But by looking at the computational problem from a different angle, a new solution arises.&lt;/p&gt;
&lt;h1 id="explore-it-yourself"&gt;Explore it yourself!&lt;/h1&gt;
&lt;p&gt;Along with this post, I've written a couple simple python classes to explore learning in the traditional perceptron and divergent autoassociator models. They're probably not what you want to use for any rigorous study (they don't even &lt;em&gt;classify&lt;/em&gt; things), but they're enough to explore what kinds of weight solutions are learned.&lt;/p&gt;
&lt;p&gt;The classes can be found &lt;a href="https://nolanbconaway.github.io/blog/2017/single_layer_nets.zip"&gt;here&lt;/a&gt;. You can download them and import as a module like so:&lt;/p&gt;
&lt;div class="highlight codehilitetable"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_printoptions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;precision&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# import network objects&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;single_layer_nets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;perceptron&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;single_layer_nets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;autoassociator&lt;/span&gt;

&lt;span class="c1"&gt;# function to make 3d array printing prettier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;single_layer_nets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;printattr&lt;/span&gt; 

&lt;span class="c1"&gt;# define classes&lt;/span&gt;
&lt;span class="n"&gt;NLS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;description&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'NLS'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;LS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;  &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
            &lt;span class="n"&gt;description&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'LS'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NLS&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;' ----------- '&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'description'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Inputs:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;perceptron&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;autoassociator&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;

        &lt;span class="c1"&gt;# initialize network model&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'A'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;categories&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'B'&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
        &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;' output:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;printattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;''&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;' weights:'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;printattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="blog"></category></entry><entry><title>I'll be at MathPsych and CogSci 2016</title><link href="https://nolanbconaway.github.io/blog/2016/summer-conferences.html" rel="alternate"></link><published>2016-07-15T00:00:00-04:00</published><updated>2016-07-15T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2016-07-15:/blog/2016/summer-conferences.html</id><summary type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here's what I'll be up to! Also check out my &lt;a href="https://nolan.shinyapps.io/whos-at-cogsci/"&gt;Shiny App&lt;/a&gt; to see who else is presenting at CogSci.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="mathpsych-new-brunswick-nj"&gt;MathPsych (New Brunswick, NJ)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I'm giving a talk on some joint work with &lt;a href="https://alab.psych.wisc.edu"&gt;Joe Austerweil&lt;/a&gt; (with whom I'll be working as a postdoc!) and &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="cogsci-philadelphia-pa"&gt;CogSci (Philadelphia, PA)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/manuscripts/conaway-kurtz-cogsci2016.pdf"&gt;PDF ‚Ä¶&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here's what I'll be up to! Also check out my &lt;a href="https://nolan.shinyapps.io/whos-at-cogsci/"&gt;Shiny App&lt;/a&gt; to see who else is presenting at CogSci.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="mathpsych-new-brunswick-nj"&gt;MathPsych (New Brunswick, NJ)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I'm giving a talk on some joint work with &lt;a href="https://alab.psych.wisc.edu"&gt;Joe Austerweil&lt;/a&gt; (with whom I'll be working as a postdoc!) and &lt;a href="https://www.binghamton.edu/psychology/people/kkurtz.html"&gt;Ken Kurtz&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="cogsci-philadelphia-pa"&gt;CogSci (Philadelphia, PA)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/manuscripts/conaway-kurtz-cogsci2016.pdf"&gt;PDF&lt;/a&gt;&lt;/strong&gt;] I'm giving a talk on a paper I coauthored with Ken Kurtz.&lt;/li&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/manuscripts/honke-conaway-kurtz-cogsci2016.pdf"&gt;PDF&lt;/a&gt;&lt;/strong&gt;] &lt;a href="http://bingweb.binghamton.edu/~ghonke1/"&gt;Garrett Honke&lt;/a&gt; is giving a talk on a paper we coauthored with Ken Kurtz.&lt;/li&gt;
&lt;li&gt;[&lt;strong&gt;&lt;a href="/pdfs/posters/cogsci2016.pdf"&gt;PDF&lt;/a&gt;&lt;/strong&gt;] &lt;a href="https://www.marist.edu/sbs/facviewer.html?uid=487"&gt;Kimery Levering&lt;/a&gt; and I are presenting a poster on some research we conducted with Ken Kurtz.&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Who's At Cogsci? 2016.</title><link href="https://nolanbconaway.github.io/blog/2016/whos-at-cogsci-2016.html" rel="alternate"></link><published>2016-07-01T00:00:00-04:00</published><updated>2016-07-01T00:00:00-04:00</updated><author><name>Nolan Conaway</name></author><id>tag:nolanbconaway.github.io,2016-07-01:/blog/2016/whos-at-cogsci-2016.html</id><content type="html"></content><category term="app"></category></entry></feed>