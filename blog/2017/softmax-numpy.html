<!DOCTYPE html>

<html lang="en">

<head>
  <title>Nolan Conaway's Blog</title>
  <meta charset="utf-8" />

  <!-- describe the site -->
  <meta name="description" content="Nolan Conaway's Blog">
<meta name="keywords" content="numpy, softmax, python">
  <meta name="author" content="Nolan Conaway">

  <!-- viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- favicon -->
  <link rel="icon" type="image/png" sizes="16x16" href="/theme/images/favicon.ico">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-80719882-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-80719882-1');
  </script>

  <!-- mathjax -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

  <!-- bootstrap -->
  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
    integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
    integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
    integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
    crossorigin="anonymous"></script>

  <!-- custom -->
  <link rel="stylesheet" href="/theme/css/pygment.css">
  <link rel="stylesheet" href="/theme/css/custom.css">
</head>

<body id="index" class="home">
  <div class="container">
    <nav class="navbar navbar-expand-lg sticky-top navbar-dark bg-primary">
      <a class="navbar-brand" href="/">Nolan Conaway</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
          <li class="nav-item"><a class="nav-link" href="/about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="/pdfs">PDFs</a></li>
          <li class="nav-item"><a class="nav-link" href="/apps">Apps</a></li>
        </ul>
        <a class="navbar-brand" href="https://github.com/nolanbconaway">
          <img src="/theme/images/GitHub-Mark-Light-64px.png" width="30" height="30" alt="Github">
        </a>
      </div>
    </nav>

    <hr>

    <div class="content">
<h1>A softmax function for numpy.</h1>
<div class="text-right">March 2017</div>
<hr>
<div class='markdown_insert'>
    <blockquote class="blockquote">
<h4 id="update-jan-2019-scipy-120-now-includes-the-softmax-as-a-special-function-its-really-slick-use-it-here-are-some-notes">Update (Jan 2019): SciPy (1.2.0) now includes the softmax as a special function. It's really slick. Use it. <a href="/blog/2019/softmax-numpy-update">Here are some notes</a>.</h4>
</blockquote>
<hr/>
<p>I use the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function <em>constantly</em>. It's handy anytime I need to model choice among a set of mutually exclusive options. In the canonical example, you have some metric of evidence, \(X = \{ X_1, X_2, ... X_n\} \), that an item belongs to each of \(N\) classes: \( C = \{C_1, C_2, ... C_n\} \). \(X\) can only belong to one class, and larger values indicate more evidence for class membership. So you need to convert the relative amounts of evidence into probabilities of membership within each of the classes.</p>
<p>That's what the softmax function is for. Below I have written the mathematics, but idea is simple: you divide each element of \(X\) by the sum of all the elements:</p>
<p>$$
p(C_n) =
\frac{ \exp{\theta \cdot X_n} }
{ \sum_{i=1}^{N}{\exp {\theta \cdot X_i} } }
$$</p>
<p>The use of exponentials serves to normalize \(X\), and it also allows the function to be parameterized. In the above equation, I threw in a free parameter, \(\theta\) (\(\theta \geq 0\)), that broadly controls <em>determinism</em>. Within the exponentiation, \(\theta\) makes larger values of  \(X\) larger-er, so if you set \(\theta\) to a large value, the softmax really squashes things: Elements of \(X\) with anything but the largest values will have a probability very close to zero. When \(\theta = 1\), it's as if the parameter was never there.</p>
<p>I use this sort of function all the time to simulate how people make decisions based on evidence. But unfortunately, there is no built in <code>numpy</code> function to compute the softmax. For years I have been writing code like this:</p>
<div class="codehilitetable highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">7.3</span><span class="p">])</span>  <span class="c1"># evidence for each choice</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">2.0</span>                         <span class="c1"># determinism parameter</span>

<span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
<span class="n">ps</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span>
</code></pre></div>
<p>Of course, usually <code>X</code> and <code>theta</code> come from somewhere else. This works well if you are only simulating one decision: the softmax requires literally two lines of code and its easily readable. But things get thornier if you want to simulate many choices. For example, what if <code>X</code> is a matrix where rows correspond to the different choices, and the columns correspond to the options?</p>
<p>In the 2D case, you can either run a loop through the rows of <code>X</code> or use <code>numpy</code> matrix <a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">broadcasting</a>. Here's the loop solution:</p>
<div class="codehilitetable highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">7.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">8.8</span><span class="p">,</span> <span class="mf">5.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.7</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="mf">9.6</span><span class="p">,</span> <span class="mf">7.4</span><span class="p">],</span>
    <span class="p">])</span>  

<span class="c1"># looping through rows of X</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">ps</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span>
    <span class="n">ps</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>
</code></pre></div>
<p>That's not <em>terrible</em>, but you can imagine that it's annoying to write one of those <em>every time</em> you need to softmax. Likewise, you'd have to change up the code if you wanted to softmax over columns rather than rows. Or for that matter, what if <code>X</code> was a 3D-array, and you wanted to compute softmax over the third dimension?</p>
<p>At this point it feels more useful to write a generalized softmax function.</p>
<h2 id="my-softmax-function">My softmax function</h2>
<p>After years of copying one-off softmax code between scripts, I decided to make things a little <a href="https://en.wikipedia.org/wiki/Don't_repeat_yourself">dry</a>-er: I sat down and wrote a darn softmax function. The goal was to support \(X\) of any dimensionality, and to allow the user to softmax over an arbitrary axis. Here's the function:</p>
<div class="codehilitetable highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute the softmax of each element along an axis of X.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X: ND-Array. Probably should be floats.</span>
<span class="sd">    theta (optional): float parameter, used as a multiplier</span>
<span class="sd">        prior to exponentiation. Default = 1.0</span>
<span class="sd">    axis (optional): axis to compute values along. Default is the</span>
<span class="sd">        first non-singleton axis.</span>

<span class="sd">    Returns an array the same size as X. The result will sum to 1</span>
<span class="sd">    along the specified axis.</span>
<span class="sd">    """</span>

    <span class="c1"># make X at least 2d</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># find axis</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">j</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">if</span> <span class="n">j</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># multiply y against the theta parameter,</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="c1"># subtract the max for numerical stability</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>

    <span class="c1"># exponentiate y</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># take the sum along the specified axis</span>
    <span class="n">ax_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>

    <span class="c1"># finally: divide elementwise</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">y</span> <span class="o">/</span> <span class="n">ax_sum</span>

    <span class="c1"># flatten if X was 1D</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">p</span>
</code></pre></div>
<h2 id="whats-up-with-that-max-subtraction">What's up with that max subtraction?</h2>
<p>The only aspect of this function that does not directly correspond to something in the softmax equation is the subtraction of the maximum from each of the elements of <code>X</code>. This is done for stability reasons: when you exponentiate even large-ish numbers, the result can be quite large. <code>numpy</code> will return <code>inf</code> when you exponentiate values over 710 or so. So if values of <code>X</code> aren't limited to some fixed range (e.g., \([0...1]\)), or even if you let <code>theta</code> take on any value, you run the distinct possibility of hitting the <code>inf</code> ceiling.</p>
<p>But! If you subtract the maximum value from each element, the largest pre-exponential value will be zero, thus avoiding numerical instability.</p>
<p>So that solves the numerical stability problem, but is it <em>mathematically</em> correct? To clear this up, let's write out the softmax equation with the subtraction terms in there. To keep it simple, I've also removed the \(\theta\) parameter:</p>
<p>$$
p(C_n) =
\frac{ \exp{ X_n - \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i - \max(X) } } }
$$</p>
<p>Subtracting within an exponent is the same as dividing between exponents (<a href="http://www.rapidtables.com/math/number/exponent.htm">remember</a>? \(e^{a-b} = e^a / e^b\)), so:</p>
<p>$$
\frac{ \exp{ X_n - \max(X) }  }
{ \sum_{i=1}^{N}{\exp { X_i - \max(X) } } }
= \frac{ \exp { X_n  } \div \exp { \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i } \div \exp { \max(X) } } }
$$</p>
<p>Then you just cancel out the maximum terms, and you're left with the original equation:</p>
<p>$$
\frac{ \exp { X_n  } \div \exp { \max(X) } }
{ \sum_{i=1}^{N}{\exp { X_i } \div \exp { \max(X) } } } =
\frac{ \exp{ X_n } }
{ \sum_{i=1}^{N}{\exp { X_i } } }
$$</p>
<h2 id="summary">Summary</h2>
<p>The function works beautifully and has a nice safeguard against overflow in the exponential. And, if you're like me, including it will prevent you from writing a handful of one-off implementations of the softmax. I'll round this out with a few examples of its usage:</p>
<h3 id="2d-usage">2D Usage</h3>
<div class="codehilitetable highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">7.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">8.8</span><span class="p">,</span> <span class="mf">5.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.7</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="mf">9.6</span><span class="p">,</span> <span class="mf">7.4</span><span class="p">],</span>
    <span class="p">])</span>

<span class="c1"># softmax over rows</span>
<span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([[</span> <span class="mf">0.055</span><span class="p">,</span>  <span class="mf">0.407</span><span class="p">,</span>  <span class="mf">0.015</span><span class="p">,</span>  <span class="mf">0.413</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">0.822</span><span class="p">,</span>  <span class="mf">0.165</span><span class="p">,</span>  <span class="mf">0.395</span><span class="p">,</span>  <span class="mf">0.152</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">0.123</span><span class="p">,</span>  <span class="mf">0.428</span><span class="p">,</span>  <span class="mf">0.59</span> <span class="p">,</span>  <span class="mf">0.435</span><span class="p">]])</span>

<span class="c1"># softmax over columns</span>
<span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([[</span> <span class="mf">0.031</span><span class="p">,</span>  <span class="mf">0.22</span> <span class="p">,</span>  <span class="mf">0.054</span><span class="p">,</span>  <span class="mf">0.695</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">0.204</span><span class="p">,</span>  <span class="mf">0.039</span><span class="p">,</span>  <span class="mf">0.645</span><span class="p">,</span>  <span class="mf">0.112</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">0.022</span><span class="p">,</span>  <span class="mf">0.072</span><span class="p">,</span>  <span class="mf">0.68</span> <span class="p">,</span>  <span class="mf">0.226</span><span class="p">]])</span>

<span class="c1"># softmax over columns, and squash it!</span>
<span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">500.0</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">],</span>
           <span class="p">[</span> <span class="mf">0.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">0.</span><span class="p">]])</span>
</code></pre></div>
<h3 id="3d-and-beyond">3D (and beyond!)</h3>
<div class="codehilitetable highlight"><pre><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">X</span>
<span class="n">array</span><span class="p">([[[</span> <span class="mf">0.844</span><span class="p">,</span>  <span class="mf">0.237</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.364</span><span class="p">,</span>  <span class="mf">0.768</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.811</span><span class="p">,</span>  <span class="mf">0.959</span><span class="p">]],</span>

       <span class="p">[[</span> <span class="mf">0.511</span><span class="p">,</span>  <span class="mf">0.06</span> <span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.594</span><span class="p">,</span>  <span class="mf">0.029</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.963</span><span class="p">,</span>  <span class="mf">0.292</span><span class="p">]],</span>

       <span class="p">[[</span> <span class="mf">0.463</span><span class="p">,</span>  <span class="mf">0.869</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.704</span><span class="p">,</span>  <span class="mf">0.786</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.173</span><span class="p">,</span>  <span class="mf">0.89</span> <span class="p">]]])</span>


<span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([[[</span> <span class="mf">0.575</span><span class="p">,</span>  <span class="mf">0.425</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">0.45</span> <span class="p">,</span>  <span class="mf">0.55</span> <span class="p">],</span>
            <span class="p">[</span> <span class="mf">0.482</span><span class="p">,</span>  <span class="mf">0.518</span><span class="p">]],</span>

           <span class="p">[[</span> <span class="mf">0.556</span><span class="p">,</span>  <span class="mf">0.444</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">0.57</span> <span class="p">,</span>  <span class="mf">0.43</span> <span class="p">],</span>
            <span class="p">[</span> <span class="mf">0.583</span><span class="p">,</span>  <span class="mf">0.417</span><span class="p">]],</span>

           <span class="p">[[</span> <span class="mf">0.449</span><span class="p">,</span>  <span class="mf">0.551</span><span class="p">],</span>
            <span class="p">[</span> <span class="mf">0.49</span> <span class="p">,</span>  <span class="mf">0.51</span> <span class="p">],</span>
            <span class="p">[</span> <span class="mf">0.411</span><span class="p">,</span>  <span class="mf">0.589</span><span class="p">]]])</span>
</code></pre></div>
</div>
    </div>

  </div>
</body>

<script>
  // apply bootstrap table classes/scopes
  $('table.table>thead>tr>th').attr('scope', 'col')

  // wrap all tables for overflow scrolling
  $("table.table").not('.highlighttable').not('.blogroll').wrap("<div class='table_wrap'></div>");

  // add anchor URLS for header elements in markdown pages
  $(".markdown_insert h2,h3,h4").each(function (index) {
    $(this).append("<a class='anchor' href='#" + this.id + "'><sup>#</sup></a>")
  });
</script>


<script>
  // More elegantly link to internal IDs. In practice these links put you BELOW the header
  // element, but I want to include that element in the first view.

  // The function actually applying the offset
  function offsetAnchor() {
    if (location.hash.length !== 0) {
      window.scrollTo(window.scrollX, window.scrollY - 70);
    }
  }

  // Captures click events of all <a> elements with href starting with #
  $(document).on('click', 'a[href^="#"]', function (event) {
    // Click events are captured before hashchanges. Timeout
    // causes offsetAnchor to be called after the page jump.
    window.setTimeout(function () {
      offsetAnchor();
    }, 0);
  });

  // Set the offset when entering page with hash present in the url
  window.setTimeout(offsetAnchor, 0);
</script>

</html>