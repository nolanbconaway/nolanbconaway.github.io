<!DOCTYPE html>

<html lang="en">

<head>
  <title>Nolan Conaway's Blog</title>
  <meta charset="utf-8" />

  <!-- describe the site -->
  <meta name="description" content="Nolan Conaway's Blog">
<meta name="keywords" content="pitchfork, music criticism, analysis">
  <meta name="author" content="Nolan Conaway">

  <!-- viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- favicon -->
  <link rel="icon" type="image/png" sizes="16x16" href="/theme/images/favicon.ico">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-80719882-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-80719882-1');
  </script>

  <!-- mathjax -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

  <!-- bootstrap -->
  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
    integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
    integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
    integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
    crossorigin="anonymous"></script>

  <!-- custom -->
  <link rel="stylesheet" href="/theme/css/pygment.css">
  <link rel="stylesheet" href="/theme/css/custom.css">
</head>

<body id="index" class="home">
  <div class="container">
    <nav class="navbar navbar-expand-lg sticky-top navbar-dark bg-primary">
      <a class="navbar-brand" href="/">Nolan Conaway</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
          <li class="nav-item"><a class="nav-link" href="/about">About</a></li>
          <!-- <li class="nav-item"><a class="nav-link" href="/pdfs">PDFs</a></li> -->
          <li class="nav-item"><a class="nav-link" href="/apps">Apps</a></li>
        </ul>
        <a class="navbar-brand" href="https://github.com/nolanbconaway">
          <img src="/theme/images/GitHub-Mark-Light-64px.png" width="30" height="30" alt="Github">
        </a>
      </div>
    </nav>

    <hr>

    <div class="content">
<h1>What I found in 18,000 Pitchfork album reviews.</h1>
<div class="text-right">June 2017</div>
<hr>
<div class='markdown_insert'>
    <blockquote class="blockquote">
<p><em>Over the Winter of 2016-2017, I scraped over 18,000 reviews published on <a href="http://pitchfork.com/">Pitchfork</a>. I published the dataset on Github, and wrote <a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/author-autocorrelation.ipynb">several</a> <a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb">Jupyter</a> <a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/artist-development.ipynb">Notebooks</a> <a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/review-score-exploration.ipynb">exploring</a> the data. This post provides a discussion about what I found. For a more code-driven approach, check out the notebooks on <a href="https://github.com/nolanbconaway/pitchfork-data">Github</a>!</em></p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>As a person who reads a fair amount of music criticism, and as a scientist, I've long been curious about the degree of objectivity in reviews of new releases. There is of course no such thing as an <em>objective</em> assessment for the quality of a piece music, and reviews are based on the personal taste of each reviewer. But also, readers clearly take something away from music reviews (otherwise why would they be read?), and major publications are thought to have wide influence on the behavior of listeners (though, as far as I can tell, there's no data to back up that claim).</p>
<p>So, while subjective, reviews are at least <em>interpreted</em> as authoritative. Setting aside routine subjectivity having to do with taste, the scientist in me wonders about sources of bias observable across many authors. To address my curiosity, I collected over 18,000 reviews published on <a href="http://pitchfork.com/">Pitchfork</a> between January 1999 and January 2017. I chose Pitchfork because it is widely viewed as authoritative, publishes a large amount of content (25+ reviews per week), and offers a precise scoring (0.0-10.0) of each album. In this post, I'll describe what I found.</p>
<h2 id="the-reviews">The Reviews</h2>
<p>Since 1998, Pitchfork has published five new reviews every weekday. In 2016, Pitchfork added an additional five reviews on Saturdays, as well as a "Sunday Reviews" section containing long-form articles on a classic albums (one review per week, published on Sundays). They've also occasionally published more targeted series of reviews (such as following the deaths of <a href="http://pitchfork.com/artists/438-david-bowie/">David Bowie</a> and <a href="http://pitchfork.com/artists/3397-prince/">Prince</a>).</p>
<p>The typical review addresses a single release by a specific artist, and is attributed to a single author. All reviews are given a numerical score (0.0-10.0), are labeled by genre, report the record label (if any), and contain several paragraphs of text (the review itself). Since 2003, authors have awarded some releases the title "<em>Best New Music</em>", which means what you'd think. Best New Music albums are displayed prominently on Pitchfork, and even have their own <a href="http://pitchfork.com/reviews/best/albums/">page</a>.</p>
<p>Album scores and Best New Music awards will be the focus of my analysis, so it's worth taking a look at those distributions.</p>
<p><object data="https://nolanbconaway.github.io/blog/2017/score-bmn-hist.svg" type="image/svg+xml"></object></p>
<blockquote class="blockquote">
<p>Note: Most high scoring albums that are <em>not</em> Best New Music are from reviews prior to the advent of Best New Music, or are reviews of classic releases.</p>
</blockquote>
<p>A little more than 5% of reviews are awarded Best New Music. Most scores lie between 6.4 and 7.8, with the average review getting a score of 7.0. Best New Music is typically awarded to albums scoring greater than 8.4. Scores between 8.1 and 8.4 have a decent shot at Best New Music, but many albums in that range are not given the title. </p>
<h2 id="statistical-heaping">Statistical Heaping</h2>
<p>Inspired by <a href="https://gutterstats.wordpress.com/2015/11/03/are-nfl-officials-biased-with-their-ball-placement/">this really cool blog post</a>, I thought about what a reviewer might do if they have a <em>general</em> sense of an album's score, but they need to pick a <em>specific</em> value. Is this a 7.8? 7.9? 7.7? The above mentioned post found that NFL officials tend to place the line of scrimmage at tidy yard numbers (i.e., 10s and 5s). The technical term for this is <a href="http://ww2.amstat.org/sections/SRMS/Proceedings/y1958/Patterns%20Of%20Heaping%20In%20The%20Reporting%20Of%20Numerical%20Data.pdf">statistical heaping</a>, and its observed commonly in survey data. So maybe Pitchfork reviewers behave similarly? I counted the number of scores at each decimal place (e.g., \( <em>.0, </em>.1... *.9 \)), here's the result:</p>
<p><object data="https://nolanbconaway.github.io/blog/2017/score-anchor-points.svg" type="image/svg+xml"></object></p>
<p>Those diamond markers show what you'd expect if Pitchfork reviewers were totally unbiased. The <em>Uniform Sampling</em> model is what you'd get if you picked scores at random; the <em>Normal Sampling</em> model shows what you'd get if you picked scores around a normal distribution  based on the observed scores (\(\mu=7.006, \sigma = 1.294\)). *.0 gets a slight bump in the uniform model because 10.0 is a possibility, but 10.1-10.9 are not. </p>
<p>Obviously, the tidy, round <em>.0 value is much more frequently chosen than you'd expect given either sampling technique: _It's nearly twice as frequent as </em>.1_. There's also a slight bump at the <em>0.5 and </em>.8  marks, but those values are a bit weaker. The point is, Pitchfork reviewers absolutely show the heaping behavior: at least in this sense, the review scores are biased. </p>
<h2 id="borderline-best-new-music-decisions">Borderline Best New Music Decisions</h2>
<p>Still, its important to consider that the impact of the heaping is not <em>huge</em>. We're taking about a reviewer picking between, for example, a score of 7.0 or 7.1. A more impactful difference lies in the choice to award <em>Best New Music</em>. As I noted above, while most releases scoring 8.5 or better are given the award, releases scoring between 8.1 and 8.4 have a shot but it is not guaranteed. I've long wondered about how these decisions are made, so I looked into it.</p>
<p>I reduced the dataset to the borderline Best New Music candidates: there are 1223 in all, 269 (22%) of which are Best New Music. I checked out a bunch of possible explanations for why some get Best New Music and some do not; for full disclosure I'll list them here:</p>
<ul>
<li>Are "tougher" authors (who give out lower scores) more or less likely to award Best New Music? <em>No</em>.</li>
<li>Is an artist's first album more likely be awarded Best New Music? <em>No</em>.</li>
<li>Are authors less likely to grant the award if they have less expertise in the genre? <em>No</em>.</li>
<li>Are authors less likely to grant the award if they recently awarded it to another album? <em>No</em>.</li>
</ul>
<p>To Pitchfork's credit: all of these would be reasonable biases to observe and I found little evidence of each. I <em>did</em> find that <a href="https://twitter.com/nolanbconaway/status/875568013050658818">some genres are more likely to be considered Best New Music</a>, but I'm not sure if that constitutes bias or just Pitchfork's focus.</p>
<p>But when I looked into whether some record labels where favored over others, the results were striking. I computed the proportion of borderline cases from each label that were Best new Music; most only had one or two borderlines cases, but here are the labels with at least ten:</p>
<p><object data="https://nolanbconaway.github.io/blog/2017/borderline-by-label.svg" type="image/svg+xml"></object></p>
<p>The first thing to note is that the labels are all major labels or at least big-name indie labels (with the exception of "self-released"). The second thing to note is the huge degree of differences between labels: <a href="https://en.wikipedia.org/wiki/4AD">4AD</a> got Best New Music on 8/14 borderline cases, and <a href="https://en.wikipedia.org/wiki/Thrill_Jockey">Thrill Jockey</a>, <a href="https://en.wikipedia.org/wiki/Relapse_Records">Relapse</a>, <a href="https://en.wikipedia.org/wiki/Nonesuch_Records">Nonesuch</a>, and <a href="https://en.wikipedia.org/wiki/Anti-_(record_label)">Anti-</a> collectively went 0 out of 44. </p>
<p>That <em>feels</em> unlikely, but are those differences routine? I sent each label's data through an unbiased model, where the probability of \(k\) Best New Musics out of \(n\) borderline cases follows a Binomial distribution, with \( p=0.22 \) (the overall proportion of Best New Music among borderline cases). This model (depicted in the figure) shows how likely each label's data is <em>individually</em>, but it does not really address the question: How probable is the data <em>collectively</em>?  How likely are we to observe four labels with zero Best New Music? How likely are we to observe four labels with more than 40% Best new Music? How likely are we to observe both scenarios simultaneously?</p>
<p>So I did some Monte Carlo sampling. I simulated each label's Best New Music record 1 million times using the above Binomial distributions. Out of the 1,000,000 samples, 4523 (0.4523%) contained four or more labels with zero Best New Music, 22469 (2.2469%) contained four or more labels with more than 30% Best New Music, and only <em>42</em> (<em>0.0042%</em>) had both. Obviously, the data we have is <em>very unlikely</em> to occur, assuming each label were awarded Best New Music with equal probability.</p>
<p>Unfortunately, I do not have a clear idea as to why record labels are treated differently. I certainty wouldn't go as far as to suggest that there is <em>overt</em> favoritism. My best theory is that maybe authors <em>expect</em> Best New Music from some labels and they do not expect it from others. These expectations wouldn't usually influence decisions because most releases are clearly Best new Music or they are not, but they emerge in borderline cases. <a href="mailto:nolanbconaway@gmail.com">Let me know</a> if you can think of a way to test that theory!</p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>The reason why I was interested in revealing these sources of bias is that music reviews carry with them a sense of authority: that the favorability of a review is in some sense objective. But, obviously, authors are people, and <a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases#Decision-making.2C_belief.2C_and_behavioral_biases">people are biased</a>. So it's no surprise that, once you get digging into the data, you can find evidence of all sorts of biases. </p>
<p>To their credit, in this post I did <em>not</em> report many of the analyses I conducted which uncovered no evidence of bias (like <a href="https://twitter.com/nolanbconaway/status/873754026080436224">this one</a>, or <a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/reviewer-development.ipynb">this one</a>, or <a href="http://nbviewer.jupyter.org/github/nolanbconaway/pitchfork-data/blob/master/notebooks/best-new-music-iid.ipynb">this one</a>). Of course, that's not exactly evidence that there is <em>no</em> bias. But, Pitchfork reviewers are professionals, and my guess is that many of them have considered these sorts of biases before, and may even attempt to combat their influence.</p>
<p>As always, feel free to <a href="mailto:nolanbconaway@gmail.com">get in touch</a> if you have comments/questions, or even if you're just curious about some aspect of the data and you don't want to do the analysis yourself.</p>
</div>
    </div>

  </div>
</body>

<script>
  // apply bootstrap table classes/scopes
  $('table.table>thead>tr>th').attr('scope', 'col')

  // wrap all tables for overflow scrolling
  $("table.table").not('.highlighttable').not('.blogroll').wrap("<div class='table_wrap'></div>");

  // add anchor URLS for header elements in markdown pages
  $(".markdown_insert h2,h3,h4").each(function (index) {
    $(this).append("<a class='anchor' href='#" + this.id + "'><sup>#</sup></a>")
  });
</script>


<script>
  // More elegantly link to internal IDs. In practice these links put you BELOW the header
  // element, but I want to include that element in the first view.

  // The function actually applying the offset
  function offsetAnchor() {
    if (location.hash.length !== 0) {
      window.scrollTo(window.scrollX, window.scrollY - 70);
    }
  }

  // Captures click events of all <a> elements with href starting with #
  $(document).on('click', 'a[href^="#"]', function (event) {
    // Click events are captured before hashchanges. Timeout
    // causes offsetAnchor to be called after the page jump.
    window.setTimeout(function () {
      offsetAnchor();
    }, 0);
  });

  // Set the offset when entering page with hash present in the url
  window.setTimeout(offsetAnchor, 0);
</script>

</html>