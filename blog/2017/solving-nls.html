<!DOCTYPE html>

<html lang="en">

<head>
  <title>Nolan Conaway's Blog</title>
  <meta charset="utf-8" />

  <!-- describe the site -->
  <meta name="description" content="Nolan Conaway's Blog">
<meta name="keywords" content="linear separability, neural networks, python">
  <meta name="author" content="Nolan Conaway">

  <!-- viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- favicon -->
  <link rel="icon" type="image/png" sizes="16x16" href="/theme/images/favicon.ico">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-80719882-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-80719882-1');
  </script>

  <!-- mathjax -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

  <!-- bootstrap -->
  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
    integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
    integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
    integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6"
    crossorigin="anonymous"></script>

  <!-- custom -->
  <link rel="stylesheet" href="/theme/css/pygment.css">
  <link rel="stylesheet" href="/theme/css/custom.css">
</head>

<body id="index" class="home">
  <div class="container">
    <nav class="navbar navbar-expand-lg sticky-top navbar-dark bg-primary">
      <a class="navbar-brand" href="/">Nolan Conaway</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
          <li class="nav-item"><a class="nav-link" href="/about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="/pdfs">PDFs</a></li>
          <li class="nav-item"><a class="nav-link" href="/apps">Apps</a></li>
        </ul>
        <a class="navbar-brand" href="https://github.com/nolanbconaway">
          <img src="/theme/images/GitHub-Mark-Light-64px.png" width="30" height="30" alt="Github">
        </a>
      </div>
    </nav>

    <hr>

    <div class="content">
<h1>Solving nonlinearly separable classifications in a single layer neural network.</h1>
<div class="text-right">January 2017</div>
<hr>
<div class='markdown_insert'>
    <blockquote class="blockquote">
<p><em>Recently, <a href="https://www.binghamton.edu/psychology/people/kkurtz.html">Ken Kurtz</a> (my graduate advisor) and I figured out a unique solution to the famous limitation that single-layer neural networks cannot solve nonlinearly separable classifications. We published our findings in <a href="http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00931">Neural Computation</a>. This post is intended to provide a more introductory-level description of our solution. Read the paper for a more formal report!</em></p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>In an <a href="https://en.wikipedia.org/wiki/Perceptrons_(book)">influential book</a> published in 1969,  Marvin Minsky and Seymour Papert proved that the conventional neural networks of the day could not solve nonlinearly separable (NLS) classifications.</p>
<p>Their conclusions spurred a decline in research on neural network models during the following two decades. It wasn't until the popularization of the <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation algorithm</a> in the 1980s, which enabled models to solve NLS classifications through learning an additional layer of weights, that interest picked back up. But even after backprop, and after the advent of methods to train up <a href="https://en.wikipedia.org/wiki/Deep_learning">deep networks</a>, the conventional wisdom has been that the only way to solve a NLS classification is by <em>adding layers of weights</em>.</p>
<p>Recently, <a href="https://www.binghamton.edu/psychology/people/kkurtz.html">Ken Kurtz</a> (my graduate advisor) and I figured out how you can solve NLS classifications with only a single layer of weights. In this post, I'll explain how!</p>
<h2 id="what-is-a-nonlinearly-separable-classification">What is a nonlinearly separable classification?</h2>
<p>Nonlinearly separable classifications are most straightforwardly understood through contrast with linearly separable ones: if a classification is linearly separable, you can draw a line to separate the classes. </p>
<p>Below is an example of each. Imagine you are trying to discriminate between two classes, A and B, on the basis of two input dimensions (\(D_1\) and \(D_2\)):</p>
<p><img class="img-fluid" src="https://nolanbconaway.github.io/blog/2017/ls-nls.png"/></p>
<p>The NLS problem above is the ubiquitous <a href="https://en.wikipedia.org/wiki/Exclusive_or">Exclusive-Or</a> (XOR) problem. Whereas you can easily separate the LS classes with a line, this task is not possible for the NLS problem. </p>
<h2 id="why-cant-a-single-layer-network-solve-nls-classifications">Why <em>can't</em> a single layer network solve NLS classifications?</h2>
<p>Drawing a line between the classes is like solving the classification with regression: the aim is to find the boundary the best separates the members of the two classes. And as it turns out, the traditional single-layer classifier is identical to a regression model:</p>
<p><img class="img-fluid" height="150" src="https://nolanbconaway.github.io/blog/2017/traditional-perceptron.png"/></p>
<p>The network has two input units, \(D_1\) and \(D_2\), a bias unit, and a single output unit coding the response (say, 0 codes for A, and 1 codes for B). The inputs are connected to the outputs with weights, \(W_1\) and \(W_2\). So the model's output is given by:</p>
<p>$$
\begin{align}
output &amp; = D_1W_1 + D_2W_2 + bias
\end{align}
$$</p>
<p>Of course, that's also the canonical formula for regression. \(W_1\) and \(W_2\) are the slopes, and the bias is the intercept. The learning objective is to find the values of \(W_1\), \(W_2\), and bias that produce values of 0 for items in category A, and 1 for items in category B.</p>
<p>The above architecture is just a specific case of the more general multi-class network:</p>
<p><img class="img-fluid" height="150" src="https://nolanbconaway.github.io/blog/2017/traditional-multiclass-perceptron.png"/></p>
<p>Where each class (again, A and B) gets its own output node. I've color-coded the weights so you can see how this is just two versions of the earlier network, put next to each other. So now output for each unit \(c\) is the sum of the product of each dimension \(D_k\) multiplied against the dimension's weight to the unit, \(W_{ck}\):</p>
<p>$$
\begin{align}
output_c &amp; = bias + \Sigma_k{D_k W_{ck}}
\end{align}
$$</p>
<p>In this more general case, it's useful to store the weights in an array to take advantage of matrix algebra to compute the output:</p>
<div class="codehilitetable highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

                    <span class="c1"># bias  D_1  D_2  # Correct Category</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span>  <span class="c1"># A</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span>    <span class="mi">0</span><span class="p">,</span>   <span class="mi">1</span><span class="p">],</span>  <span class="c1"># A</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span>  <span class="c1"># B</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span>    <span class="mi">1</span><span class="p">,</span>   <span class="mi">1</span><span class="p">],</span>  <span class="c1"># B</span>
    <span class="p">])</span>

<span class="c1"># A perfect solution to the LS problem!</span>
<span class="c1">#                  A    B</span>
<span class="n">wts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="c1"># bias</span>
                <span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="c1"># W_1</span>
                <span class="p">[</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="c1"># W_2</span>
            <span class="p">])</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">wts</span><span class="p">)</span>

<span class="c1"># output = </span>
<span class="c1">#      A   B</span>
<span class="c1">#   [[ 1.  0.]</span>
<span class="c1">#    [ 1.  0.]</span>
<span class="c1">#    [ 0.  1.]</span>
<span class="c1">#    [ 0.  1.]]</span>
</code></pre></div>
<blockquote class="blockquote">
<p><em>Note</em>: for simplicity, bias is handled as an extra input unit that always has value 1. The real value of bias is specified in its weights. </p>
</blockquote>
<p>But, again, this is identical to the simpler case, we're just doing it twice (once for <code>A</code> and once for <code>B</code>), and using matrix algebra for the computation in the equation above. </p>
<p><em>Anyway</em>, the goal, of course, is to find values for \(W\) that correctly predict the category. But since this is akin to separating the categories with a line (which is what regression does), you'll never find useful values for an NLS problem: by definition, you cannot separate the classes with a line.</p>
<h2 id="how-to-solve-the-nls-classification">How to solve the NLS classification</h2>
<p>You definitely can't solve the NLS problem with any of the architectures above. So, the solution we came up with involves a re-envisioning of the problem, which is realized in a change to the architecture. Instead of the traditional perceptron architecture with dimensions-as-inputs and categories-as-outputs, we designed an autoassociator-based classifier:</p>
<p><img class="img-fluid" height="200" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc.png"/></p>
<blockquote class="blockquote">
<p><em>Note</em>: That architecture is actually a single-layer version of Ken's <a href="https://link.springer.com/article/10.3758/BF03196806">Divergent Autoencoder</a> model, which is also very cool!</p>
</blockquote>
<p>I've color-coded the weights so that it doesn't look like a tangled mess. Instead of having one output unit per category, the network has one output unit per category, per dimension. The output units are split into two, category-specific, channels, which is why we call it a "<em>Divergent Autoassociator</em>".</p>
<p>You could also imagine that network as two separate, regular autoassociators, which is formally the same (it just doesn't <em>look</em> divergent):</p>
<p><img class="img-fluid" height="200" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc-split.png"/></p>
<p>The network's goal is to "reconstruct" everything it sees in the correct category. So if \(D_1=0\) and \(D_2=1\) (i.e., <code>input = [0,1]</code>), and the network is told that belongs to category A, then the goal is for \(A_1=0\) and \(A_2=1\). The units on the opposite-category channel (\(B_1\) and \(B_2\)) are left alone.</p>
<p>Based on that learning objective, the network needs to learn weights that accurately reconstruct each category's items on the correct channel. This produces a network which learns how dimensions are correlated within each category, rather than a network which learns how dimensions predict categories.</p>
<h3 id="classification-with-the-divergent-autoassociator">Classification with the Divergent Autoassociator</h3>
<p>Getting the model to classify things is about comparing the amount of <em>error</em> on each category channel. If, for example, the category A channel does a good job reconstructing <code>[0, 1]</code>, and category B does a poor job, then the model will classify the item into category A.</p>
<p>Just to provide a sense of how the classification rule works, I calculated the optimal weights for the LS problem, and I copied over the network's output using those weights. Then, based on the output from each category, I computed the error and classification response.</p>
<div class="codehilitetable highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span> 
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span>   <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span>   <span class="mi">1</span><span class="p">]</span> <span class="p">])</span>

<span class="c1"># outputs copied from an optimal LS solution</span>
<span class="n">output_A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span> 
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span>   <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span>   <span class="mi">1</span><span class="p">]</span> <span class="p">])</span>

<span class="n">output_B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span> 
                    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span>   <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span>  <span class="mi">1</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span>  <span class="mi">1</span><span class="p">,</span>   <span class="mi">1</span><span class="p">]</span> <span class="p">])</span>

<span class="n">error_A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">output_A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">error_B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">output_B</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">classification</span> <span class="o">=</span> <span class="n">error_B</span> <span class="o">&lt;</span> <span class="n">error_A</span>
<span class="c1"># classification = </span>
<span class="c1">#   [0 0 1 1]</span>
</code></pre></div>
<h3 id="the-single-layer-nls-solution">The Single-Layer NLS Solution</h3>
<p>So "solving" a classification in the Divergent Autoassociator involves finding a set of weights that allow each class to reconstruct its own members, but poorly reconstruct members of the other class. Here's how it's done:</p>
<p><img class="img-fluid" height="275" src="https://nolanbconaway.github.io/blog/2017/divergent-autoassoc-split-solution.png"/></p>
<p>To prevent things from looking like a tangled mess, I've used the two-separate-autoassociators figure. But remember, this is just a single network! I've put the optimal weight values next to each weight.</p>
<p>Those weights result in the following outputs:</p>
<table class="table-sm table-hover table">
<thead>
<tr>
<th>Class</th>
<th>\(D_1\)</th>
<th>\(D_2\)</th>
<th>\(A_1\)</th>
<th>\(A_2\)</th>
<th>\(B_1\)</th>
<th>\(B_2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.33</td>
<td>0.33</td>
</tr>
<tr>
<td>A</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0.67</td>
<td>0.67</td>
</tr>
<tr>
<td>B</td>
<td>0</td>
<td>1</td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>B</td>
<td>1</td>
<td>0</td>
<td>0.5</td>
<td>0.5</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>You can see that the network has "solved" the NLS problem: items in category A are perfectly reconstructed by the A channel, but not in the B channel, and vice-versa. Here's the math for the final line: \(D_1\)  = 1, \(D_2\) = 0</p>
<p>$$
\begin{align}
A_1 = D_1 \cdot 0.5 + D_2 \cdot 0.5 + bias \\
    = 1.0 \cdot 0.5 + 0.0 \cdot 0.5 + 0\\
    = 0.5 + 0.0 + 0\\
    = 0.5 \\
    \\
A_2 = D_1 \cdot 0.5 + D_2 \cdot 0.5 + bias\\
    = 1.0 \cdot 0.5 + 0.0 \cdot 0.5 + 0\\
    = 0.5 + 0.0 + 0\\
    = 0.5 \\
\end{align}
$$</p>
<p>So the weights going to each category A output are identical. This shows that the network learned a key within-category regularity: \(D_1\) = \(D_2\). That is, you can always predict the value of \(D_1\) based on \(D_2\), because within category A the dimensions are positively correlated.</p>
<p>$$
\begin{align}
B_1 = D_1 \cdot 0.67 + D_2 \cdot -0.33 + bias\\
    = 1.0 \cdot 0.67 + 0.0 \cdot -0.33 + 0.33\\
    = 0.67 + 0.0 + 0.33\\
    = 1.0 \\
\\
B_2 = D_1 \cdot -0.33 + D_2 \cdot 0.67 + bias\\
    = 1.0 \cdot -0.33 + 0.0 \cdot 0.67 + 0.33\\
    = -0.33 + 0.0 + 0.33\\
    = 0.0\\
\end{align}
$$</p>
<p>The key difference in the category B channel is that the <em>cross-dimension</em> weights (e.g., going from \(D_1\) to \(B_2\)) are negative, showing that the network has learned the opposite pattern in category B: \(D_1\) will be the opposite of \(D_2\).</p>
<h2 id="summary">Summary</h2>
<p>The Divergent Autoassociator solves the NLS classification without actually learning how to classify anything! Really, the model sidesteps the problem: first, it learns how to predict dimensions within each category, then it classifies items based on their consistency with what has been learned.</p>
<p>It doesn't do anything <em>that</em> differently from the traditional architecture: we didn't add a new computational step, we didn't add preprocessing, we didn't add a hidden layer. But by looking at the computational problem from a different angle, a new solution arises.</p>
<h1 id="explore-it-yourself">Explore it yourself!</h1>
<p>Along with this post, I've written a couple simple python classes to explore learning in the traditional perceptron and divergent autoassociator models. They're probably not what you want to use for any rigorous study (they don't even <em>classify</em> things), but they're enough to explore what kinds of weight solutions are learned.</p>
<p>The classes can be found <a href="https://nolanbconaway.github.io/blog/2017/single_layer_nets.zip">here</a>. You can download them and import as a module like so:</p>
<div class="codehilitetable highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># import network objects</span>
<span class="kn">from</span> <span class="nn">single_layer_nets</span> <span class="kn">import</span> <span class="n">perceptron</span>
<span class="kn">from</span> <span class="nn">single_layer_nets</span> <span class="kn">import</span> <span class="n">autoassociator</span>

<span class="c1"># function to make 3d array printing prettier</span>
<span class="kn">from</span> <span class="nn">single_layer_nets</span> <span class="kn">import</span> <span class="n">printattr</span> 

<span class="c1"># define classes</span>
<span class="n">NLS</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span> <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]),</span>
            <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]),</span>
            <span class="n">description</span> <span class="o">=</span> <span class="s1">'NLS'</span><span class="p">)</span>

<span class="n">LS</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>  <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]),</span>
            <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]),</span>
            <span class="n">description</span> <span class="o">=</span> <span class="s1">'LS'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">categories</span> <span class="ow">in</span> <span class="p">[</span><span class="n">LS</span><span class="p">,</span> <span class="n">NLS</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">' ----------- '</span> <span class="o">+</span> <span class="n">categories</span><span class="p">[</span><span class="s1">'description'</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Inputs:'</span><span class="p">)</span>
    <span class="nb">print</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">categories</span><span class="p">[</span><span class="s1">'A'</span><span class="p">],</span> <span class="n">categories</span><span class="p">[</span><span class="s1">'B'</span><span class="p">]])</span>

    <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">perceptron</span><span class="p">,</span> <span class="n">autoassociator</span><span class="p">]:</span>

        <span class="c1"># initialize network model</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">categories</span><span class="p">[</span><span class="s1">'A'</span><span class="p">],</span> <span class="n">categories</span><span class="p">[</span><span class="s1">'B'</span><span class="p">]])</span>
        <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">model</span> <span class="o">+</span> <span class="s1">' output:'</span><span class="p">)</span>
        <span class="n">printattr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">output</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">model</span> <span class="o">+</span> <span class="s1">' weights:'</span><span class="p">)</span>
        <span class="n">printattr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">wts</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</code></pre></div>
</div>
    </div>

  </div>
</body>

<script>
  // apply bootstrap table classes/scopes
  $('table.table>thead>tr>th').attr('scope', 'col')

  // wrap all tables for overflow scrolling
  $("table.table").not('.highlighttable').not('.blogroll').wrap("<div class='table_wrap'></div>");

  // add anchor URLS for header elements in markdown pages
  $(".markdown_insert h2,h3,h4").each(function (index) {
    $(this).append("<a class='anchor' href='#" + this.id + "'><sup>#</sup></a>")
  });
</script>


<script>
  // More elegantly link to internal IDs. In practice these links put you BELOW the header
  // element, but I want to include that element in the first view.

  // The function actually applying the offset
  function offsetAnchor() {
    if (location.hash.length !== 0) {
      window.scrollTo(window.scrollX, window.scrollY - 70);
    }
  }

  // Captures click events of all <a> elements with href starting with #
  $(document).on('click', 'a[href^="#"]', function (event) {
    // Click events are captured before hashchanges. Timeout
    // causes offsetAnchor to be called after the page jump.
    window.setTimeout(function () {
      offsetAnchor();
    }, 0);
  });

  // Set the offset when entering page with hash present in the url
  window.setTimeout(offsetAnchor, 0);
</script>

</html>